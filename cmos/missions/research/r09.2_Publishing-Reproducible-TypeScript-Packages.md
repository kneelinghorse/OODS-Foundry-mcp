A Definitive Guide to Publishing Reproducible TypeScript Packages in 2024-2025: A Monorepo Workflow
Executive Summary
This report provides a comprehensive, expert-level guide for publishing modern, secure, and reproducible TypeScript packages from a pnpm monorepo. It details a complete workflow for 2024–2025, addressing the complexities of dual ECMAScript Module (ESM) and CommonJS (CJS) module support, supply chain security via package provenance, and deterministic build verification. It establishes a gold-standard configuration using tsup for building, package.json conditional exports for module resolution, and Conventional Commits for automated versioning. This document serves as both a strategic blueprint and a practical, step-by-step implementation manual for library authors targeting the Node.js 20 ecosystem and beyond.

Part I: Architecting the Modern TypeScript Package
The foundation of a reliable and maintainable package lies in its architecture and configuration. A correctly structured project prevents a cascade of hard-to-debug issues related to module resolution, type inference, and cross-environment compatibility. This section establishes the canonical configuration for a modern TypeScript library.

1.1 The Dual-Module Challenge in a Modern Ecosystem
Publishing packages compatible with both ESM and CJS remains a critical requirement for library authors. While the JavaScript ecosystem is steadily migrating towards ESM as the standard module system, a significant portion of projects, tools, and older Node.js runtimes still rely on CJS. Recent advancements in Node.js v22 and later have introduced native support for CJS modules to require() ESM modules, easing some of the interoperability pain. However, this feature does not obviate the need for dual publishing.   

The broader ecosystem, including popular bundlers like webpack, various test runners, and a vast number of downstream applications, may not support this new interoperability layer. Consequently, to ensure maximum compatibility and provide a seamless experience for all consumers, library authors must continue to provide artifacts in both formats. The primary challenge is not merely producing two sets of JavaScript files but correctly configuring the package manifest (package.json) so that consuming tools automatically select the appropriate format and its corresponding type definitions without ambiguity or error. This dual-module hazard is a well-documented pain point for developers in the ecosystem.   

1.2 Mastering package.json: The Canonical Configuration
The package.json file serves as the manifest for a package, and its correct configuration is paramount. For modern packages, the exports field is the authoritative source for defining entry points, taking precedence over legacy fields like main and module in environments that support it. The recommended strategy is to adopt an exports-first approach, providing legacy fields solely for backward compatibility with older tooling.   

A crucial decision in this setup is the value of the "type" field. Some legacy guidance suggests omitting this field or setting it to "commonjs" to facilitate dual publishing. This approach typically relies on distinct file extensions (.mjs for ESM, .cjs for CJS) to signal the module format to the runtime. However, for projects targeting the modern Node.js ecosystem, this is a suboptimal strategy.   

The superior approach is to explicitly declare the package's primary paradigm by setting "type": "module". This establishes ESM as the default for all .js files within the package, aligning with the future direction of JavaScript. With this foundation, the exports field is then used to provide an explicit resolution path for CJS require() calls, pointing them to a dedicated .cjs build artifact. This configuration is more explicit, less ambiguous, and better supported by modern tooling, particularly TypeScript's nodenext module resolution strategy.   

Below is a minimal, annotated template for a package's package.json file, embodying these principles.

Minimal package.json Template:

JSON

{
  "name": "my-package",
  "version": "1.0.0",
  "private": false,
  "description": "A modern, dual-module TypeScript package.",
  "keywords": [
    "typescript",
    "esm",
    "cjs"
  ],
  "license": "MIT",
  "author": "Your Name <your.email@example.com>",
  "repository": {
    "type": "git",
    "url": "https://github.com/your-username/your-repo.git",
    "directory": "packages/my-package"
  },
  "type": "module",
  "main": "./dist/index.cjs",
  "module": "./dist/index.js",
  "types": "./dist/index.d.ts",
  "files": [
    "dist"
  ],
  "scripts": {
    "build": "tsup src/index.ts --format cjs,esm --dts --sourcemap --clean"
  },
  "exports": {
    ".": {
      "import": {
        "types": "./dist/index.d.ts",
        "default": "./dist/index.js"
      },
      "require": {
        "types": "./dist/index.d.cts",
        "default": "./dist/index.cjs"
      }
    }
  },
  "devDependencies": {
    "@arethetypeswrong/cli": "^0.1.2",
    "tsup": "^8.0.0",
    "typescript": "^5.4.0"
  },
  "engines": {
    "node": ">=20.0.0"
  },
  "publishConfig": {
    "access": "public"
  }
}
"type": "module": Declares the package's context as ESM, meaning .js files are treated as ES Modules by default.   

"main": "./dist/index.cjs": Provides a fallback entry point for legacy CJS environments (e.g., older Node.js versions) that do not interpret the "exports" field.   

"module": "./dist/index.js": Provides a fallback for older ESM-aware tools (e.g., early bundlers) that look for this field instead of "exports".   

"types": "./dist/index.d.ts": A fallback for legacy TypeScript versions, pointing to the primary ESM-compatible type declarations.   

"files": ["dist"]: Explicitly defines which files and directories should be included in the published tarball. This is a critical step to prevent source code, test files, or other development artifacts from being inadvertently published.   

1.3 Conditional Exports: The Definitive Entry Point Map
The "exports" field provides a powerful mechanism for defining a package's public API and controlling how different environments resolve its modules. A subtle but critical detail in its configuration is the order of conditions: for TypeScript to correctly resolve types, the "types" condition must appear before the corresponding "import" or "require" condition within each block. Failure to adhere to this ordering can lead to type resolution failures in consuming projects.   

The following configuration for the "exports" field is the recommended standard for dual-module packages.

Canonical exports Configuration:

JSON

"exports": {
  ".": {
    "import": {
      "types": "./dist/index.d.ts",
      "default": "./dist/index.js"
    },
    "require": {
      "types": "./dist/index.d.cts",
      "default": "./dist/index.cjs"
    }
  }
}
".": This key defines the resolution for the main package entry point (e.g., when a user writes import { something } from 'my-package').

"import" Condition: This block is matched by ESM import statements. It contains two sub-conditions:

"types": Points to the TypeScript declaration file (.d.ts) compatible with ESM syntax.

"default": Points to the compiled ESM JavaScript file (.js).

"require" Condition: This block is matched by CJS require() calls. It also contains two sub-conditions:

"types": Points to a distinct TypeScript declaration file (.d.cts) specifically generated for CJS compatibility. The .d.cts extension is crucial for preventing type resolution conflicts and ensuring that TypeScript understands the module format context correctly.

"default": Points to the compiled CJS JavaScript file (.cjs).

To validate the correctness of this complex configuration, the @arethetypeswrong/cli tool is indispensable. By running npx @arethetypeswrong/cli --pack. in the package directory, it analyzes the generated tarball and simulates module resolution from various environments, flagging any inconsistencies in the package.json configuration.   

1.4 TypeScript Configuration for Modern Module Resolution
The tsconfig.json file must be configured in harmony with the package.json module strategy. The module and moduleResolution compiler options directly influence how TypeScript interprets import statements and resolves modules through the "exports" field.

While the traditional moduleResolution: "node" setting is widely used, it does not fully comprehend the nuances of conditional exports or the distinct resolution algorithms for ESM and CJS in modern Node.js. To address this, TypeScript introduced the "node16" and "nodenext" options, which are designed to accurately mirror the behavior of the Node.js runtime's module resolver.   

For any project targeting Node.js 20 and employing a dual-module strategy with conditional exports, using module: "NodeNext" and moduleResolution: "NodeNext" is mandatory. This ensures that TypeScript's static analysis, type-checking, and IDE features like auto-import and go-to-definition will correctly interpret the "exports" map and provide an accurate developer experience.

Annotated tsconfig.json Template:

JSON

{
  "compilerOptions": {
    /* Type Checking */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noFallthroughCasesInSwitch": true,

    /* Modules */
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "resolveJsonModule": true,

    /* Emit */
    "declaration": true,
    "declarationMap": true,
    "sourceMap": true,
    "outDir": "./dist",
    
    /* JavaScript Support */
    "esModuleInterop": true,
    "forceConsistentCasingInFileNames": true,
    "allowSyntheticDefaultImports": true,

    /* Language and Environment */
    "target": "ES2022",
    "lib":,
    "skipLibCheck": true
  },
  "include": ["src"],
  "exclude": ["node_modules", "dist"]
}
"module": "NodeNext" and "moduleResolution": "NodeNext": This pair instructs TypeScript to use the modern Node.js module system rules, which are essential for correctly processing conditional exports and handling the differences between import and require resolution.

"target": "ES2022": Compiles to a modern JavaScript version that is fully supported by Node.js 20, preserving modern syntax features like top-level await.

"esModuleInterop": true: While NodeNext handles much of the complexity, this flag ensures better compatibility between CJS and ESM modules, particularly around default exports, which has historically been a source of confusion.   

Part II: The Build Pipeline: A Comparative Analysis
Selecting the right build tool is a critical decision that impacts developer productivity, build times, and the quality of the final package. This section provides a comparative analysis of the leading tools for building TypeScript libraries and presents a concrete implementation plan using the recommended tool.

2.1 Choosing Your Bundler: tsup vs. rollup vs. esbuild
The choice of a bundler involves a tradeoff between raw build speed, configuration complexity, and the degree of output optimization (e.g., bundle size via tree-shaking). For the specific task of building a dual-module TypeScript library, a higher-level tool that orchestrates underlying primitives is often more effective than using those primitives directly.

esbuild: Written in Go, esbuild offers unparalleled build speed, often orders of magnitude faster than JavaScript-based tools. However, its primary focus is on bundling and transpiling JavaScript. It does not natively generate TypeScript declaration files (.d.ts). Achieving this requires running the TypeScript compiler (tsc) in parallel, which adds complexity to the build process and requires careful coordination.   

rollup: rollup is widely regarded as the gold standard for library bundling. It is renowned for its superior tree-shaking capabilities, which produce the smallest possible bundles by eliminating unused code—a critical feature for libraries. However, configuring rollup for a modern dual-module setup requires significant boilerplate code and the integration of multiple plugins (e.g., rollup-plugin-esbuild for transpilation, rollup-plugin-dts for declaration files), increasing the maintenance burden.   

tsup: tsup occupies an ideal middle ground, providing the best of both worlds for TypeScript library authors. It leverages esbuild internally, inheriting its exceptional speed, while providing a simple, CLI-driven interface tailored to this specific use case. With a single command, tsup can generate dual ESM/CJS outputs, create the corresponding .d.ts and .d.cts declaration files, and produce sourcemaps. It effectively abstracts away the complexity of wiring esbuild and tsc together, making it the recommended choice for an efficient, reproducible, and low-maintenance workflow.   

The following table summarizes the key tradeoffs between these tools.

Table: Build Tool Tradeoff Matrix

Feature	tsup	rollup	esbuild
Foundation	Leverages esbuild	Standalone	Standalone (Go)
Configuration Complexity	Very Low (Zero-config focus)	Moderate to High	Low
Build Performance	Very Fast (inherits from esbuild)	Moderate	Fastest
Bundle Size (Tree-Shaking)	Good	Excellent	Good
Dual-Module Support	Built-in	Requires plugins	Requires external tooling
Type Declaration Generation	Built-in	Requires plugins	Requires external tooling

Export to Sheets
2.2 Implementing the Build Process with tsup
Integrating tsup into a project is straightforward. The build process can be defined with a single command in the scripts section of package.json.

The Build Script:

JSON

"scripts": {
  "build": "tsup src/index.ts --format cjs,esm --dts --sourcemap --clean"
}
This command instructs tsup to perform a comprehensive build operation based on the following flags:

src/index.ts: Specifies the entry point for the build.

--format cjs,esm: Directs tsup to generate output in both CommonJS (.cjs) and ECMAScript Module (.js) formats.   

--dts: Generates the necessary TypeScript declaration files. Crucially, tsup is smart enough to create both .d.ts for the ESM build and .d.cts for the CJS build, which is essential for the conditional exports strategy.   

--sourcemap: Generates source maps for both output formats, enabling easier debugging for consumers of the package.

--clean: Instructs tsup to delete the output directory (typically dist/) before starting a new build, ensuring no stale artifacts are left behind.

For more complex projects with multiple entry points (as required for subpath exports), a tsup.config.ts file can be used to define the build configuration programmatically, offering greater flexibility.   

Part III: Defining a Granular Public API with Subpath Exports
Modern packages often benefit from exposing more than a single entry point. Subpath exports allow library authors to define a rich, multi-faceted public API while maintaining strict control over the package's internal structure.

3.1 Rationale: Encapsulation and Intentional API Design
The exports map in package.json functions as more than just a routing mechanism; it is an access control list (ACL) for the package. When the exports field is defined, only the paths explicitly listed within it are resolvable by consumers. Any attempt to import a file not exposed through this map will result in a ModuleNotFound error.   

This enforces strong encapsulation, preventing users from depending on internal implementation details that are subject to change. By making the public API explicit, library authors can refactor internal code with confidence, knowing they will not break downstream consumers who might have been importing private modules. This is a significant advancement in library maintainability and API stability.

3.2 Configuring Subpath Exports in package.json
To expose internal modules, the exports map is extended with additional keys corresponding to the desired import paths. Both specific paths and wildcard patterns are supported.

Example exports Configuration with Subpaths:

JSON

"exports": {
  ".": {
    "import": {
      "types": "./dist/index.d.ts",
      "default": "./dist/index.js"
    },
    "require": {
      "types": "./dist/index.d.cts",
      "default": "./dist/index.cjs"
    }
  },
  "./utils": {
    "import": {
      "types": "./dist/utils.d.ts",
      "default": "./dist/utils.js"
    },
    "require": {
      "types": "./dist/utils.d.cts",
      "default": "./dist/utils.cjs"
    }
  },
  "./feature/*.js": {
    "import": {
      "types": "./dist/feature/*.d.ts",
      "default": "./dist/feature/*.js"
    },
    "require": {
      "types": "./dist/feature/*.d.cts",
      "default": "./dist/feature/*.cjs"
    }
  }
}
This configuration exposes three entry points:

The main entry point (.), for import 'my-package'.

A specific utility module (./utils), for import { someUtil } from 'my-package/utils'.

A collection of feature modules via a wildcard (./feature/*.js), for import { someFeature } from 'my-package/feature/some-feature.js'.

It is important to include the .js extension in wildcard exports. This practice is necessary to ensure that TypeScript's module resolver can correctly match the import path to the corresponding entry in the exports map and locate the associated type definitions.   

3.3 Ensuring Full Type Safety for Subpaths
As demonstrated in the example above, each subpath export must be a complete conditional export object. It must define both import and require conditions, and each of those must contain its own types and default (or import/require) entries.

This pattern ensures that no matter how a consumer imports a subpath—whether via ESM import in a modern project or CJS require() in a legacy environment—they will receive the correct module format and the corresponding, context-aware type definitions. Omitting any part of this structure will lead to a broken experience for a subset of users, undermining the goal of broad compatibility. The @arethetypeswrong/cli tool is again essential for verifying the correctness of these more complex exports maps.

Part IV: Establishing Trust: Provenance and Reproducibility
In the contemporary software landscape, securing the supply chain is as important as writing correct code. Modern attacks frequently target developer credentials or CI/CD pipelines to publish malicious package versions, rather than compromising the source code directly. This threat model necessitates mechanisms to verify not just the code, but the integrity of the entire publishing process. This section details two complementary pillars of trust: package provenance and deterministic builds.   

4.1 Generating npm Package Provenance (Verifying the Process)
npm Package Provenance is a security feature that creates a verifiable, cryptographic link between a published package, its source code repository, and the specific CI/CD build job that created it. It uses the Sigstore open-source project and OpenID Connect (OIDC) tokens to sign an attestation statement, which is then logged in a public, tamper-evident transparency ledger. This makes it exceedingly difficult for an attacker with a stolen npm token to publish a malicious package from an unauthorized location, as they would be unable to generate a valid provenance statement.   

While provenance can be enabled by adding a --provenance flag to the npm publish command, the recommended and more secure method is to use Trusted Publishing. This feature establishes a trust relationship between the npm registry and a specific CI/CD workflow. When configured, the CI provider can obtain a short-lived, single-use token via OIDC to authenticate with npm, completely eliminating the need for long-lived NPM_TOKEN secrets in the CI environment. For public packages published from public repositories via Trusted Publishing, provenance is generated automatically without needing the --provenance flag.   

The following GitHub Actions workflow demonstrates how to configure Trusted Publishing for a pnpm workspace.

GitHub Actions Workflow for Trusted Publishing (.github/workflows/publish.yml):

YAML

name: Publish Package to npm

on:
  push:
    tags:
      - 'v*.*.*'

jobs:
  publish:
    runs-on: ubuntu-latest
    permissions:
      id-token: write # Required for OIDC authentication with npm
      contents: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 9

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          registry-url: 'https://registry.npmjs.org'
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Build all packages
        run: pnpm -r build

      - name: Publish packages to npm
        run: pnpm -r publish --no-git-checks
This workflow is triggered whenever a new version tag (e.g., v1.2.3) is pushed. The permissions: id-token: write block is the critical piece that allows the job to request an OIDC token. The pnpm publish command can then execute without any NODE_AUTH_TOKEN environment variable, as authentication is handled automatically.   

Consumers of a package can verify its provenance by looking for the green checkmark on npmjs.com or by running npm audit signatures in their project after installation.   

4.2 Achieving Deterministic Builds (Verifying the Artifact)
While provenance verifies that a package was built in a trusted environment, it does not guarantee that the build process itself is sound. A compromised build tool or a non-deterministic build script could still introduce malicious code after the source code is checked out but before the package is published.

Reproducible builds provide a stronger guarantee. A build is considered reproducible if the exact same source code commit always produces a bit-for-bit identical package tarball, regardless of when or where the build is run. This allows any third party to independently clone the source, run the build, and verify that the resulting artifact matches the one published on the registry, confirming that no tampering occurred.   

Common sources of non-reproducibility include varying file timestamps embedded in the tarball, differences in dependency versions resolved by the package manager, or build scripts that behave differently based on environment variables. Pinning all dependencies to exact versions is a prerequisite for reproducibility.   

The following script provides a local verification mechanism to ensure a build is deterministic before publishing. It builds the package twice in separate, clean directories and performs a deep comparison of the resulting tarballs.

Script to Verify Deterministic npm pack (scripts/verify-reproducible.sh):

Bash

#!/bin/bash
set -e

# Get the package name from the root package.json
PACKAGE_NAME=$(node -p "require('./package.json').name")
PACKAGE_VERSION=$(node -p "require('./package.json').version")
PACKAGE_FILE_NAME=$(echo "$PACKAGE_NAME" | sed 's/@//' | sed 's/\//-/')"-$PACKAGE_VERSION.tgz"

echo "Verifying reproducibility for $PACKAGE_FILE_NAME..."

# Ensure a clean state
rm -rf.repro-tmp
mkdir -p.repro-tmp/run1.repro-tmp/run2

# --- First Run ---
echo "Performing first build..."
pnpm build
pnpm pack --pack-destination.repro-tmp/run1

# Move the tarball to a predictable name
mv.repro-tmp/run1/"$PACKAGE_FILE_NAME".repro-tmp/pack1.tgz

# --- Clean and Second Run ---
echo "Cleaning and performing second build..."
git clean -fdx # Clean all untracked files and directories
pnpm install --frozen-lockfile
pnpm build
pnpm pack --pack-destination.repro-tmp/run2

# Move the tarball to a predictable name
mv.repro-tmp/run2/"$PACKAGE_FILE_NAME".repro-tmp/pack2.tgz

# --- Verification ---
echo "Comparing tarball contents..."

# Extract contents for deep diff
mkdir -p.repro-tmp/extract1.repro-tmp/extract2
tar -xzf.repro-tmp/pack1.tgz -C.repro-tmp/extract1
tar -xzf.repro-tmp/pack2.tgz -C.repro-tmp/extract2

# Compare file checksums
echo "Comparing file checksums..."
CHECKSUM1=$(find.repro-tmp/extract1 -type f -print0 | sort -z | xargs -0 shasum | shasum)
CHECKSUM2=$(find.repro-tmp/extract2 -type f -print0 | sort -z | xargs -0 shasum | shasum)

if; then
    echo "❌ Reproducibility check failed: File contents differ between builds."
    echo "Running diff to show differences:"
    diff -qr.repro-tmp/extract1.repro-tmp/extract2
    # Clean up and exit with error
    rm -rf.repro-tmp
    exit 1
else
    echo "✅ Reproducibility check passed: File contents are identical."
fi

# Clean up
rm -rf.repro-tmp
echo "Verification complete."
This script serves as a critical local gate in the release process. A release should not proceed if this check fails.

Part V: The Professional Release Workflow
A professional release workflow synthesizes architecture, tooling, and security into a coherent, repeatable process. It should be automated as much as possible to reduce human error and be independent of any single developer's machine.

5.1 Versioning and CHANGELOG Strategy
The recommended strategy for versioning and changelog management is to adopt the Conventional Commits specification. This lightweight convention adds machine-readable meaning to commit messages, which enables powerful automation.   

The core benefit of Conventional Commits is its direct mapping to Semantic Versioning (SemVer). Automated tools can parse the commit history since the last release and determine the correct version bump:

Commits prefixed with fix: correspond to a PATCH release.

Commits prefixed with feat: correspond to a MINOR release.

A commit containing a BREAKING CHANGE: footer triggers a MAJOR release.   

This system should be implemented with the following tooling:

Enforcement: Use commitlint to validate that commit messages adhere to the specification. This is best enforced automatically using husky to create a commit-msg git hook, which prevents non-compliant commits from being created.   

Automation: Use a release automation tool like release-it (with the @release-it/conventional-changelog plugin) or standard-version. These tools can perform three critical release tasks in a single command:

Analyze the commit history to determine the next semantic version.

Automatically generate or update a CHANGELOG.md file with categorized changes, linking to the relevant commits.   

Create a new Git commit for the version bump and changelog update, and create an associated Git tag for the new version.

5.2 A CI-Independent Local Release Checklist
The release process should separate the preparation and triggering steps (performed locally by a maintainer) from the sensitive act of publishing (performed by the secure CI environment). This separation of concerns enhances security and ensures the process is robust.

The following checklist outlines a CI-independent local release workflow.

Sync Local Environment:

Ensure the local main branch is up-to-date with the remote repository.

git checkout main && git pull origin main

Verify the working directory is clean: git status

Run Local Quality Gates:

Execute the full suite of tests for the entire workspace: pnpm -r test

Run the linter to catch any code style issues: pnpm -r lint

Verify Build Reproducibility:

For each package being released, run the deterministic build verification script.

cd packages/<package-to-release> &&../../scripts/verify-reproducible.sh

Do not proceed if this step fails. Investigate and resolve the source of non-determinism.

Execute Automated Versioning and Tagging:

Run the release automation tool. This will calculate the new version, update package.json and CHANGELOG.md, and create the version commit and tag.

pnpm release (assuming release is a script that runs release-it or standard-version).

Trigger the Secure Publish Workflow:

Push the newly created commit and tag to the remote repository. The --follow-tags flag ensures the tag is pushed along with the commit.

git push --follow-tags origin main

Monitor and Confirm:

Observe the GitHub Actions workflow that was triggered by the tag push.

Confirm that the workflow completes successfully and that the new package version appears on npmjs.com with the provenance verification badge.

Part VI: Advanced Dependency Management in a pnpm Monorepo
pnpm offers several powerful features tailored for monorepos that help maintain consistency, improve performance, and simplify dependency management.

6.1 The workspace: Protocol: Safe Intra-Repo Linking
Within a pnpm workspace, dependencies between local packages should be declared using the workspace: protocol. For example, if package-a depends on package-b, its package.json would contain:   

JSON

"dependencies": {
  "package-b": "workspace:*"
}
During development, pnpm interprets this by creating a symlink from package-a/node_modules/package-b to the source directory of package-b. This ensures that changes in package-b are immediately reflected in package-a without needing to be rebuilt or reinstalled.

Crucially, when pnpm publish is run, it automatically transforms these workspace: specifiers. For example, workspace:* is replaced with the actual version number from package-b's package.json (e.g., "1.2.3"). This transformation makes the published package consumable by any standard package manager, as it contains a standard SemVer dependency range.   

6.2 Enforcing Dependency Consistency and Pinning
In a large monorepo, preventing dependency version fragmentation is essential for avoiding subtle bugs, security vulnerabilities, and bloated application bundles. pnpm provides several centrally-managed mechanisms to enforce consistency.

A multi-layered strategy is most effective:

Default to Exact Versions: For maximum reproducibility, all dependencies should be pinned to exact versions. This can be enforced as the default behavior for all pnpm add commands by creating a .npmrc file in the workspace root with the line save-prefix=''. This removes the default caret (^) prefix.   

Manage Shared Dependencies with Catalogs: For common dependencies shared across many packages (e.g., typescript, react, eslint), using pnpm catalogs is the most elegant solution. A catalog is defined in the root pnpm-workspace.yaml file, creating a named alias for a version range. Packages can then reference this alias. This centralizes version management, allowing for a workspace-wide upgrade with a single-line change.   

Patch Transitive Dependencies with Overrides: Occasionally, a transitive dependency (a dependency of a dependency) may have a critical bug or security flaw. The overrides field in pnpm-workspace.yaml allows you to forcefully replace any version of a package anywhere in the dependency graph with a specific version. This acts as a powerful, surgical tool for patching issues that are outside of your direct control.   

The following table compares these strategies.

Table: pnpm Dependency Pinning Strategies

Strategy	Mechanism	Scope	Centralization	Primary Use Case
Exact Pinning	save-prefix='' in .npmrc	Newly added dependencies	High	Enforcing reproducibility by default for all new dependencies.
Catalogs	catalog: in pnpm-workspace.yaml	Explicitly opted-in dependencies	High	Managing versions of common, shared direct dependencies across the workspace.
Overrides	overrides in pnpm-workspace.yaml	Entire dependency graph	High	Forcibly replacing a specific version of any dependency, especially transitive ones, to patch bugs or security issues.

Export to Sheets
By combining these three mechanisms, maintainers can achieve a high degree of control and consistency over the entire dependency tree of the monorepo.

Conclusion: Synthesizing a Bulletproof Publishing Strategy
Publishing a modern TypeScript package is no longer a simple act of running a command. It is a comprehensive engineering discipline that requires a holistic strategy integrating architecture, tooling, security, and process. The workflow detailed in this report provides a robust and forward-looking blueprint for creating high-quality, trustworthy, and maintainable software artifacts for the 2024–2025 ecosystem.

The key pillars of this strategy are:

Explicit Architecture: Utilizing "type": "module" and a meticulously crafted conditional "exports" map in package.json to unambiguously support both ESM and CJS consumers.

Optimized Tooling: Employing tsup as a specialized build tool that combines the speed of esbuild with the TypeScript-specific features needed for a simple and efficient dual-module build process.

Verifiable Security: Implementing a two-pronged security model that uses npm Package Provenance (via Trusted Publishing) to verify the integrity of the publishing process and deterministic build checks to verify the integrity of the final artifact.

Disciplined Process: Adopting Conventional Commits and automated release tooling to create a reliable, CI-independent workflow that minimizes human error and ensures consistent versioning and changelog generation.

Centralized Management: Leveraging the advanced features of pnpm workspaces, such as the workspace: protocol, catalogs, and overrides, to maintain a healthy and consistent monorepo at scale.

By adopting this comprehensive approach, library authors can navigate the complexities of the modern JavaScript ecosystem with confidence, delivering professional-grade packages that are secure, reliable, and easy for the community to consume.


Sources used in the report

snyk.io
Building an npm package compatible with ESM and CJS in 2024 - Snyk
Opens in a new window

lirantal.com
TypeScript in 2025 with ESM and CJS npm publishing is still a mess - Liran Tal
Opens in a new window

johnnyreilly.com
Dual Publishing ESM and CJS Modules with tsup and Are the Types ...
Opens in a new window

2024.stateofjs.com
Build Tools - State of JavaScript 2024
Opens in a new window

webpack.js.org
Package exports - webpack
Opens in a new window

nodejs.org
Modules: Packages | Node.js v24.10.0 Documentation
Opens in a new window

blog.r0b.io
ESM Node.js TypeScript with subpath exports | r0b blog
Opens in a new window

typescriptlang.org
Documentation - Modules - ESM/CJS Interoperability - TypeScript
Opens in a new window

npm-compare.com
esbuild vs rollup vs webpack vs tsup | JavaScript Bundlers ...
Opens in a new window

gist.github.com
The best Rollup config for TypeScript libraries - GitHub Gist
Opens in a new window

blog.atomrc.dev
Compiling your TypeScript library (2023 edition) - Thomas Belin
Opens in a new window

rlee.dev
How To Use an ESM dependency in a library that exports ESM+CJS - Ryan's Blog
Opens in a new window

github.blog
Introducing npm package provenance - The GitHub Blog
Opens in a new window

medium.com
NPM Provenance: The Missing Security Layer in Popular JavaScript Libraries - Medium
Opens in a new window

docs.npmjs.com
Generating provenance statements | npm Docs
Opens in a new window

github.com
Details on npm provenance - GitHub
Opens in a new window

docs.npmjs.com
Trusted publishing for npm packages
Opens in a new window

docs.npmjs.com
Viewing package provenance - npm Docs
Opens in a new window

blog.vlt.sh
Reproducibility vs. Provenance: Trusting the JavaScript Supply Chain
Opens in a new window

sanghani.cs.vt.edu
Investigating The Reproducibility of NPM Packages | Sanghani Center | Virginia Tech
Opens in a new window

conventionalcommits.org
Conventional Commits
Opens in a new window

github.com
chermdev/conventional-commits-setup: How to add conventional commits in 3 steps for your projects. - GitHub
Opens in a new window

github.com
Enforce conventional commit messages · Issue #236 · rdkcentral/Ripple - GitHub
Opens in a new window

gitlab.com
ci: enforce conventional commit format - Merge requests - GitLab
Opens in a new window

fireflysemantics.medium.com
Generate the Changelog with Release-It and Conventional Commits | by Ole Ersoy
Opens in a new window

dev.to
How (and Why) to enforce conventional commits on your project? - DEV Community
Opens in a new window

github.com
Generate Changelog based on Conventional Commits · Actions · GitHub Marketplace
Opens in a new window

cuyl.github.io
Workspace | pnpm
Opens in a new window

pnpm.io
Workspace | pnpm
Opens in a new window

news.ycombinator.com
We switched already to pnpm and won't look back. > Hardened Mode, constraints en... | Hacker News
Opens in a new window

github.com
How to make --save-exact the default when using pnpm add · Issue #7172 - GitHub
Opens in a new window

pnpm.io
Catalogs | pnpm
Opens in a new window

pnpm.io
Catalogs | pnpm
Opens in a new window

pnpm.io
Settings (pnpm-workspace.yaml)
Opens in a new window

squash.io
How to Use pnpm Overrides for Dependency Management - Squash.io