Architectures of Agency: A Comparative Analysis of Tool Integration for Claude MCP and OpenAI Agents
Executive Summary
This report provides a comprehensive architectural analysis of the two dominant paradigms for AI agent tool integration in 2024-2025: Anthropic's stateful, protocol-driven Model Context Protocol (MCP) for Claude, and OpenAI's stateless, developer-orchestrated Responses API. We will dissect connection patterns, authentication mechanisms, and tool definition schemas for each ecosystem, highlighting key strategic differences. The analysis culminates in a practical "smoke flow" implementation, a detailed examination of common pitfalls such as CORS and timeouts, and strategic recommendations for architects selecting a platform. A primary finding is the fundamental trade-off between MCP's standardized, discovery-based approach, which fosters a decoupled ecosystem but introduces protocol complexity, and OpenAI's flexible, developer-centric model, which offers greater control at the cost of requiring more explicit orchestration logic.

Part I: The Model Context Protocol (MCP) for Claude Agents
1.1. Architectural Philosophy: The "USB-C for AI"
The Model Context Protocol (MCP) is an open protocol standardizing how applications provide context—including tools, resources, and prompts—to Large Language Models (LLMs). Introduced by Anthropic in late 2024, its design philosophy is to function as a "USB-C for AI," creating a vendor-agnostic ecosystem where a tool server, once built, can work with any MCP-compliant client.   

The protocol's architecture is founded on stateful, persistent connections. Unlike stateless REST APIs, MCP utilizes low-latency, two-way communication channels, typically established over WebSockets or Server-Sent Events (SSE). This foundational design choice enables interactive, stateful sessions where context is maintained throughout the conversation, which is critical for complex, multi-step tasks that require memory and iterative reasoning.   

The central mechanism of MCP is dynamic capability discovery. Upon connection, an MCP server sends a self-describing manifest—a JSON list of its capabilities—to the client. This manifest explicitly informs the AI model of the available tools, resources, and prompts, including their names, functions, and required parameters. This process ensures the model does not need to guess or "hallucinate" API endpoints; it is told precisely what it can do and how to do it. This discovery-based approach contrasts sharply with architectures where tools must be declared in every API request, forming the basis of MCP's unique value proposition.   

1.2. Connection Patterns and Configuration
The MCP specification supports two primary connection types, each catering to different use cases and environments.

Local (stdio) vs. Remote (url) Servers
Local Servers: These are processes executed on the user's local machine. They communicate with an MCP client, such as the Claude Desktop application, via standard input/output (stdio) streams. This pattern is the predominant choice for local development tools, enabling tight integration with IDEs, terminals, and local file systems.   

Remote Servers: These are internet-accessible endpoints that communicate with clients over standard web protocols. The most common transport mechanisms are Server-Sent Events (SSE) or the Streamable HTTP protocol, which combines HTTP with SSE for flexible bidirectional communication. This pattern is essential for integrating with Software-as-a-Service (SaaS) platforms like Asana, Stripe, and PayPal, which have deployed their own remote MCP servers.   

Centralized Configuration: .claude/mcp.json and claude_desktop_config.json
Configuration of MCP servers is managed through local JSON files, which act as a registry for the Claude client. The structure of these files is critical for defining how servers are launched and managed.

A typical configuration for a local stdio server specifies the type, the command used to execute the server (e.g., npx, node), and the args to pass to that command.   

JSON

{
  "mcpServers": {
    "my-custom-tool": {
      "type": "stdio",
      "command": "node",
      "args": ["/path/to/my/server/index.js"]
    }
  }
}
A crucial component of this configuration is the env block. This object provides the primary documented mechanism for securely passing secrets, such as API keys, to local stdio servers as environment variables. For example, the community-built mcp-omnisearch tool consolidates multiple search APIs and is configured by passing API keys through the env block.   

JSON

{
  "mcpServers": {
    "mcp-omnisearch": {
      "type": "stdio",
      "command": "npx",
      "args": ["-y", "mcp-omnisearch"],
      "env": {
        "TAVILY_API_KEY": "your-tavily-key",
        "BRAVE_API_KEY": "your-brave-key",
        "KAGI_API_KEY": "your-kagi-key"
      }
    }
  }
}
The heavy emphasis in documentation and community examples on stdio-based servers, configured via local JSON files that launch local processes, points toward an initial design focused on the developer's local environment. The most significant and under-documented challenges within the community revolve around authentication, particularly passing custom headers or dynamic API keys to these local servers, as stdio has no native concept of HTTP headers. In contrast, remote servers utilize a more robust, web-native OAuth flow. This evidence suggests that MCP was conceived with a "developer-first" or "local-first" mindset, prioritizing integration with local development workflows. The architectural challenges surrounding authentication for local servers appear to be a direct consequence of this initial focus.   

1.3. The HTTP Bridge: A Community-Driven Pattern
The HTTP bridge is not an official component of the MCP specification but has emerged as a necessary and common architectural pattern developed by the community to overcome the inherent isolation of local servers. This pattern involves a standard MCP server, often stdio-based, that functions as an intermediary. It receives JSON-RPC messages from the Claude client and translates them into standard HTTP or WebSocket requests directed at another, separate service.

A clear example of this pattern is in browser automation. The browser-tools-mcp project allows Claude to control a web browser. Its architecture involves Claude Code communicating via stdio to a local MCP server. This server, in turn, makes HTTP requests to a bridge server, which finally communicates with a Chrome browser extension using WebSockets. This multi-hop architecture is necessary because a stdio process cannot directly access browser extension APIs.   

Another use case is enabling remote access to a local Claude Code session. One community project bridges local sessions to a Telegram bot. A background listener script intercepts Claude's output, sends it to a Telegram bot, and receives user replies from the bot. These replies are then injected back into the original session using the claude --resume command, preserving the context. This effectively bridges the isolated local terminal session to an external, mobile-accessible messaging platform.   

The prevalence of the HTTP bridge pattern highlights both a limitation and a strength of the local MCP model. The stdio transport is inherently isolated, designed for process-to-process communication on a single machine. However, many valuable tools—browsers, mobile applications, and SaaS APIs—exist outside this isolated environment and communicate via standard web protocols. To connect the MCP ecosystem to this broader web ecosystem, developers have been compelled to build these "bridge" services. This reveals the limitation of the protocol's isolation but also demonstrates its strength: the protocol is simple and flexible enough that developers can effectively build these adapters, resulting in a powerful, albeit more complex, architecture.

1.4. Authentication Mechanisms
Authentication is a critical aspect of tool integration, and the MCP framework presents two distinct models depending on the server connection type.

Remote Servers: The Standard OAuth 2.0 Flow
For third-party remote servers, such as those provided by Asana or Stripe, MCP specifies a standard OAuth 2.0 authorization code flow. This is a well-understood and secure standard for delegated authorization. The interaction proceeds as follows:   

The user initiates the connection in the Claude client.

The Claude client connects to the remote MCP server and discovers the authorization server's URL from its metadata.

The user is redirected to the third-party service's authorization page to grant permissions.

Upon successful authorization, an authorization_token (an OAuth access token) is generated and returned to the Claude client.

This token is then included in subsequent Messages API requests to Anthropic, which passes it to the remote MCP server to authenticate tool calls. The responsibility for managing this token, including its refresh lifecycle, lies with the API consumer.   

Local Servers: The Custom Header Challenge
In stark contrast, there is no officially documented, standardized mechanism within the .claude/mcp.json configuration for passing arbitrary, dynamic, or per-request credentials (e.g., Authorization: Bearer... or X-API-Key:...) to an MCP server. This represents a significant architectural gap for securing custom tools, particularly those running locally or in private networks. The community has developed several workarounds to address this limitation:   

Environment Variables (env): As previously noted, the env block in the configuration file is the primary method for passing static secrets to stdio-based servers. The server process then reads these values from its environment. While effective for static API keys, this approach is inflexible, as the credentials are set once per server definition and cannot be changed dynamically per user or per request.   

Server-Side Context Injection: In more sophisticated environments like AWS AgentCore, developers can pass custom headers to the agent host application. This application must then be programmatically configured to inject these headers into the MCP client's connection request. This approach is powerful but requires custom code on the "host" side (the application running the MCP client), shifting the burden of credential management.   

Embedding Credentials in Tool Arguments: A highly discouraged but technically possible workaround is to include credentials as part of the tool's input arguments. This poses a significant security risk, as the sensitive credentials become part of the LLM's context and may be logged or exposed inadvertently.   

The authentication model is thus fundamentally tied to the choice of a local versus a remote server. The robust, standard-based OAuth flow for remote servers is designed for enterprise and SaaS integrations. Meanwhile, the lack of a standardized solution for passing dynamic credentials to local servers remains the single greatest point of friction in the current MCP ecosystem for developers building custom tools.

1.5. Tool Definition and Invocation
The core of MCP's tool integration lies in its manifest and the use of the JSON-RPC 2.0 protocol for invocation.

The Manifest as a Contract
The manifest provided by the MCP server upon connection serves as a machine-readable contract that defines the server's available tools. This structured document, typically in JSON format, contains critical metadata for each tool, including:   

Tool Name and Description: Human-readable identifiers that clarify the tool's purpose. Tool names should be unique within a server and follow a verb_noun snake case convention (e.g., generate_report).   

Parameters and Types: A detailed schema for input arguments, specifying data types, required fields, and validation rules.   

Security Scopes and Permissions: Constraints that define what the tool can access or modify, supporting least-privilege principles.   

The Invocation Flow (JSON-RPC)
The lifecycle of a tool call follows a clear, protocol-defined sequence:

Model Decision: Based on the user's prompt and the available tools listed in the manifest, the model decides to invoke a specific tool (e.g., send_tokens).   

Structured Request: The model constructs a structured JSON object containing the tool_name and the required args.   

JSON-RPC Transmission: The MCP client sends this request to the server over the persistent connection using the JSON-RPC 2.0 protocol. This is a live, stateful exchange.   

Server Execution: The server receives the JSON-RPC request, parses it, executes the corresponding tool logic with the provided arguments, and prepares a response.

Structured Response: The server returns a JSON-RPC response object. On success, this object contains a result field with the tool's output. On failure, it contains an error object with details about the failure.   

Model Continues: The model receives the result and uses this new information to continue the conversation, generate a response for the user, or chain another tool call, enabling iterative reasoning.   

Part II: Function Calling and the Responses API for OpenAI Agents
2.1. Architectural Philosophy: Developer-Orchestrated and Stateless
OpenAI's approach to tool integration is fundamentally different from Anthropic's MCP. It is characterized by a stateless, developer-orchestrated model that prioritizes flexibility and developer control over protocol-level standardization.

A significant strategic shift is underway within the OpenAI ecosystem. The company is actively deprecating the stateful Assistants API in favor of the new, more powerful Responses API. The Assistants API, with its object-heavy model of Assistants, Threads, and Runs, will be shut down on August 26, 2026, after the Responses API achieves full feature parity. This move signals a deliberate transition away from a managed state model toward a simpler, more elegant, and stateless API primitive that provides a more flexible foundation for building agentic applications.   

The core of this architecture is a multi-step, stateless request/response cycle over HTTP. Unlike MCP's persistent connection, each API call is independent. The developer's application is entirely responsible for maintaining the state of the conversation between API calls. The developer acts as the central orchestrator of the tool-use workflow. The model does not execute tools directly. Instead, when it determines a tool is needed, it returns a tool_calls object in its response. This effectively pauses the model's execution and hands control back to the developer's application. The application code is then responsible for parsing this object, executing the appropriate external functions or API calls, and submitting the results back to the model in a subsequent request to continue the process.   

2.2. Connection Patterns and Configuration
In the OpenAI model, the concept of a separate "tool server" is absent. The developer's application code serves as the bridge between the OpenAI API and the external tool's endpoint.

The "Bring Your Own Bridge" Approach
By default, the developer's application is the HTTP bridge. There is no standalone server process to configure or run. The same application logic that calls the OpenAI API is also responsible for executing the tool, which typically involves making a standard HTTP request to a third-party API endpoint using a library like requests in Python or fetch in JavaScript.   

SDK-Level Configuration
All configuration is handled programmatically within the code during the initialization of the OpenAI client SDK. Key parameters that can be set at the client level include the api_key for authentication, a custom base_url to route requests through a proxy or to an OpenAI-compatible endpoint (like Azure, Google Gemini, or a local LM Studio instance), and default_headers for passing additional metadata with every request.   

The Rise of AgentKit: Abstracting Orchestration
OpenAI recognizes that the developer-orchestrated model can lead to complex application logic for sophisticated agents. To address this, the company introduced AgentKit, a higher-level toolkit designed to simplify the construction of agentic workflows. AgentKit includes:   

Agent Builder: A visual canvas for composing multi-agent workflows with drag-and-drop nodes.

Connector Registry: A centralized place to manage connections to data and tools.

ChatKit: Embeddable UI components for creating chat-based agent experiences.

AgentKit represents OpenAI's strategic solution for managing the orchestration complexity that its stateless API inherently pushes onto the developer.

A critical strategic element of OpenAI's platform is its explicit support for MCP as a tool type. The OpenAI API reference documentation lists three categories of tools: Built-in tools (like Web Search), Function calls (custom tools defined by the developer), and MCP Tools. This indicates that OpenAI's platform is not positioned as a competitor to MCP but rather as a consumer of it. An OpenAI agent can be configured to connect to and utilize tools exposed by a third-party MCP server. This positions OpenAI's agent framework as a potential "meta-platform" or hub that can leverage tools from both its native function-calling ecosystem and the emerging, standardized MCP ecosystem. This is not a rivalry but an integration, suggesting a future where MCP could become a widely adopted interoperability layer for AI tools, with OpenAI agents being first-class citizens.   

2.3. Authentication Mechanisms
The OpenAI model necessitates a dual-layer approach to authentication, clearly separating access to the OpenAI platform from access to the external tool.

Dual-Layer Authentication
Client-to-OpenAI Authentication: The developer's application authenticates its requests to the OpenAI API using a standard Authorization: Bearer $OPENAI_API_KEY HTTP header. This is a prerequisite for any interaction with the model.   

Bridge-to-Tool Authentication: After receiving a tool_calls directive from the model, the developer's application is then separately responsible for authenticating to the actual tool's HTTP endpoint. This logic is implemented entirely within the developer's code and can involve any authentication scheme required by the tool's API, such as adding another Authorization header, an X-API-Key header, or using OAuth tokens.

Managing Custom Headers in the SDK
The OpenAI Python SDK provides parameters like default_headers (set during client initialization) and extra_headers (set on a per-request basis). It is crucial to understand that these headers are intended for the request made to the OpenAI API itself, not to the final tool endpoint. Their primary use cases are for routing requests through custom proxy gateways that may require their own authentication, or for passing metadata to OpenAI, such as organization and project IDs for billing and logging purposes.   

Passing custom headers to the actual tool is handled by the developer's code during tool execution. After receiving the tool_calls object, the developer would use a standard HTTP client library to construct the request to the tool's endpoint, adding any necessary authentication headers at that stage.   

2.4. Tool Definition and Invocation
Tool definition in the OpenAI ecosystem is based on the well-established JSON Schema standard, providing a powerful and flexible way to describe function interfaces.

JSON Schema as the Standard
Tools are defined as a JSON object within the tools parameter of an API request. This object must conform to the JSON Schema specification, allowing developers to describe the function's name, a detailed description of its purpose, and a parameters object that defines all input arguments, including their data types, whether they are required, and constraints such as enums.   

JSON

{
  "type": "function",
  "name": "get_weather",
  "description": "Retrieves current weather for a given location.",
  "parameters": {
    "type": "object",
    "properties": {
      "location": {
        "type": "string",
        "description": "City and country, e.g., Bogotá, Colombia"
      }
    },
    "required": ["location"]
  }
}
The Invocation Flow (Stateless)
The tool-calling flow is a multi-step conversation orchestrated by the developer's application:

Step 1: Request with Tools: The developer sends an initial request to the Responses API. This request includes the user's message and the tools array containing the JSON Schema definitions for all functions the model is allowed to call.   

Step 2: Receive Tool Call: If the model determines that a tool is necessary to answer the user's query, the API responds with a message containing a tool_calls array. Each object in this array includes a unique tool_call_id, the name of the function to be called, and a JSON string of the arguments the model has generated. At this point, the model's turn is paused, awaiting the tool's output.   

Step 3: Execute Tool: The developer's application code parses the tool_calls array. For each tool call, it identifies the corresponding function in the application's codebase, parses the arguments, and executes the function. This is the stage where the actual HTTP request to the external tool's endpoint is made.   

Step 4: Submit Output: After executing the tool(s), the developer makes a second request to the Responses API. This request must include the entire conversation history up to this point, plus a new message with role: "tool". This message contains the content (the output from the function execution) and the tool_call_id to link the result back to the specific tool call that prompted it.   

Step 5: Receive Final Response: The model processes the tool's output in the context of the conversation and generates a final, user-facing text response, completing the turn.   

Part III: Comparative Analysis and a Practical Smoke Flow
3.1. Core Architectural Divergence: Discovery vs. Declaration
The fundamental difference between the Claude/MCP and OpenAI/Responses API architectures lies in how tools are made available to the model.

Claude/MCP employs a discovery model. The agent establishes a persistent connection to an MCP server and discovers the available toolset by parsing the server's manifest. The set of available tools is tied to the server connection itself, not to an individual request. This creates a stateful, session-based interaction where the agent has a continuous understanding of its capabilities as long as the connection is active.

OpenAI/Responses API uses a declaration model. The developer declares which tools are available to the model within the tools parameter of every API request. The toolset is ephemeral and scoped only to that specific request-response cycle. This creates a stateless interaction that gives the developer fine-grained, per-call control over the model's capabilities.

This divergence leads to significant architectural trade-offs. MCP's discovery model is well-suited for stable environments with a relatively fixed set of tools, such as an IDE plugin or a dedicated enterprise agent. It fosters a decoupled ecosystem where tool providers can independently manage and update their MCP servers. Conversely, OpenAI's declaration model offers greater flexibility for dynamic scenarios where the available tools might change based on user permissions, application state, or other contextual factors, placing the onus of orchestration squarely on the developer.

3.2. Implementation of a Smoke Flow: get_api_status(url)
To illustrate these differences concretely, this section provides a side-by-side implementation of a simple get_api_status(url) tool, which checks the HTTP status of a given URL.

Step 1: The Universal HTTP Bridge/Server
First, we create a simple HTTP server using Node.js and Express. This server will act as the external tool's endpoint for both the Claude and OpenAI implementations.

tool-server.js

JavaScript

const express = require('express');
const fetch = require('node-fetch');
const app = express();
app.use(express.json());

app.post('/check-status', async (req, res) => {
  const { url } = req.body;
  if (!url) {
    return res.status(400).json({ error: 'URL is required' });
  }
  try {
    const response = await fetch(url, { method: 'HEAD' });
    res.json({ status: response.status, statusText: response.statusText });
  } catch (error) {
    res.status(500).json({ error: 'Failed to fetch URL', details: error.message });
  }
});

const PORT = 3000;
app.listen(PORT, () => {
  console.log(`Tool server listening on http://localhost:${PORT}`);
});
To run this server: node tool-server.js

Step 2: Claude/MCP Implementation
We will create a local stdio-based MCP server that exposes the get_api_status tool. When called, this server will make an HTTP request to our universal tool server.

mcp-status-checker.js

JavaScript

const { StdioMcpServer } = require('@modelcontextprotocol/server');
const fetch = require('node-fetch');

const server = new StdioMcpServer({
  name: 'status-checker',
  version: '0.1.0',
});

server.addTool({
  name: 'get_api_status',
  description: 'Gets the HTTP status code for a given URL.',
  parameters:,
  execute: async ({ url }) => {
    const response = await fetch('http://localhost:3000/check-status', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ url }),
    });
    const result = await response.json();
    return JSON.stringify(result);
  },
});

server.listen();
.claude/mcp.json Configuration

JSON

{
  "mcpServers": {
    "status-checker-bridge": {
      "type": "stdio",
      "command": "node",
      "args": ["/path/to/your/mcp-status-checker.js"]
    }
  }
}
With this configuration, starting Claude Desktop will automatically launch the MCP server. A prompt like "check the status of https://www.anthropic.com" will trigger the get_api_status tool.

Step 3: OpenAI/Responses API Implementation
We will write a Python script that defines the get_api_status tool, makes the initial API call, handles the tool_calls response by calling our universal server, and submits the result back to the model.

openai-status-checker.py

Python

import os
import json
import requests
from openai import OpenAI

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

def get_api_status(url):
    """Gets the HTTP status code for a given URL."""
    try:
        response = requests.post("http://localhost:3000/check-status", json={"url": url})
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        return {"error": str(e)}

def run_conversation():
    messages =
    tools =,
                },
            },
        }
    ]

    # Step 1 & 2: Make request and receive tool call
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        tools=tools,
        tool_choice="auto",
    )
    response_message = response.choices.message
    tool_calls = response_message.tool_calls

    if tool_calls:
        # Step 3: Execute tool
        messages.append(response_message)
        available_functions = {"get_api_status": get_api_status}
        
        for tool_call in tool_calls:
            function_name = tool_call.function.name
            function_to_call = available_functions[function_name]
            function_args = json.loads(tool_call.function.arguments)
            function_response = function_to_call(url=function_args.get("url"))
            
            # Step 4: Submit output
            messages.append(
                {
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": function_name,
                    "content": json.dumps(function_response),
                }
            )
        
        # Step 5: Receive final response
        second_response = client.chat.completions.create(model="gpt-4o", messages=messages)
        print(second_response.choices.message.content)

if __name__ == "__main__":
    run_conversation()
Running this script (python openai-status-checker.py) executes the full, multi-step tool-calling flow.

3.3. Table 1: Feature and Implementation Comparison
The following table provides a high-density summary for architects to quickly compare the two ecosystems across key architectural and implementation dimensions.

Feature	Claude / Model Context Protocol (MCP)	OpenAI / Responses API
Connection Type	Stateful, persistent (WebSocket/SSE)	Stateless, request-response (HTTP)
State Management	Handled by the protocol/persistent connection	Responsibility of the developer's application
Tool Discovery	Dynamic discovery via server manifest on connect	Declarative; tools defined in each API request
Configuration	Filesystem-based (.claude/mcp.json)	In-code SDK client initialization
Auth (Client-to-Platform)	N/A (Client authenticates to its own backend)	Authorization: Bearer header to OpenAI API
Auth (Bridge-to-Tool)	Complex for local (stdio) servers (env vars); Standard OAuth 2.0 for remote servers	Handled entirely by developer's application code
Tool Definition Standard	MCP Manifest Specification (JSON)	JSON Schema
Orchestration Logic	Resides within the MCP server	Resides within the developer's application code
Ecosystem Model	Decoupled: Tool providers run MCP servers	Coupled: Developer's code is the integration point
Relative Complexity	Higher initial protocol implementation; simpler ongoing calls	Lower initial setup; more complex state/orchestration logic

Export to Sheets
Part IV: Navigating Common Implementation Pitfalls
4.1. The CORS Conundrum
Cross-Origin Resource Sharing (CORS) is a browser-enforced security mechanism, not an error. It is one of the most common hurdles when building a web-based UI for an AI agent that communicates with a backend HTTP bridge hosted on a different origin (domain, protocol, or port).   

Root Cause Analysis
The Same-Origin Policy (SOP) is a fundamental security principle implemented by all modern web browsers. It restricts how a document or script loaded from one origin can interact with a resource from another origin. This policy is designed to prevent malicious scripts on one page from accessing sensitive data on another page, such as a user's banking session. When an agent's UI (e.g., https://my-agent-ui.com) attempts to make an API call to its HTTP bridge (e.g., https://my-http-bridge.com), the browser blocks the request unless the bridge server explicitly permits it via CORS headers.   

The Preflight OPTIONS Request
Not all cross-origin requests are treated equally. Requests are categorized as "simple" or "preflighted". A simple request (e.g., GET, HEAD, or POST with a standard Content-Type) does not trigger a preflight check. However, a request becomes "preflighted" if it uses other HTTP methods (like PUT or DELETE) or, crucially for agent architectures, if it includes custom HTTP headers such as Authorization or X-API-Key.   

When a preflighted request is made, the browser first sends an OPTIONS request to the target URL. This OPTIONS request asks the server for permission to make the actual request. The server must be configured to handle this OPTIONS request and respond with a 2xx status code along with the appropriate CORS headers:

Access-Control-Allow-Origin: Specifies which origin(s) are allowed to make requests.

Access-Control-Allow-Methods: Lists the allowed HTTP methods (e.g., GET, POST, OPTIONS).

Access-Control-Allow-Headers: Lists the allowed custom headers (e.g., Content-Type, Authorization).

If the server fails to respond correctly to the OPTIONS request, the browser will block the actual request from being sent, resulting in a CORS error in the developer console.   

Robust Mitigation Strategies for the HTTP Bridge
Server-Side Proxy (Recommended): The most robust and secure solution is to configure the web server that hosts the agent's UI to also act as a proxy for API requests. In this setup, the browser sends API requests to a path on the same origin (e.g., /api/tool). The web server receives this request and forwards it to the actual HTTP bridge endpoint (https://my-http-bridge.com/tool). Since the browser's request is to the same origin, the SOP is not violated, and no CORS preflight is necessary.   

Correct Server-Side CORS Headers: If a proxy architecture is not feasible, the HTTP bridge server itself must be configured to handle CORS correctly. This involves implementing logic to respond appropriately to OPTIONS preflight requests and to include the Access-Control-Allow-Origin header in all responses to the client.

Serverless Functions as Intermediaries: A serverless function, such as an AWS Lambda function fronted by an API Gateway, can serve as a scalable and manageable proxy. The API Gateway can be configured to handle CORS preflight requests automatically and forward valid requests to the Lambda function, which then communicates with the tool server.   

4.2. Managing Timeouts in Asynchronous Workflows
AI agent tool calls, especially those involving complex data processing, third-party API interactions, or code execution, can be long-running. A simple, synchronous request-response model is often fragile and prone to timeouts from various sources, including the client's HTTP library, an API Gateway (which often have a 30-second limit), a serverless function (with limits typically up to 15 minutes), or the underlying service itself.

Architectural Patterns for Long-Running Tools
To build resilient agents, it is essential to adopt asynchronous patterns for any tool call that may exceed a few seconds.

Asynchronous Initiation + Webhook: The initial tool call to the HTTP bridge is designed to be fast. It validates the request, initiates the long-running task in the background, and immediately returns a 202 Accepted status code along with a unique task ID. The bridge server processes the task asynchronously. Upon completion, it sends the result to a webhook URL that was provided by the agent in the initial request.

Asynchronous Initiation + Polling: Similar to the webhook pattern, the initial tool call returns a 202 Accepted and a task ID. The agent is then responsible for periodically polling a status endpoint (e.g., /tasks/{id}) until the task's status changes to "completed" or "failed," at which point it can retrieve the result.

Durable Execution Frameworks: For complex, multi-step workflows, leveraging a durable execution framework is the most robust approach. Platforms like Trigger.dev, AWS Step Functions, or Azure Durable Functions are specifically designed to manage long-running, stateful processes. These frameworks handle state checkpointing, automatic retries with exponential backoff, and error handling, abstracting away the complexity of implementing custom webhook or polling logic and effectively eliminating timeout issues.   

4.3. Designing for Agent Self-Correction via Robust Error Handling
For an AI agent to operate autonomously and resiliently, it must be able to understand and recover from failures. The design of the error handling in the HTTP bridge is therefore critical.

The Pitfall of Discarded Error Details
A common implementation mistake is for the HTTP bridge to catch an error from a downstream API—for example, a 422 Unprocessable Entity with a detailed JSON body explaining validation errors—and return only a generic 500 Internal Server Error to the agent. This practice discards the crucial context that the agent needs to understand why the tool call failed.   

Best Practice: Propagate Detailed Errors
The bridge must be designed to capture not only the HTTP status code but also the full response body of any failed downstream tool call. This complete error context should be formatted (e.g., as a JSON string) and returned as the output of the tool call to the AI model. When the model receives a detailed error message like {"detail": [{"loc": ["body", "last_name"], "msg": "Field required"}]}, it can comprehend the specific nature of the failure. This enables a self-correcting loop where the agent can attempt to fix its inputs—for instance, by asking the user for their last name—and retry the tool call intelligently.   

Handling Specific HTTP Error Codes
The agent's logic should be designed to interpret and act upon common, actionable HTTP error codes returned by the bridge:

401 Unauthorized: This indicates that the request lacks valid authentication credentials. The agent should recognize this as a need to trigger a re-authentication flow or prompt the user for credentials.   

403 Forbidden: This means the agent is successfully authenticated, but the provided credentials do not grant permission to perform the requested action. The agent should not retry the request but instead inform the user that they are not authorized for the operation.   

429 Too Many Requests: This signifies that a rate limit has been exceeded. The agent should implement an exponential backoff strategy, waiting for a progressively longer period before retrying the request, to avoid overwhelming the downstream service.   

Part V: Conclusion and Strategic Recommendations
5.1. Synthesis of Findings
This analysis reveals two distinct and powerful architectural philosophies for AI agent tool integration. Anthropic's Model Context Protocol (MCP) establishes a stateful, discovery-based paradigm built on an open protocol. It fosters a decoupled ecosystem where tool providers can run independent servers, but it introduces protocol-level complexity and currently faces challenges with standardized authentication for local tools. In contrast, OpenAI's Responses API offers a stateless, developer-orchestrated model that prioritizes flexibility and control. It requires developers to manage state and orchestration logic within their application but provides a lower barrier to entry for integrating with existing REST APIs.

A key strategic development is OpenAI's decision to incorporate MCP as a supported tool type. This suggests a future not of competition, but of convergence, where MCP could emerge as a de facto interoperability standard for AI tools, and high-level orchestration frameworks like OpenAI's AgentKit will consume them alongside native function calls.

5.2. Actionable Recommendations for Architects
The choice between these two architectures is a strategic one that depends on specific project requirements, team capabilities, and long-term ecosystem goals.

Choose Claude/MCP when:

The primary use case involves building tools for a tightly integrated, local-first development environment, such as IDE or terminal plugins.

The strategic goal is to create a decoupled ecosystem of third-party tool providers who can independently develop and host compliant MCP servers.

Stateful, persistent sessions are a core architectural requirement for the agent's tasks, enabling complex, multi-step reasoning with memory.

Long-term standardization and protocol compliance are valued over the speed of initial implementation.

Choose OpenAI/Responses API when:

Rapid prototyping and integration with a wide range of existing, stateless REST APIs are the main priorities.

The development team prefers to maintain full, granular control over the tool orchestration logic directly within their application code.

The set of required tools is highly dynamic and needs to be injected or modified on a per-request basis, depending on user context or permissions.

Leveraging the broader OpenAI ecosystem, including advanced models and higher-level frameworks like AgentKit, is a strategic priority.

5.3. Future Outlook: Towards a Converged Agentic Architecture
The current landscape presents a clear dichotomy, but the future of agentic architecture is likely to be hybrid. The industry is moving towards greater interoperability, and standards like MCP are poised to play a crucial role. It is plausible that MCP could evolve to become the gRPC or OpenAPI Specification for AI tools—a common language that allows any agent to communicate with any tool.

In this converged future, high-level orchestration frameworks like OpenAI's AgentKit will likely mature to provide developers with a choice. They will be able to connect to simple tools via stateless function calls for rapid development and flexibility, while also being able to establish connections to standardized, stateful MCP servers for enterprise-grade security, reliability, and interoperability. The ultimate architecture will empower architects to select the right integration pattern for the right job, combining the declarative flexibility of OpenAI's model with the protocol-driven robustness of MCP.


Sources used in the report

dysnix.com
Model Context Protocol (MCP) Comprehensive Guide for 2025 ...
Opens in a new window

docs.claude.com
Model Context Protocol (MCP) - Claude Docs
Opens in a new window

claudemcp.com
Claude MCP Community
Opens in a new window

loginov-rocks.medium.com
Build Remote MCP with Authorization | by Danila Loginov - Medium
Opens in a new window

secoda.co
MCP Tool Manifest - Explanation & Examples - Secoda
Opens in a new window

anthropic.com
Desktop Extensions: One-click MCP server installation for Claude Desktop - Anthropic
Opens in a new window

lobehub.com
Browser Tools for Claude Code | MCP ... · LobeHub
Opens in a new window

scottspence.com
Configuring MCP Tools in Claude Code - The Better Way - Scott ...
Opens in a new window

modelcontextprotocol.io
Example Servers - Model Context Protocol
Opens in a new window

docs.claude.com
Remote MCP servers - Claude Docs
Opens in a new window

developers.make.com
Usage with Claude Desktop - MCP Server - Make Developer Hub
Opens in a new window

github.com
No way to send custom headers in claude_desktop_config.json ...
Opens in a new window

reddit.com
How to pass credentials to a SSE MCP server? Or how to build multi-user MCP servers?
Opens in a new window

support.claude.com
Getting Started with Custom Connectors Using Remote MCP - Claude Help Center
Opens in a new window

reddit.com
Built a bridge to continue Claude Code conversations from my phone via Telegram - Reddit
Opens in a new window

repost.aws
How can I pass custom headers from the frontend to the Agent or MCP running in AgentCore, and how can I read those headers within the Agent or MCP? | AWS re:Post
Opens in a new window

code.visualstudio.com
MCP developer guide | Visual Studio Code Extension API
Opens in a new window

platform.openai.com
Assistants API deep dive Deprecated - OpenAI Platform
Opens in a new window

platform.openai.com
Assistants API tools - OpenAI Platform
Opens in a new window

learn.microsoft.com
Azure OpenAI Responses API - Microsoft Learn
Opens in a new window

community.openai.com
New tools for building agents: Responses API, web search, file ...
Opens in a new window

community.openai.com
Is there a future for the Assistants API? - OpenAI Developer Community
Opens in a new window

platform.openai.com
Function calling - OpenAI API
Opens in a new window

medium.com
OpenAI API's and Tools Calling - Medium
Opens in a new window

ai.google.dev
OpenAI compatibility | Gemini API - Google AI for Developers
Opens in a new window

lmstudio.ai
OpenAI Compatibility API | LM Studio Docs
Opens in a new window

ai-sdk.dev
AI SDK Providers: OpenAI
Opens in a new window

learn.microsoft.com
How to switch between OpenAI and Azure OpenAI endpoints with Python - Microsoft Learn
Opens in a new window

docs.pay-i.com
OpenAI Custom Headers - Getting Started with Pay-i
Opens in a new window

openai.com
Introducing AgentKit - OpenAI
Opens in a new window

platform.openai.com
API Reference - OpenAI Platform
Opens in a new window

contentstack.com
Understanding and Resolving CORS Error - Contentstack
Opens in a new window

requestly.com
What is CORS and how to bypass it? - Requestly
Opens in a new window

dev.to
Fixing CORS in your SPA - DEV Community
Opens in a new window

dev.to
CORS, Preflight Requests, and Common Cross-Origin Issues - DEV Community
Opens in a new window

developer.mozilla.org
Cross-Origin Resource Sharing (CORS) - HTTP - MDN
Opens in a new window

medium.com
Understanding CORS and Preflight Requests in APIs | by Bale - Medium
Opens in a new window

dev.to
I got a CORS error, now what? - DEV Community
Opens in a new window

developer.mozilla.org
Preflight request - Glossary - MDN - Mozilla
Opens in a new window

stackoverflow.com
Confused about how to handle CORS OPTIONS preflight requests - Stack Overflow
Opens in a new window

community.openai.com
Cross-Origin Resource Sharing (CORS) - API - OpenAI Developer Community
Opens in a new window

trigger.dev
Trigger.dev | Build and deploy fully-managed AI agents and workflows.
Opens in a new window

medium.com
Handling HTTP Errors in AI Agents: Lessons from the Field | by Pol Alvarez Vecino | Medium
Opens in a new window

developer.mozilla.org
401 Unauthorized - HTTP - MDN - Mozilla
Opens in a new window

en.wikipedia.org
List of HTTP status codes - Wikipedia
Opens in a new window

permit.io
401 vs. 403 Error Codes: What's the Difference? When to Use Each? (Updated 2024)
Opens in a new window

docs.apigee.com
HTTP status codes | Apigee Edge