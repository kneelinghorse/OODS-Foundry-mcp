Expert Report: Multimodal AI Mechanisms and Comparative Efficacy in Visual Design Research
I. Introduction: The MLLM Paradigm in Visual Research
1.0. Overview of Multimodal AI in the Creative and Research Ecosystems
The integration of Artificial Intelligence (AI) into graphic design (AIGD) marks a fundamental shift from specialized, atomic models toward unified Multimodal Large Language Models (MLLMs). Historically, AI efforts in design relied on decomposing tasks into isolated components, such as employing specialized models solely for typography generation , layout optimization, or color palette recommendation. While effective for generating individual elements, this decompositional approach introduced systemic fragmentation that complicated holistic design analysis.   

The emergence of MLLMs has catalyzed a transition, offering a framework capable of processing and interpreting complex, multisensory information simultaneously. These models can comprehend the full context of interleaved data‚Äîtext, images, audio, and increasingly, video‚Äîmaking them highly effective for applications such as content analysis, automated video captioning, and interactive media creation. This ability to interpret complex, multisensory information is critical in today‚Äôs data environment, particularly in media, entertainment, and communications. Forecasts by major consulting firms underscore the profound economic impact of this transition, suggesting that generative AI in graphic design could contribute more than USD 8 trillion to the global economy by 2030.   

The core challenge for MLLMs in the creative sector is the progression beyond simple perception tasks‚Äîsuch as object classification or recognition‚Äîinto complex, creative interpretation tasks. This includes achieving sophisticated aesthetic and semantic understanding and comprehensive layout analysis. MLLMs aim to bridge the gap between localized visual features and the overarching, global intent of a design.   

1.1. Defining Visual Research in the Age of MLLMs
In contemporary R&D and UX contexts, visual research is no longer confined to cataloging images. It now involves the automated, fine-grained analysis of specific design elements‚Äîcolors, typography, spacing‚Äîand the interpretation of abstract concepts related to human behavior, such as user motivations, perceived emotional context, and the Jobs to Be Done (JTBD) framework.

The integrated approach of MLLMs allows researchers to input complex visual artifacts, such as annotated mockups or demographic photography, alongside textual queries, resulting in a model that can comprehend the full context of the visual input. This integrated understanding is vital for analyzing visual data holistically, catering to different learning styles, and improving the effectiveness of complex analytical tools in educational and professional settings.   

1.2. Architectural Foundation: The Synergy of Vision Transformers (ViTs) and LLMs
The foundational technology driving the sophisticated visual processing capabilities of MLLMs is the Vision Transformer (ViT) architecture. ViTs adapt the breakthrough transformer architecture, originally optimized for Natural Language Processing (NLP), to handle computer vision tasks by converting an image into a sequence of small patches.   

This architectural adaptation allows ViTs to leverage the powerful self-attention mechanism, enabling the modeling of long-range dependencies and global relationships within images with greater efficacy than traditional Convolutional Neural Networks (CNNs). Key benefits include superior global feature modeling, greater efficiency when scaling to very large model sizes, and strong transfer learning capabilities when pre-trained on massive datasets, allowing them to excel in tasks like object detection and image segmentation. The ViT architecture typically consists of four main components: patch embedding, positional encoding, the transformer encoder, and a classification head.   

A significant challenge arises, however, in the optimization of these visual components. Unlike CNNs, ViTs operate primarily on token embeddings and possess minimal inherent inductive bias. The empirical evidence shows that directly adapting widely used Neural Architecture Search (NAS) techniques, standard in CNN optimization, often leads to catastrophic failures and frustrating instability when training ViT 'superformers'. The instability stems from the imbalance of channels across different candidate architectures, which worsens the weight-sharing assumptions intrinsic to the NAS process.   

Addressing this architectural instability requires complex engineering solutions. One such approach involves developing a new cyclic weight-sharing mechanism specifically for the token embeddings of ViTs, ensuring that each channel contributes more evenly to all candidate architectures. Additionally, techniques like identity shifting are proposed to alleviate the "many-to-one" issue in the superformer model, alongside the use of weak augmentation and regularization for steady training. The necessity of developing specialized mechanisms such as ViTAS, which achieved significant performance superiority with improved accuracy on datasets like ImageNet-1k, demonstrates that reliable visual analysis is highly dependent on stabilizing and optimizing the foundational visual encoder. This required architectural tuning confirms that the visual component of MLLMs is technically less mature than the language component, and its reliability hinges on resolving fundamental issues related to accurate visual representation learning.   

II. Core Mechanics of AI Visual Analysis: Architecture and Grounding
2.0. Architectural Evolution: From Cascaded Systems to Native Multimodality
The competitive landscape of MLLMs is characterized by two distinct architectural philosophies: cascaded (dense) and native (sparse) systems. Earlier powerful models, such as predecessors to GPT-4, utilized dense transformer architectures where vision modules were often "tacked on" to existing language models, leveraging complex, separate neural network types (e.g., CNNs for images, transformers for text) that were aligned in a shared feature space.   

In contrast, Google's Gemini models employ a natively multimodal, sparse Mixture-of-Experts (MoE) Transformer architecture. This MoE design is crucial because it dynamically routes input tokens to specific expert subnetworks, activating only a subset of parameters for any given input token. This provides a substantial advantage in computational efficiency, allowing the model to achieve massive total capacity‚Äîbillions of parameters spread across experts‚Äîwithout a proportional increase in the computational cost per token. This architecture, where the model is pre-trained from the ground up on text, images, audio, and video together, facilitates joint reasoning across modalities more effectively than systems combining separate networks.   

DeepMind‚Äôs Flamingo model illustrates one implementation strategy for this integrated design. It processes visual data through a pretrained, frozen image encoder to extract embeddings, which are then passed through a Perceiver Resampler. These fixed image embeddings and text tokens are fed into gated cross-attention dense blocks, which are inserted between the frozen LLM blocks and trained from scratch. This integrated approach allows these natively multimodal models to excel in open-ended tasks, such as visual dialogue and generating free-form text based on interleaved image and text inputs.   

2.1. The Mechanism and Challenge of Visual Grounding (VG)
Visual Grounding (VG) is the core technical capability enabling detailed design analysis. The objective of VG is to accurately localize a specific object in an image based on a corresponding natural language description. This is essential for applications requiring precise spatial context, such as analyzing the contrast of a specific user interface element or measuring the exact margin between two components. The latest VG methods, such as DETR-based approaches, aim to predict the target object's coordinates directly, bypassing the need for pre-generated proposal candidates.   

Despite the advancements in MLLMs, a persistent and critical limitation is the shortfall in robust localization capabilities. This constraint prevents models from fully grounding their understanding to the precise visual context, thereby limiting their potential in real-world applications such as augmented reality or autonomous systems.   

A systematic shortcoming in MLLM visual capabilities is largely traceable to their dependence on CLIP (Contrastive Language-Image Pre-training). Research reveals that the visual embedding space generated by CLIP can exhibit inherent flaws, leading to "CLIP-blind pairs"‚Äîimages that CLIP treats as similar despite clear and obvious visual differences. This systematic visual misunderstanding introduces fundamental errors in the model‚Äôs foundational visual processing. When models, including state-of-the-art systems like GPT-4V, encounter these visual patterns, they often provide incorrect answers and "hallucinated explanations" for straightforward visual questions. The inherent challenge of visual representation learning remains an open problem, and resolving these systematic flaws by integrating features from vision self-supervised learning (for example, through a Mixture of Features or MoF approach) is necessary to enhance visual grounding accuracy in future multimodal systems. Accurate visual grounding is therefore understood to be a prerequisite for successful multimodal systems.   

2.2. Advanced Visual Search Methods for Design Workflows
2.2.1. Visual Prompting Techniques
Given the identified intrinsic limitations in MLLM localization, visual prompting has become an essential technique for enforcing high-precision analysis in design research. Visual prompts serve as explicit guidance mechanisms for MLLMs, helping them interpret and process specific visual data. These prompts can take several forms, including bounding boxes, markers, pixel-level inputs, or soft prompts. They provide crucial additional information that substantially enhances a model's visual perception capabilities.   

In practice, this method can be implemented by adapting techniques such as overlaying bright numeric markers at the center of user-defined regions of interest. When a marked image is provided as input, it explicitly "unleashes the grounding capabilities" of models like GPT-4V, enabling it to easily reference specific image regions by addressing the corresponding numbers. This technique effectively shifts the burden of high-precision localization away from the MLLM's internal (and potentially flawed) visual grounding mechanism to the external guidance provided by the human researcher.   

For critical design tasks demanding pixel-level accuracy‚Äîsuch as checking for alignment discrepancies, analyzing precise margins, or verifying specific visual weights‚Äîthe manual injection of explicit coordinates or markers via visual prompting is currently indispensable for achieving reliable and verifiable outputs. This process acts as a necessary external layer that compensates for the models' current localization deficits.   

2.2.2. Multimodal Retrieval Augmented Generation (MM-RAG)
Multimodal Retrieval Augmented Generation (MM-RAG) systems are critical for processing modern documents that combine diverse content‚Äîinterleaved text, images, tables, equations, and charts‚Äîwhich traditional text-focused RAG systems cannot process effectively. MM-RAG systems provide a comprehensive processing framework that ensures non-textual details are preserved and retrievable.   

The complexity of MM-RAG implementation is categorized into three evolving approaches:

Text-translation approach: The simplest method, which converts all non-text data (e.g., images) into text (e.g., captions or descriptions) at the time of storage and retrieval.   

Text retrieval with multimodal generation: A more advanced approach that performs retrieval over text embeddings (captions, metadata) but allows an MLLM to access the original non-text data directly during response generation.   

Multimodal retrieval: The most advanced approach, which utilizes true multimodal embeddings. These embeddings, created using models like Convolutional Neural Networks (CNNs) for images or transformers for text, are stored in a shared or aligned feature space. This alignment, often achieved through contrastive learning with paired data (image-caption), enables sophisticated cross-modal semantic matching.   

Tools such as RAG-Anything offer an all-in-one comprehensive solution, addressing the challenge of unifying processing and querying across all content modalities within a single, integrated framework. This system includes specialized content analysis for images, tables, and mathematical equations, alongside hybrid intelligent retrieval capabilities that span textual and multimodal content with contextual understanding. The utilization of vision language models (VLMs) within these pipelines enhances retrieval performance by generating detailed image summaries and captions, which enrich the vector store with textual descriptions of visual content.   

III. Comparative Model Performance: Gemini, GPT, and Deep Search LLMs
3.0. Establishing Multimodal Benchmarks for Visual Systems
Evaluating MLLMs requires specialized metrics that move beyond traditional NLP evaluations to address the complexities of cross-modal integration and processing. Benchmarks such as MMMU-Pro and Video-MMMU are essential for assessing a model‚Äôs integrated understanding and ability to reason simultaneously across different data types.   

A significant challenge in evaluation is reliably measuring the alignment between visual output and complex textual prompts, particularly given that simple metrics like CLIPScore can fail when dealing with prompts involving compositions of objects, attributes, and relations. For instance, a text encoder might treat "the horse is eating the grass" and "the grass is eating the horse" identically, viewing the prompt as a mere "bag of words". To address this, the VQAScore was introduced. It uses a Visual-Question-Answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple question: "Does this figure show '{text}'?". The VQAScore, computed even with off-the-shelf VQA models, produces state-of-the-art results across numerous image-text alignment benchmarks. Other benchmarks, like CV-Bench, focus specifically on 2D and 3D visual understanding, recognizing the scarcity of vision-centric evaluations.   

3.1. General Multimodal Understanding and Reasoning (MMMU-Pro)
The MMMU-Pro benchmark is recognized as a key test for integrated multimodal understanding and reasoning. Recent evaluations demonstrate a significant performance differential between proprietary MLLMs, particularly validating the advantages of a natively multimodal architecture.

Gemini 3 Pro, built upon a sparse Mixture-of-Experts architecture, exhibits superior performance on this metric. The model scored 81.0% on MMMU-Pro, establishing a substantial 5-point performance gap ahead of its competitor, GPT-5.1, which scored 76.0%. This performance validates the technical superiority of Gemini‚Äôs integrated design, which was pre-trained on text, images, and other modalities concurrently.   

The implication for R&D and UX professionals is clear: for complex tasks requiring the interpretation of richly detailed documents, such as technical manuals or research reports containing interleaved visual diagrams, annotated charts, and dense text, the technical advantages of native multimodality provide a material edge. Gemini 3 Pro‚Äôs ability to reason jointly across modalities makes it the technically superior choice for deep, single-artifact analysis where cross-modal context is paramount.

3.2. Performance in Complex and Relational Tasks (MIRB and Logical Reasoning)
While MLLMs show strengths in integrated analysis, substantial gaps remain in processing relational visual data. Existing benchmarks for Vision Language Models (VLMs) have historically focused almost exclusively on single-image inputs, overlooking the crucial challenge of multi-image understanding.   

The Multi-Image Relational Benchmark (MIRB) was introduced to specifically evaluate a model's ability to compare, analyze, and reason across multiple images. MIRB encompasses four categories: perception, visual world knowledge, reasoning, and multi-hop reasoning. Comprehensive evaluations of state-of-the-art models, including GPT-4V, revealed a systematic difficulty in handling this benchmark. While open-source VLMs approach the performance of GPT-4V on single-image tasks, a significant performance deficit remains in multi-image reasoning. Furthermore, attempting simple aggregation methods, such as concatenating multiple images into a single input, generally resulted in performance degradation. The most challenging MIRB categories for open-source models were visual world knowledge and multi-hop reasoning.   

This systematic difficulty with complex comparative analysis indicates a significant limitation for design workflows, which frequently rely on reviewing and comparing multiple iterations, A/B test results, or competitive audit screenshots. Since MLLMs struggle to achieve high performance on the MIRB, automated comparative visual reasoning is currently unreliable. This requires that human researchers must manually verify all comparative claims generated by MLLMs, thereby constraining the potential for fully autonomous design evaluation systems.

In contrast to visual reasoning, for pure logical and structured tasks, especially those involving complex word problems or multi-step mathematical questions, GPT-4 exhibits slightly more intelligence and problem-solving skills. Although Claude 3 performs well, it falls slightly behind GPT-4 in accuracy on advanced tasks. Gemini has been observed to struggle sometimes with understanding instructions in complex logical reasoning scenarios.   

The following table summarizes the comparative performance of leading MLLMs on key multimodal benchmarks relevant to design research:

Table 1: Comparative Multimodal Reasoning Benchmark Scores and Design Relevance

Benchmark	Gemini 3 Pro Score	GPT-5.1/4V Score	Key Challenge Evaluated	Design Research Relevance	Citation
MMMU-Pro	81.0%	76.0% (GPT-5.1)	Complex, integrated multimodal understanding	Interpreting charts, diagrams, and annotated mockups in research reports.	
MIRB	N/A (Struggles)	Struggles (Low Accuracy)	Comparative reasoning across multiple images (Multi-hop)	A/B testing, competitive audits, design iteration comparisons.	
Logical Reasoning (Math Arena)	Variable (Lower than GPT)	High (Excels)	Advanced mathematical tasks, complex instructions	Structured analysis, verifying complex rule sets.	
  
3.3. Evaluating Deep Search LLMs for Visual Data Collection
A distinct category of AI tools, defined by their deep search capabilities, prioritizes information volume and retrieval speed, often featuring highly responsive performance. Grok Deep Search, for example, has been observed to be approximately 10 times faster than ChatGPT Deep Research, searching approximately 3 times more webpages. Claude Deep Search is also recognized for its responsiveness.   

However, this speed and volume often comes at the expense of integrated MLLM analysis. In one comparative analysis, Gemini, despite its superior multimodal reasoning architecture, was observed to be slower, researching 62 sources over 15 minutes, compared to the faster search tools. This suggests a fundamental trade-off: high-speed, high-volume aggregation versus slower, deeper, multimodal reasoning capacity.   

Perplexity's approach demonstrates a successful specialization in visual search for product identification, aligning with modern e-commerce and visual shopping trends. Perplexity‚Äôs visual search tool, Snap to Shop, efficiently scans images, identifying key features such as color, shape, and texture. This approach quickly delivers lists of matched or similar products, complete with detailed specifications and pricing. This rapid, specialized visual search aligns with user behavior, as images are processed 60,000 times faster than text, and visual search is favored by a majority of consumers.   

For strategic deployment, the choice of tool depends entirely on the mission objective. For tasks requiring rapid trend spotting, high-volume data aggregation, or market surveillance of visual competitive landscapes, high-speed deep search tools (Grok, Claude) are generally more appropriate. Conversely, when the mission requires in-depth, integrated multimodal analysis of a specific artifact (e.g., interpreting a dense technical diagram), the superior reasoning capabilities of MLLMs like Gemini 3 Pro or GPT are necessary.   

IV. Application in Visual Design Research: Element-Specific Analysis
4.0. AI‚Äôs Role in Design Interpretation and Generation
The academic evolution of AI in Graphic Design (AIGD) has always aimed to bridge the gap between localized visual features and the overarching global design intent. MLLMs facilitate this by enabling both perception tasks (understanding and analyzing existing design elements) and generation tasks (creating new elements and layouts). The success of AI in design interpretation relies on its capacity to quantify and formalize principles traditionally considered subjective.   

4.1. Design Aesthetics and Semantic Understanding
AI‚Äôs capacity to evaluate aesthetic qualities is rapidly advancing, moving beyond simple classification to nuanced semantic evaluation. Research confirms that achieving aesthetic appeal, often through techniques like color harmony and visual balance, is a critical component of design evaluation. Models such as Claude Opus 4.1 have been specifically noted for their ability to assess aesthetic qualities within images, demonstrating an extension of capabilities from basic recognition to deeper evaluative understanding.   

Aesthetic assessment is now formalized through dedicated benchmarks. These systems evaluate aesthetic perception using specialized metrics such as IFD (Image Fidelity Disagreement) and NRD (Neural Response Disagreement), with scores typically ranging from 1 to 5. Furthermore, proprietary models like GPT-4o have achieved state-of-the-art performance, scoring highly (88.58) on instruction-following aesthetic benchmarks such as MIA-Bench, significantly outpacing the best open-source models (79.84). The strong correlation observed between the size of the underlying language model and performance on these instruction-following capabilities validates the scaling law in aesthetic alignment.   

Regarding color analysis, MLLMs, sometimes integrated into specialized tools (e.g., ColorGPT variants), show considerable potential for tasks like color palette completion and analysis. These models demonstrate adaptability in maintaining color harmony even in complex graphic documents by analyzing physical characteristics such as lightness contrast, hue differences, and the spatial density of color blocks. Generally, lower lightness contrast and cooler color combinations are observed to increase the perception of harmony in multi-color combinations.   

4.2. Layout, Spacing, and Visual Balance
One of the most valuable capabilities of AI in visual research is its capacity to transform subjective layout critique into objective, prescriptive recommendations by quantifying spatial relationships. AI systems analyze layout using quantifiable parameters derived directly from established graphic design and accessibility guidelines.

For spacing and alignment, the AI utilizes coordinate analysis to detect precise alignment, flagging misalignments of even a few pixels. It employs proximity rules, interpreting elements closer than 30 pixels as belonging together, while elements separated by over 100 pixels are recognized as forming distinct groups, thereby establishing the structural hierarchy of a design. Furthermore, AI verifies adherence to accessibility standards, such as the Web Content Accessibility Guidelines (WCAG), by comparing foreground and background colors and calculating the contrast ratio, with 4.5:1 or higher passing readability standards.   

For the complex analysis of visual balance, automated layout systems utilize a specialized approach based on WeightMaps. A WeightMap is a bitmap representation that encodes the perceived visual weight of every object within a presentation. Algorithms then employ image-processing techniques, including pyramids and edge detection, to efficiently analyze the WeightMap for balance. Derivatives of the sums of the rows and columns are used to generate suggestions for layout improvement. This approach is fundamentally based on the concepts of visual weight and balance, which are foundational to the visual arts.   

The systematic transformation of subjective evaluation into prescriptive output represents a significant capability boost for design iteration. Instead of generic feedback, the AI generates specific, measurable instructions, such as: "Increase spacing between the logo and first input field to 800 pixels for better visual breathing room" or "Reduce spacing between input fields to 60 pixels to show their relationship as a group". This capability is highly valuable because it delivers rapid, objective design critique anchored to measurable metrics. The successful implementation of WeightMaps and coordinate analysis confirms that AI's effectiveness in layout analysis stems from its ability to convert complex spatial data into algorithmic metrics, mitigating its current weakness in tasks that require abstract spatial imagination.   

4.3. Typography and Visual Element Analysis
MLLMs demonstrate high proficiency in visual classification tasks necessary for analyzing visual elements. These tasks include image classification, object detection, various forms of image segmentation (instance and semantic), and Optical Character Recognition (OCR). The capability for OCR‚Äîextracting text and semantic information from images‚Äîis increasingly integrated into modern multimodal benchmarks.   

However, the analysis of typography presents a dichotomy between efficiency and aesthetic reliability. While AI excels at rapid classification and semantic extraction, it currently lacks the intuitive understanding of good design principles required for holistic aesthetic evaluation. AI-generated typography may not consistently account for essential parameters such as legibility, readability, and consistency. This lack of intuitive understanding of design principles means that designs generated or analyzed solely by AI can result in aesthetically unpleasing or inconsistent outcomes.   

Therefore, while AI can significantly speed up the processing and categorization of text elements, human quality assurance remains necessary to maintain creative control, ensuring that the critical typographic details resonate effectively with the intended audience.   

Table 2: AI Capabilities in Specific Visual Design Analysis

Design Research Component	AI Assessment Technique	Primary Function/Metric	Model Performance Trait	Key Limitation
Layout/Spacing	WeightMaps, Coordinate Analysis	Visual balance, proximity rules (30/100px), WCAG contrast ratio (4.5:1)	High accuracy on quantifiable metrics; prescriptive critique.	
Struggles with abstract spatial imagination.

Color Harmony	Pretrained LLMs (ColorGPT), Contrastive Learning	Lightness/tone contrast, hue differences, aesthetic appeal scoring.	
Effective for completion/suggestion tasks; increasing sophistication in scoring.

Requires human validation for complex cultural/emotional context.
Typography	OCR, Visual Element Classification	Semantic extraction, classification, font recognition.	Strong classification, good for efficiency.	
Lack of intuitive understanding of legibility, consistency, or emotional context.

Design Aesthetics	MMMU-Pro, MIA-Bench, Specific scoring systems	Integrated reasoning, abstract aesthetic quality assessment (e.g., Claude Opus 4.1).	
Rapidly closing the gap, but prone to systematic visual shortcomings.

Still reliant on existing data; cannot make subjective value judgments.

  
V. Advanced UX Research: Personas, JTBD, and Emotional Context
5.0. AI in Analyzing Abstract and Contextual User Data
UX research requires analyzing abstract concepts, such as user motivations and behavioral drivers, often expressed through the Jobs to Be Done (JTBD) framework or traditional personas. MLLMs are equipped to handle this complexity by combining formal symbolic reasoning, derived from textual data, with visual reasoning, which allows for the manipulation and interpretation of complex visual information. This synergistic capability creates a robust framework for addressing challenges that neither modality could fully address alone.   

The capability of MLLMs to generate instruction data themselves through self-instruction, by prompting auxiliary models or using chain-of-thought processes to create new tasks and answers, enhances the dataset size without heavy manual labeling. This self-generation capability, combined with blending conversation data with instruction data, allows MLLMs to build strong dialog and reasoning skills necessary for understanding complex user research prompts.   

5.1. Translating Visual Cues into User Motivations (JTBD)
User experience frameworks like JTBD emphasize understanding the user's motivations, the triggers that precede an action, and the ultimate goal the person is attempting to accomplish. To translate visual data into these complex psychological insights, MLLMs must possess sophisticated visual reasoning capabilities.   

Advanced research indicates that MLLMs are now capable of constructing verifiable, evidence-based reasoning chains that link subtle visual details to a logical conclusion. This involves identifying minute visual clues‚Äîsuch as faint architectural patterns, small textual signs, or subtle environmental details‚Äîand systematically connecting these observations to generate insights.   

For UX research, where visual data often includes contextual ethnographic photos or screenshots of a user's environment, this fine-grained detail identification and reasoning chain generation allows the MLLM to systematically extract contextual triggers relevant to JTBD. For instance, by identifying environmental factors in a photograph, the MLLM can infer structural constraints or technological limitations that necessitate specific user behavior, thus linking a visual cue to a fundamental "Job" the user is trying to accomplish. This capacity moves AI beyond simple descriptive image captioning toward complex sociological and behavioral inference.   

Furthermore, research demonstrates that MLLMs can be designed with an intrinsic motivation mechanism that encourages them to autonomously revisit visual information during the reasoning process. This capability effectively mirrors how human researchers iteratively review and reflect on visual evidence to confirm or refine their hypotheses, without requiring the explicit re-injection of images by the user.   

5.2. Limitations in Affective and Emotionally Intelligent Design
While MLLMs excel at structured analysis, a significant limitation persists in their ability to engage in emotionally intelligent design, which is crucial for strengthening user engagement and loyalty. AI systems lack the inherent human ability to make intuitive leaps or generate insights based on subconscious processes, personal experiences, and subjective value judgments.   

AI cannot make subjective value judgments or aesthetic choices because these necessitate the subjective experiences and emotions that machines fundamentally do not possess. This limitation affects core design tasks, such as generating typefaces that feel aesthetically right or creating designs that resonate with specific cultural or emotional contexts.   

Existing systems can classify emotional responses. For example, multimodal affective interfaces utilize PAD (Pleasure-Arousal-Dominance) dimensional models of emotion fused with physiological signal processing to classify emotional responses associated with aesthetic impressions in real-time. However, this is strictly emotion classification‚Äîthe recognition of a state‚Äîrather than true empathetic understanding.   

The reliance on AI for creative tasks introduces a risk of the potential loss of the "unique, human touch". If designers become overly dependent on automated templates and styles, the result can be generic, monotonous designs that fail to connect with target audiences because AI lacks the cultural or emotional context necessary for empathy. Therefore, the human designer remains the critical interpreter for affective data, ensuring that metaphor integration, cultural context, and subjective aesthetic choices align with human values and emotional needs.   

VI. Critical Challenges and Strategic Outlook
6.0. Systemic Failures in Current MLLMs
Despite the rapid advancements in MLLM capabilities, several systemic failures currently limit their potential for fully autonomous and reliable visual research, particularly in complex design environments.

6.1. Spatial Understanding Deficits and Visual Shortcomings
A significant and persistent limitation is the MLLMs' struggle with spatial understanding, which is essential for perception, reasoning, and planning in complex environments. Current studies reveal that performance in spatial understanding converges quickly as training data increases, indicating that merely scaling up data is insufficient to achieve satisfactory results. The performance upper bound remains relatively low, particularly for tasks that require genuine spatial imagination‚Äîthe ability to visualize and manipulate spatial concepts. This suggests that architectural tuning, rather than brute-force data scaling, is the path forward for improving spatial reasoning, which relies more heavily on positional encoding within the visual encoder than the language model.   

Compounding this issue, MLLMs exhibit a general performance disparity compared to human capabilities, even on straightforward visual questions. Most models perform below the level of random guessing on simple visual patterns, and even state-of-the-art models like GPT-4V exhibit a performance gap exceeding 50% relative to human performance. This demonstrates that systematic shortcomings in foundational visual capabilities, exacerbated by issues like "CLIP-blindness" , fundamentally constrain the reliability of MLLMs in visual perception tasks.   

6.2. The Persistent Threat of Hallucination in Visual Contexts
Hallucination in Vision Language Models (VLMs) is a significant risk, defined as the generation of plausible falsehoods that are not grounded in the training data or the prompt. For visual design research, this manifests as inaccurate interpretations of visual evidence or the generation of factually incorrect claims about a design artifact.   

One common mechanism causing hallucination is source-reference divergence in the training data. This divergence, often an artifact of heuristic data collection, encourages the model to generate text that is not faithful to the provided visual source. In a design context, this compromises interpretability and fact-checking, making it crucial to ensure that all generated outputs are visually grounded and verifiable.   

6.3. Conclusion and Strategic Recommendations for Design Research Integration
The analysis confirms that MLLMs are transforming visual research by integrating complex reasoning with visual perception, but they are not yet capable of superseding human intuition, creativity, or comprehensive emotional understanding. The current capabilities necessitate a strategy of hybrid intelligence, where AI acts as a supplementary, powerful analytical engine guided by expert human prompts and oversight.   

Strategic Recommendation 1: Model Selection by Task Specialization The optimal MLLM choice depends heavily on the specific research task:

Gemini 3 Pro: The superior performance on the MMMU-Pro benchmark (81.0%) makes it the primary recommendation for tasks requiring complex, integrated multimodal reasoning, such as interpreting technical documentation, complex data visualizations, or richly annotated mockups.   

GPT-4V/GPT-5: Recommended for high-stakes, structured analysis, logical reasoning, and tasks requiring verification against specific rule sets (e.g., WCAG compliance, or complex layout rule application), where its established strength in logical processing is valuable.   

Deep Search LLMs (Grok, Claude): Recommended for rapid data aggregation, time-sensitive trend spotting, and high-volume market surveillance of visual trends where speed and source breadth are prioritized over deep, integrated analysis.   

Strategic Recommendation 2: Mitigate Visual Grounding Risk Due to the systematic localization deficits inherent in MLLMs (the CLIP-blindness bottleneck), visual prompts are mandatory for high-precision design analysis. Researchers must utilize explicit external controls, such as bounding boxes or numeric markers, to provide explicit spatial context, thereby mitigating grounding risk and enforcing verifiable reference points for analysis. Furthermore, implementing advanced Multimodal Retrieval Augmented Generation (MM-RAG) pipelines is essential for documents containing mixed content, ensuring that every claim can be traced back to its specific, verifiable, and visually preserved source.   

Strategic Recommendation 3: Human Oversight for Affective and Aesthetic Design AI must not be relied upon for tasks requiring subjective judgment, emotional intelligence, cultural context, or the creation of truly original, non-generic designs. The human designer must retain critical control over final typography selections, persona creation, and the interpretation of abstract aesthetic and emotional concepts. While AI can classify emotional cues, the empathetic interpretation required for emotionally intelligent design must be anchored in human intuition to ensure the final product resonates authentically with the target audience.   


pmc.ncbi.nlm.nih.gov
From Fragment to One Piece: A Review on AI-Driven Graphic Design - PMC
Opens in a new window

azure.microsoft.com
What are multimodal LLMs? - Microsoft Azure
Opens in a new window

codecademy.com
How do Vision Transformers Work? Architecture Explained - Codecademy
Opens in a new window

ecva.net
ViTAS: Vision Transformer Architecture Search
Opens in a new window

macaron.im
Gemini 3 vs ChatGPT‚Äë4 vs Claude 2: A Comprehensive Comparison - Macaron AI
Opens in a new window

research.aimultiple.com
Large Multimodal Models (LMMs) vs LLMs - Research AIMultiple
Opens in a new window

neptune.ai
Multimodal Large Language Models - Neptune.ai
Opens in a new window

proceedings.neurips.cc
Referencing Where to Focus: Improving Visual Grounding with Referential Query - NIPS papers
Opens in a new window

ecva.net
Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models - European Computer Vision Association
Opens in a new window

ieeexplore.ieee.org
Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs - IEEE Xplore
Opens in a new window

arxiv.org
Visual Prompting in Multimodal Large Language Models: A Survey - arXiv
Opens in a new window

github.com
HKUDS/RAG-Anything: "RAG-Anything: All-in-One RAG Framework" - GitHub
Opens in a new window

ibm.com
What is Multimodal RAG? - IBM
Opens in a new window

galileo.ai
Multimodal LLM Evaluation: Overcoming Challenges - Galileo AI
Opens in a new window

vellum.ai
Google Gemini 3 Benchmarks - Vellum AI
Opens in a new window

arxiv.org
[2404.01291] Evaluating Text-to-Visual Generation with Image-to-Text Generation - arXiv
Opens in a new window

arxiv.org
MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs - arXiv
Opens in a new window

the-decoder.com
Analysts say Google now leads the AI performance race with Gemini 3 Pro
Opens in a new window

arxiv.org
[2406.12742] Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning - arXiv
Opens in a new window

researchgate.net
Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning | Request PDF - ResearchGate
Opens in a new window

themoonlight.io
[Literature Review] Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning - Moonlight | AI Colleague for Research Papers
Opens in a new window

favourkelvin17.medium.com
Claude 3 vs GPT-4 vs Gemini: Which is Better in 2024? | by Favour Kelvin - Medium
Opens in a new window

research.aimultiple.com
AI Deep Research: Claude vs ChatGPT vs Grok - Research AIMultiple
Opens in a new window

pageon.ai
Mastering Visual Search with Perplexity's Snap to Shop Tool - PageOn AI
Opens in a new window

brightedge.com
The Ultimate Guide to Perplexity - BrightEdge
Opens in a new window

bairesdev.com
The Ultimate AI Test: ChatGPT vs. Gemini vs. Perplexity vs. Copilot vs. Claude ‚Äì Who's the Smartest? - BairesDev
Opens in a new window

arxiv.org
From Fragment to One Piece: A Survey on AI-Driven Graphic Design - arXiv
Opens in a new window

aclanthology.org
AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment - ACL Anthology
Opens in a new window

arxiv.org
ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation
Opens in a new window

frontiersin.org
Attribute analysis and modeling of color harmony based on multi-color feature extraction in real-life scenes - Frontiers
Opens in a new window

medium.com
How AI Sees Your Design: The Complete Process From Upload to Understanding - Medium
Opens in a new window

scispace.com
Evaluation of Visual Balance for Automated Layout | SciSpace
Opens in a new window

arxiv.org
[2509.02359] Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture - arXiv
Opens in a new window

ibm.com
What is a Multimodal LLM (MLLM)? - IBM
Opens in a new window

medium.com
AI Reshaping Typography: A Critical Analysis | ùêÄùêà ùê¶ùê®ùêßùê§ùê¨.ùê¢ùê® - Medium
Opens in a new window

nicolesteffen.com
The Limits of Artificial Intelligence in Graphic Design: Why AI Cannot Be Truly Creative...YET
Opens in a new window

dscout.com
A 5 Step Framework for Building Better Personas - Dscout
Opens in a new window

youtube.com
How to Use Jobs to Be Done with User Personas - YouTube
Opens in a new window

arxiv.org
Can MLLMs Reason in Multimodality? : An Enhanced MultiModal ReAsoning Benchmark
Opens in a new window

arxiv.org
VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence - arXiv
Opens in a new window

arxiv.org
Look-Back: Implicit Visual Re-focusing in MLLM Reasoning - arXiv
Opens in a new window

uxmag.medium.com
The Role of Emotion in UX: Embracing Emotionally Intelligent Design | by UX Magazine
Opens in a new window

superside.com
Pros and Cons of AI in Graphic Design (According to Designers) - Superside
Opens in a new window

mdpi.com
A Comprehensive Study of Emotional Responses in AI-Enhanced Interactive Installation Art
Opens in a new window

arxiv.org
Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs - arXiv
Opens in a new window

cdn.openai.com
why-language-models-hallucinate | OpenAI
Opens in a new window

en.wikipedia.org
Hallucination (artificial intelligence) - Wikipedia
Opens in a new window

medium.com
Visual Answer Grounding with Multi-Modal RAG Workflow | Data Science Collective
