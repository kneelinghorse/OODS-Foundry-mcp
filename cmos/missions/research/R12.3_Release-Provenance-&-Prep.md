A Framework for Deterministic Internal Releases in a PNPM Monorepo
The Internal Release Readiness Checklist
This document outlines a comprehensive framework for establishing a deterministic, auditable, and high-confidence internal release process within a pnpm monorepo. The primary objective is to enable the confident consumption of the system directly from a Git repository, leveraging tags as immutable release markers, without publishing to an external package registry. The following checklist serves as a high-level summary of the procedure. Each step is detailed extensively in the subsequent sections of this report.

Step 1: Verify Build Reproducibility. Execute the two-pass build and pack verification script to ensure that all generated package tarballs are byte-for-byte identical across independent runs. This is the foundational guarantee of artifact integrity.

Step 2: Ensure Commit Hygiene. Confirm that all commits intended for the release adhere to the Conventional Commits specification. This structured history is a prerequisite for automated version calculation and changelog generation.

Step 3: Perform Package Sanity Checks. Run the package validation script against all workspace packages. This step checks for common packaging errors and verifies the correctness of package.json exports maps and their corresponding TypeScript type definitions, ensuring consumer compatibility.

Step 4: Generate the Internal Release. Execute the unified release script, which orchestrates the entire process: calculating the next semantic version, generating the CHANGELOG.md, creating the lightweight Git tag, and capturing comprehensive diagnostic data into a diagnostics.json file.

Achieving Byte-for-Byte Build Reproducibility
The Imperative of Determinism in Software Supply Chains
The ability to produce build artifacts that are byte-for-byte identical from a given source code revision is known as build reproducibility. This practice is not merely a matter of technical tidiness; it is a fundamental pillar of modern software supply chain security and engineering discipline. Reproducible builds provide an independently verifiable path from human-readable source code to machine-executable binaries, establishing a chain of trust that is essential for confident consumption of software artifacts.   

The core value proposition of determinism lies in its ability to guarantee that a compiled artifact has not been tampered with. It allows any third party to check out the same source code, run the same build process, and produce an identical binary. This capability provides several critical benefits:

Security and Trust: It enables independent verification that a distributed binary matches the public source code, mitigating the risk of malicious code injection in the build or distribution process. This fosters high confidence that the software is genuine and free from hidden backdoors.   

Transparency and Debugging: A deterministic build process ensures that code behaves consistently across all environments. This eliminates a significant class of "works on my machine" issues and simplifies debugging by guaranteeing that the build output is a direct function of the source inputs alone.   

Protection of Build Infrastructure: Attacks on build systems can have widespread consequences. If builds are reproducible, any unauthorized modification to the build process or its dependencies will result in a different binary hash, allowing for early detection of a compromised system.   

For an internal release process where artifacts are consumed directly from a version control system, these principles are paramount. The goal to "consume the system confidently" is unachievable without a strong guarantee of artifact integrity. Confidence requires trust, and trust in a build artifact is only possible if it can be independently verified as the direct, untampered result of a specific source code commit. Therefore, achieving byte-for-byte reproducibility is the foundational prerequisite for the entire internal release framework.

Deconstructing Non-Determinism in the Node.js Ecosystem
While modern package managers like pnpm use lockfiles (pnpm-lock.yaml) to ensure deterministic installation of dependencies, this is a necessary but insufficient condition for achieving a fully reproducible build. The Node.js ecosystem is rife with sources of non-determinism that can be introduced after the dependency installation phase, corrupting the final build artifact.

Common sources of non-determinism include:

Post-install Scripts: A significant number of npm packages execute post-install scripts to compile native addons or generate configuration files. These scripts can introduce non-determinism by embedding timestamps, absolute file paths from the build machine, or by being influenced by system-level dependencies like the version of GCC or Xcode installed.   

Platform-Specific Dependencies: The optionalDependencies feature in package.json allows packages to install different dependencies based on the operating system. A classic example is fsevents, which is only installed on macOS. This leads to different node_modules structures and lockfile contents across platforms, fundamentally breaking determinism.   

Build Tooling Configuration: Bundlers and transpilers like Webpack or TypeScript's tsc can introduce non-determinism if not configured correctly. For example, module IDs might be generated in a non-stable order, or timestamps might be embedded in the output. Even with deterministic settings, subtle differences in file system traversal order between platforms can alter the final bundle structure.   

Package Manager Behavior: While pnpm provides strong guarantees, complex workspace configurations or cyclic dependencies can sometimes lead to non-deterministic hoisting behavior within the virtual store (node_modules/.pnpm).   

Because non-determinism can be introduced at any stage—dependency installation, code transpilation, asset bundling, or final packaging—any verification process must operate on the final, distributable artifact. Verifying the contents of the node_modules directory is insufficient, as it would fail to detect non-determinism introduced by the build or packaging steps. The pnpm pack command creates a .tgz tarball, which is the precise artifact that would be published to a registry or consumed internally. Hashing this final tarball creates an immutable signature of the entire process, from source code to final package, providing the strongest possible guarantee of reproducibility.   

A Two-Run Verification Protocol and Script
To rigorously verify build reproducibility, a two-run protocol is employed. The process is executed twice from a completely clean state, and the resulting artifacts are compared. If the hashes of the artifacts are identical, the build is considered reproducible.

Procedure:

Clean Slate (Run 1): The repository is scrubbed of all untracked files and directories, including node_modules and any build outputs. The command git clean -fdx is ideal for this purpose.

Install & Build (Run 1): Dependencies are installed using pnpm install --frozen-lockfile to ensure the exact versions from the lockfile are used. The project's build script (e.g., pnpm run build) is then executed.

Pack & Hash (Run 1): The pnpm -r pack command is used to create .tgz tarballs for every package defined in the pnpm-workspace.yaml file. A sha256 hash is computed for each generated tarball, and the results are saved to a temporary file.   

Clean Slate (Run 2): The repository is scrubbed again using git clean -fdx to ensure the second run is completely isolated from the first.

Install, Build, Pack & Hash (Run 2): Steps 2 and 3 are repeated to produce a second set of tarballs and their corresponding sha256 hashes.

Compare: The hashes from Run 1 are compared with the hashes from Run 2. If any hash differs, the build is non-deterministic, and the process fails.

Example Script (verify-reproducibility.sh):

Bash

#!/bin/bash
set -e # Exit immediately if a command exits with a non-zero status.

# Define the temporary directory for storing results.
DIAGNOSTICS_DIR="dist/diagnostics"
HASHES_RUN1_FILE="$DIAGNOSTICS_DIR/hashes_run1.txt"
HASHES_RUN2_FILE="$DIAGNOSTICS_DIR/hashes_run2.txt"

# Function to perform a clean build, pack, and hash cycle.
run_build_cycle() {
  local output_file=$1
  echo "--- Starting Build Cycle: $output_file ---"

  # 1. Clean Slate: Remove all untracked files and directories.
  echo "Cleaning workspace..."
  git clean -fdx

  # 2. Install Dependencies: Use frozen lockfile for determinism.
  echo "Installing dependencies..."
  pnpm install --frozen-lockfile

  # 3. Build Project: Execute the build command for the entire monorepo.
  echo "Building project..."
  pnpm run build

  # 4. Pack Packages: Create tarballs for all workspace packages.
  # The --pack-destination flag places all tarballs in a predictable directory.
  echo "Packing packages..."
  local pack_dir="$DIAGNOSTICS_DIR/pack"
  rm -rf "$pack_dir"
  mkdir -p "$pack_dir"
  pnpm -r pack --pack-destination "$pack_dir"

  # 5. Generate Hashes: Calculate sha256 for each tarball and save.
  # Sort the file paths to ensure a consistent order for comparison.
  echo "Generating sha256 hashes..."
  find "$pack_dir" -type f -name "*.tgz" | sort | xargs sha256sum > "$output_file"

  echo "--- Build Cycle Finished ---"
}

# --- Main Execution ---
echo "Starting reproducibility verification..."
mkdir -p "$DIAGNOSTICS_DIR"

# Execute the first run.
run_build_cycle "$HASHES_RUN1_FILE"

# Execute the second run.
run_build_cycle "$HASHES_RUN2_FILE"

# Compare the hash files.
echo "Comparing hashes from both runs..."
if diff --brief "$HASHES_RUN1_FILE" "$HASHES_RUN2_FILE" >/dev/null; then
  echo "✅ Success: Builds are reproducible. Hashes match."
  # Keep the final hashes for the diagnostics report.
  mv "$HASHES_RUN2_FILE" "$DIAGNOSTICS_DIR/artifact_hashes.sha256"
  rm "$HASHES_RUN1_FILE"
  exit 0
else
  echo "❌ Error: Builds are not reproducible. Hashes differ."
  echo "--- Hashes from Run 1 ---"
  cat "$HASHES_RUN1_FILE"
  echo "--- Hashes from Run 2 ---"
  cat "$HASHES_RUN2_FILE"
  exit 1
fi
Automated Change Tracking with Conventional Commits
The Conventional Commits Specification: A Common Language for Change
Automating complex workflows like version bumping and changelog generation requires a machine-readable understanding of the changes within a commit history. The Conventional Commits specification provides a simple, structured format for commit messages that encodes this semantic meaning, transforming the Git log from a simple historical record into a queryable database of changes.   

The specification mandates a clear structure for commit messages:
<type>(<scope>): <description>

The key components are:

type: A noun that describes the category of change. The specification gives special meaning to feat (a new feature) and fix (a bug fix). These types directly map to Semantic Versioning: feat corresponds to a MINOR version bump, and fix corresponds to a PATCH version bump. Other types like build, chore, docs, refactor, and test are permitted but do not influence versioning by default.   

scope (optional): A noun enclosed in parentheses that provides context for the change, such as the name of an affected package or module (e.g., feat(api):...).

description: A concise, imperative summary of the change.

Breaking Changes: A commit that introduces a breaking API change must indicate this by appending a ! after the type/scope (e.g., feat(api)!:...) or by including a footer section starting with BREAKING CHANGE:. Both signals correspond to a MAJOR version bump.   

By enforcing this convention, every commit explicitly declares its intent and impact. This structured data is the engine that drives automation, enabling tools to parse the Git history, determine the appropriate next version number, and group related changes into a human-readable changelog.

Examples of Conventional Commits:

Feature Commit (Minor Bump):
feat(parser): add support for TOML configuration files

Fix Commit (Patch Bump):
fix(auth): correct redirect loop on expired session

Breaking Change Commit (Major Bump):
refactor(database)!: rename user 'id' column to 'uuid'
BREAKING CHANGE: The primary key for the users table has been changed from 'id' to 'uuid'. All direct database queries must be updated.

Documentation Commit (No Version Bump):
docs(readme): update installation instructions

Policy Enforcement with commitlint
To ensure the integrity of the automated process, the Conventional Commits specification must be strictly enforced. Allowing non-compliant messages into the main branch would break the automation tools that depend on them. The commitlint tool is the industry standard for this purpose.   

It is best implemented as a commit-msg Git hook, managed by a tool like husky. This setup intercepts every git commit command and validates the message against the configured rules before the commit is created. This provides immediate feedback to the developer and prevents non-conforming history from ever being recorded.

Setup Steps:

Install Dependencies:

Bash

pnpm add -D husky @commitlint/cli @commitlint/config-conventional
Configure commitlint: Create a commitlint.config.js file in the repository root:

JavaScript

// commitlint.config.js
module.exports = {
  extends: ['@commitlint/config-conventional'],
};
Configure husky: Initialize husky and create the commit-msg hook:

Bash

pnpm exec husky init
echo 'npx --no -- commitlint --edit "$1"' >.husky/commit-msg
This configuration ensures that every commit attempted in the repository will be validated, maintaining the hygiene required for automation.   

Scripted CHANGELOG Generation
With a well-structured commit history, generating a CHANGELOG.md file becomes a straightforward, scriptable task. The conventional-changelog-cli package provides a powerful command-line interface for this purpose, leveraging the same parsing logic used by many automated release tools.   

The script's role is to generate the changelog content for all commits since the last Git tag and prepend it to the existing CHANGELOG.md file.

Example Script (generate-changelog.sh):

Bash

#!/bin/bash
set -e

# This script generates a changelog for the latest unreleased changes.
# It should be run after a new version is determined but before the new tag is created.

# The conventional-changelog-cli tool will automatically find commits since the last tag.
echo "Generating changelog..."

# -p: Use the 'conventionalcommits' preset, which aligns with the spec.
# -i: Read from CHANGELOG.md (the infile).
# -s: Write to the same file (the outfile, 's' for same).
# This command effectively prepends the new release notes to the existing file.
npx conventional-changelog-cli -p conventionalcommits -i CHANGELOG.md -s

echo "✅ CHANGELOG.md has been updated."
This script is designed to be a component of the larger release process. It is run after the next version number has been calculated, ensuring that the new section in the changelog is correctly associated with the upcoming version tag. The ecosystem of tools for this purpose is vast , but this direct CLI approach is simple, transparent, and avoids dependencies on larger, more complex release frameworks that are often geared towards publishing to npm.   

A Non-Publish Versioning and Tagging Strategy
Decoupling Versioning from Publishing
Standard release automation tools, such as changesets or semantic-release, are fundamentally designed around the workflow of publishing packages to a registry like npm. In that model, incrementing the version field in package.json is a critical step that signals a new, publishable release. However, for an internal-only workflow where consumption happens directly from Git, this model is inappropriate. The primary goal is not to publish a package but to create an immutable, verifiable reference point in the Git history.   

In this context, the Git tag itself becomes the release artifact. It is the canonical, immutable pointer to a specific commit that has passed all verification checks and represents a stable, consumable version of the system. The version number (e.g., v1.2.3) is metadata for the tag, not a change to the source code. Modifying package.json files with new versions would create unnecessary code churn and imply an intent to publish that does not exist. This reframes the entire process: the objective is not to prepare a package for publication but to prepare a commit for tagging. This justifies a "dry-run" approach to versioning, where the next version number is calculated for the tag but never committed to the source files.

Tagging Policy: Lightweight vs. Annotated Tags
Git offers two types of tags: lightweight and annotated. The choice between them should be deliberate and aligned with the intent of the tagging process.   

Annotated Tags are full objects in the Git database. They store metadata including the tagger's name, email, date, and a distinct tagging message. They can also be GPG-signed. They are designed for formal, public releases.   

Lightweight Tags are simple pointers or references to a specific commit. They store no extra metadata beyond the commit hash they point to. They are akin to a branch that never moves.   

For an automated, internal release process, lightweight tags are the superior choice. The rationale is clearly supported by Git's own documentation, which states: "Annotated tags are meant for release while lightweight tags are meant for private or temporary object labels". Since this process is internal and fully automated, the "who" and "when" of the tag are already captured by the commit's author and date. Creating an annotated tag would introduce redundant information and add unnecessary weight to the repository's object database. The choice of lightweight tags is a conscious decision to align the tool with the specific intent of creating an efficient, internal release marker.   

Table 1: Comparison of Git Tag Types

Feature	Lightweight Tag	Annotated Tag
Git Object Type	Pointer (no separate object)	Full Git object (tag object)
Metadata Stored	None (only the commit hash)	Tagger name, email, date, message
GPG Signable	No	Yes
git describe Default	Ignored by default	Used by default
Recommended Use Case	Private, temporary, or automated labels	Public, formal releases

Export to Sheets
The Tagging Workflow and Script
The internal release tagging process should be fully automated, deriving the next version from the commit history and applying the corresponding lightweight tag. This workflow deliberately avoids commands like npm version that modify package.json files.

Script Outline (tag-release.sh):

The script will perform the following steps:

Use the conventional-recommended-bump package to analyze commits since the last tag and determine the type of version bump required (patch, minor, or major).   

Fetch the latest tags from the remote repository to ensure the local view is up-to-date.

Identify the most recent semantic version tag (e.g., v1.2.2).

Calculate the next version string based on the current version and the recommended bump type.

Create a new lightweight Git tag with the calculated version string.

Push the new tag to the remote repository.

Example Script (tag-release.sh):

Bash

#!/bin/bash
set -e

# This script calculates the next version based on conventional commits,
# creates a lightweight Git tag, and pushes it.

# 1. Ensure local tags are in sync with the remote.
echo "Fetching latest tags from remote..."
git fetch --tags

# 2. Get the latest semantic version tag.
# This command lists tags, sorts them by version, and gets the last one.
LATEST_TAG=$(git tag -l "v*.*.*" | sort -V | tail -n 1)
if; then
  echo "No existing semver tag found. Starting with v0.1.0."
  LATEST_TAG="v0.0.0"
fi
echo "Latest tag found: $LATEST_TAG"

# 3. Determine the recommended version bump type (patch, minor, major).
BUMP_TYPE=$(npx conventional-recommended-bump -p conventionalcommits)
echo "Recommended bump type: $BUMP_TYPE"

# 4. Calculate the next version using a simple semver bump script or tool.
# For simplicity, we use a Node.js one-liner with the 'semver' package.
# Ensure 'semver' is a dev dependency: pnpm add -D semver
NEXT_VERSION="v$(node -e "console.log(require('semver').inc('$LATEST_TAG', '$BUMP_TYPE'))")"
echo "Calculated next version: $NEXT_VERSION"

# 5. Create the lightweight Git tag.
echo "Creating lightweight Git tag $NEXT_VERSION..."
git tag "$NEXT_VERSION"

# 6. Push the new tag to the remote repository.
echo "Pushing tag to remote..."
git push origin "$NEXT_VERSION"

echo "✅ Successfully created and pushed tag $NEXT_VERSION."
Ensuring Package Integrity and Type Safety
The Modern JavaScript Packaging Minefield
Compiling a TypeScript project without errors is a necessary but insufficient condition for ensuring a package is safe for consumption. Modern Node.js and bundler environments rely on the exports field in package.json to resolve module imports. This field provides strong encapsulation, meaning that if a path is not explicitly listed in exports, it cannot be imported by a consumer.   

This creates a new class of integration problems. A package can be internally type-correct, but if its package.json exports map does not correctly align with the generated TypeScript declaration files (.d.ts), consumers will encounter module resolution or type errors. For example, an exports entry might point to an ESM build, but the corresponding types condition might point to a CJS-style declaration file, causing failures in certain environments.   

Therefore, ensuring consumer confidence requires validating the package's public-facing contract—the combination of its package.json configuration and its type declaration files. This validation goes beyond the scope of the TypeScript compiler (tsc), which primarily checks for internal consistency.

Automated Sanity Checks with publint and arethetypeswrong
To address this validation gap, a new generation of specialized linting tools has emerged. This framework recommends a two-pronged approach using publint and @arethetypeswrong/cli.

publint: This tool acts as a general-purpose linter for package.json. It checks for a wide range of common packaging errors, such as missing files in the published tarball, conflicts between legacy fields (main, module) and the modern exports field, and other configuration best practices. It provides a crucial first-pass health check on the package's overall configuration.   

@arethetypeswrong/cli (attw): This is a highly specialized tool designed to perform a deep analysis of the relationship between a package's exports map and its TypeScript declaration files. It simulates how different module resolution strategies (node16, bundler, etc.) will resolve both the JavaScript files and the type definitions for each entry point. This allows it to detect subtle but critical mismatches that would break builds for downstream consumers.   

By combining these tools, the release process can automatically verify that every package in the monorepo is not only internally consistent but also correctly configured for external consumption.

Implementation Script (run-sanity-checks.sh)
This script automates the process of running both publint and attw against every package in the workspace. It iterates through the package locations defined in pnpm-workspace.yaml.

Example Script (run-sanity-checks.sh):

Bash

#!/bin/bash
set -e

# This script runs package sanity checks on all workspace packages.

# Prerequisite: Install the tools as root dev dependencies.
# pnpm add -Dw publint @arethetypeswrong/cli

# Find all package directories from pnpm-workspace.yaml.
# This assumes a simple 'packages/*' glob. Adapt if your structure is different.
PACKAGE_DIRS=$(pnpm m ls --json --depth -1 | jq -r 'map(select(.name!= ".")) |. |.path')

# Flag to track if any checks fail.
HAS_FAILED=false

for PKG_DIR in $PACKAGE_DIRS; do
  echo "--- Checking package at: $PKG_DIR ---"
  cd "$PKG_DIR"

  # Run publint for general package health.
  echo "Running publint..."
  if! npx publint.; then
    echo "❌ publint failed for $PKG_DIR"
    HAS_FAILED=true
  else
    echo "✅ publint passed for $PKG_DIR"
  fi

  # Run arethetypeswrong for deep type resolution analysis.
  # The --pack flag is crucial: it creates a temporary tarball and analyzes
  # its contents, which is the most accurate way to simulate consumption.
  echo "Running arethetypeswrong..."
  if! npx attw --pack.; then
    echo "❌ arethetypeswrong failed for $PKG_DIR"
    HAS_FAILED=true
  else
    echo "✅ arethetypeswrong passed for $PKG_DIR"
  fi

  cd - > /dev/null # Return to the root directory.
  echo "-------------------------------------"
done

if; then
  echo "❌ One or more package sanity checks failed."
  exit 1
else
  echo "✅ All package sanity checks passed."
  exit 0
fi
Capturing Release Diagnostics
The diagnostics.json Artifact
To ensure a fully auditable and transparent release process, a diagnostics.json file should be generated with each internal release. This file serves as an immutable record of the release event, capturing key metadata about the build and the resulting artifacts. It is not committed to the repository but can be archived in a separate system for compliance and historical analysis.

Schema Definition:

The file should conform to a well-defined JSON schema.

JSON

{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Internal Release Diagnostics",
  "type": "object",
  "required":,
  "properties": {
    "releaseVersion": {
      "description": "The semantic version tag for this release.",
      "type": "string",
      "pattern": "^v[0-9]+\\.[0-9]+\\.[0-9]+$"
    },
    "releaseCommit": {
      "description": "The full SHA of the Git commit that was tagged.",
      "type": "string"
    },
    "buildTimestamp": {
      "description": "The ISO 8601 timestamp when the build was initiated.",
      "type": "string",
      "format": "date-time"
    },
    "buildDurationMs": {
      "description": "The total duration of the release process in milliseconds.",
      "type": "integer"
    },
    "packageInventory": {
      "description": "A list of all packages included in this release.",
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "name": { "type": "string" },
          "version": { "type": "string" },
          "path": { "type": "string" }
        },
        "required": ["name", "version", "path"]
      }
    },
    "artifactHashes": {
      "description": "A map of package names to their tarball sha256 hashes.",
      "type": "object",
      "additionalProperties": {
        "type": "string",
        "pattern": "^sha256:[a-f0-9]{64}$"
      }
    }
  }
}
Integration into the Workflow
The data for diagnostics.json is collected throughout the unified release script.

buildTimestamp and buildDurationMs are captured by timing the script's execution.

artifactHashes are sourced from the output of the reproducibility verification step.

packageInventory is generated by programmatically listing the workspace packages via pnpm m ls --json.

releaseVersion and releaseCommit are captured just before the Git tag is created.

The Unified Internal Release Script
The final step is to orchestrate all the preceding components into a single, cohesive script. This internal-release.sh script serves as the single entry point for executing an internal release, ensuring that all verification, generation, and tagging steps are performed in the correct order.

Workflow Logic:

Record the start time and timestamp.

Execute verify-reproducibility.sh. Capture the resulting artifact hashes. If the script fails, the entire release process halts.

Execute run-sanity-checks.sh. If it fails, the process halts.

Calculate the next semantic version based on the commit history since the last tag.

Execute generate-changelog.sh to update CHANGELOG.md with the changes for the new version.

Create a new commit containing only the updated CHANGELOG.md. The commit message should be standardized (e.g., chore(release): version ${NEXT_VERSION}).

Create the lightweight Git tag pointing to this new commit.

Record the end time and calculate the total duration.

Assemble all captured data (version, commit SHA, timestamp, duration, hashes, inventory) and write it to dist/diagnostics/diagnostics.json.

Print a success message and instruct the user to review the changes and push the commit and tag to the remote repository using git push --follow-tags. This final manual step provides an opportunity for a last-minute review before the release is finalized on the remote.

This unified script ensures that a tag is only created if and only if the build is reproducible, all packages pass sanity checks, and the changelog has been successfully generated, creating a robust and reliable internal release pipeline.

Conclusion and Build Implications
The framework detailed in this report provides a robust, end-to-end process for creating deterministic and auditable internal releases within a pnpm monorepo. By prioritizing byte-for-byte reproducibility, enforcing a structured commit history with Conventional Commits, and implementing rigorous package sanity checks, this system enables developers to consume artifacts from Git with a high degree of confidence. The decoupling of versioning from publishing, treating the lightweight Git tag as the canonical release artifact, is a key strategic decision that aligns the tooling with the specific goal of internal consumption. The final diagnostics.json artifact provides a complete, auditable record of each release, enhancing transparency and accountability.

Adopting this framework has several direct implications for the build and development process, which are summarized below.

Build Implications
YAML

# cmos/missions/research/R12.3_Internal-Release-Readiness.findings.yaml
reproducibility:
  verify_tarballs: true
  sha_algo: "sha256"
changelog:
  conventional_commits: true
  script: "tools/release/internal-release.sh"
versioning:
  tags: "git lightweight tags"
  dry_run: true
diagnostics:
  include: ["buildMs", "hashes", "packageInventory"]

Sources used in the report

reproducible-builds.org
Reproducible Builds — a set of software development practices that ...
Opens in a new window

blog.aspect.build
Deterministic npm dependencies with Bazel and rules_nodejs - Aspect Build Blog
Opens in a new window

github.com
package-lock.json non-deterministic due to optionalDependencies · Issue #18135 - GitHub
Opens in a new window

webpack.js.org
Optimization - webpack
Opens in a new window

github.com
[bug] Non-deterministic ordering of modules in bundle output when using deterministic module IDs on different platforms · Issue #19141 · webpack/webpack - GitHub
Opens in a new window

github.com
Shared lockfile results in nondeterministic top-level packages, differing behavior from npm install · Issue #6457 - GitHub
Opens in a new window

pnpm.io
pnpm pack
Opens in a new window

jasonwatmore.com
npm pack for local package dependency testing | Jason Watmore's Blog
Opens in a new window

pnpm.io
pnpm pack | pnpm
Opens in a new window

conventionalcommits.org
Conventional Commits
Opens in a new window

conventionalcommits.org
A specification for adding human and machine readable meaning to commit messages - Conventional Commits
Opens in a new window

conventionalcommits.org
Angular Conventional Commit
Opens in a new window

github.com
conventional-changelog/commitlint: Lint commit messages - GitHub
Opens in a new window

fireflysemantics.medium.com
Generate the Changelog with Release-It and Conventional Commits | by Ole Ersoy
Opens in a new window

ansidev.xyz
How to configure conventional commit for your project? - ansidev's blog
Opens in a new window

github.com
conventional-changelog/conventional-changelog ... - GitHub
Opens in a new window

conventionalcommits.org
About - Conventional Commits
Opens in a new window

pnpm.io
pnpm publish
Opens in a new window

pnpm.io
Using Changesets with pnpm | pnpm
Opens in a new window

git-scm.com
git-scm.com
Opens in a new window

safjan.com
Git - Annotated vs. Lightweight Tags - Krystian Safjan's Blog
Opens in a new window

hackernoon.com
Annotated and Lightweight Git Tags | HackerNoon
Opens in a new window

stackoverflow.com
What is the difference between an annotated and unannotated tag? - Stack Overflow
Opens in a new window

stackoverflow.com
Why should I care about lightweight vs. annotated tags? - Stack Overflow
Opens in a new window

webpack.js.org
Package exports - webpack
Opens in a new window

velopen.com
TypeScript and NPM package.json exports the 2024 way - Velopen
Opens in a new window

bjornlu.com
publint - Bjorn Lu
Opens in a new window

publint.dev
Getting started - publint
Opens in a new window

publint.dev
CLI | publint
Opens in a new window

github.com
arethetypeswrong/arethetypeswrong.github.io: Tool for ... - GitH