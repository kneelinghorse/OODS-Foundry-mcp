A Framework for Agent-Mediated Operations: The Plan, Approve, Execute User Experience
Part I: Foundational Principles of Agentic UX for Developer Tools
This section establishes the core philosophy governing the agent's interaction model. It moves beyond a simple feature specification to create a principled framework, ensuring that all subsequent design decisions are coherent and justified. This framework is built upon three pillars: explicit user control through a "Plan-Approve-Execute" model, a clear and consistent anatomy for every agent-mediated operation, and an immutable, signed transcript that serves as the foundation for trust, auditability, and advanced debugging capabilities.

1.1 The Control-Automation Dial: Principles of the "Plan-Approve-Execute" Model
The agent presented in this framework is not an autonomous black box but a sophisticated tool designed to augment, not replace, the developer. Its operation is governed by the principle of the "Control-Automation Dial," a metaphor representing a spectrum of autonomy that the developer must always control. The default setting for this dial is maximum control (read-only), with the agent's capabilities unlocked only through explicit, informed consent. This is achieved through the "Plan-Approve-Execute" interaction model.

This model is a direct implementation of established Human-in-the-Loop (HITL) patterns, specifically the "Approve or Reject" and "Review Before Tool Calls" paradigms. In high-stakes domains such as finance, healthcare, and, critically, software development, the cost of an unverified automated action is unacceptably high. By pausing the workflow before any critical, state-changing step‚Äîsuch as modifying a file or executing a shell command‚Äîthe model inserts a mandatory checkpoint for human review and approval. This ensures that the agent's actions align with the developer's intent and quality standards, transforming the agent from a potentially unpredictable actor into a reliable co-pilot. ¬† 

The technical feasibility of this asynchronous approval workflow is underpinned by the evolution of agentic architectures, particularly the shift towards graph-based frameworks like LangGraph. These systems model agent workflows as a series of nodes and edges, with the crucial ability to checkpoint and persist the entire graph state after each step. This persistence layer allows the agent's execution to be paused indefinitely, awaiting human input, and then resumed seamlessly from the exact point it left off. This capability is the technical bedrock upon which a robust and user-friendly approval experience is built. ¬† 

The agent's entire lifecycle can be mapped to the fundamental "Perception, Reasoning, Action" loop, a core pattern in AI agent design. ¬† 

Perception: The agent receives and processes the developer's initial prompt.

Reasoning: The agent analyzes the request, decomposes it into sub-tasks, and formulates a step-by-step strategy. This phase culminates in the generation of "The Plan."

Action: The agent executes the approved plan, interacting with tools and the local environment.

The "Approve" step is a mandatory guardrail, a safety layer that is intentionally inserted between the Reasoning and Action phases. This ensures that no action is taken based solely on the agent's reasoning; human judgment is the final and necessary gatekeeper. ¬† 

A deeper examination of developer workflows reveals that the approval step serves a purpose beyond individual safety. In tools like GitHub (Pull Requests) and ArgoCD (deployment diffs), the preview-and-approve mechanism is not just for the individual operator but for team-based review and collaboration. The agent's generated plan and diff are, in essence, a machine-generated pull request‚Äîa proposal of changes that benefits from scrutiny. Therefore, the "approve" step should be viewed as a collaborative checkpoint. This perspective has significant design implications: the user interface, both in the CLI and the Storybook panel, must facilitate the sharing and exporting of the plan and its associated transcript. This transforms a single-player safety feature into a potential multi-player review process, enhancing code quality and team alignment. ¬† 

1.2 Anatomy of an Agent Run: States and Components
To ensure clarity and consistency in design, development, and user communication, a formal vocabulary for the agent's operational lifecycle is essential. Every interaction, or "run," progresses through a defined set of states and involves a consistent set of components.

States of an Agent Run:

Planning: The initial state, triggered by the user's prompt. The agent analyzes the request and formulates a detailed execution strategy.

Previewing: The agent presents its plan, including all proposed file modifications (diffs) and new file creations (artifacts), to the user for review. The agent is in a read-only, "dry-run" mode.

Awaiting Approval: The workflow is paused. The agent has presented its plan and is waiting for an explicit "approve" or "reject" command from the user. The system can remain in this state indefinitely. ¬† 

Executing: Upon receiving approval, the agent begins to carry out the steps outlined in the plan. It interacts with the filesystem, calls tools, and performs the specified actions.

Summarizing: After execution completes (either successfully or with a failure), the agent presents a final summary of the actions taken, the artifacts created, and the final status of the run.

Core Components of an Agent Run:

The Plan: A structured, human-readable description of the agent's intent. It is not a monolithic block of text but a clear, itemized list of the steps it will take, the tools it will invoke, and the rationale behind its approach. This component is the output of the agent's "Reasoning" phase. ¬† 

The Diff: A precise, unambiguous representation of all proposed modifications to existing files. The clarity and readability of the diff are paramount for enabling an informed approval decision.

The Artifacts: A comprehensive list of all new files or other outputs the agent intends to create. Each artifact must be inspectable by the user before approval.

The Approval Checkpoint: The specific UI element (a command-line prompt or a button in a GUI) where the user makes the final decision to grant or deny the agent permission to proceed with execution.

The Transcript: The final, immutable, and cryptographically signed record of the entire run, from the initial prompt to the final summary. This component is the cornerstone of trust and advanced functionality.

1.3 The Signed Transcript: An Immutable Record for Trust and Replay
The transcript is the most critical output of an agent run, elevated from a simple log file to a core product artifact. It is the canonical source of truth, providing an immutable and verifiable record that underpins user trust, enables robust auditing, and powers the advanced replay and debugging features.

Structure of the Transcript:

Each transcript will be a structured document (formatted as JSON for machine-readability) containing the following fields:

run_id: A cryptographically secure unique identifier (e.g., UUIDv4) for the run.

timestamp: An ISO 8601 formatted timestamp indicating the start time of the run.

initial_prompt: The verbatim, unmodified prompt provided by the user.

plan: The complete, structured plan as generated by the agent during the Planning state.

diffs_and_artifacts: A structured list of all proposed changes. Each entry will include the file path, the type of change (modify, create, delete), and the full content of the diff or new artifact.

approval_record: An object capturing the approval decision, including the outcome (approved or rejected), the timestamp of the decision, and an identifier for the approving user.

execution_log: A detailed, step-by-step log of the execution phase. This includes every tool call with its parameters, the output from each tool, and any errors encountered.

final_summary: The final summary message presented to the user upon completion.

signature: A cryptographic signature (e.g., HMAC-SHA256) of the transcript's content, ensuring its integrity and proving that it has not been tampered with post-execution.

This structured approach to logging provides a complete audit trail for every agent action, which is essential for compliance and security in sensitive environments. More profoundly, it reframes the transcript from a passive record into an active, executable asset. ¬† 

Modern agentic workflow tools emphasize "Tracing & Replay" as a key capability for debugging and improving agent behavior. Frameworks like LangGraph even offer "time-travel debugging" to explore alternative execution paths. By capturing the initial_prompt, the agent's plan, and the full execution_log, the transcript effectively becomes a self-contained, high-fidelity test case for the agent. When a run produces an unexpected result, the transcript contains all the necessary inputs and intermediate steps to reproduce the failure. ¬† 

Therefore, the "replay from transcript" feature is not merely about re-executing a list of commands. It is about loading a past execution context into a debugging environment. This allows a developer to perform two powerful actions:

Replay: Re-run the agent with the exact same initial prompt to see if behavior has changed with a new version of the agent, providing a powerful regression testing mechanism.

Edit-and-Replay: Load the transcript, modify the initial_prompt or even specific steps in the plan, and then execute a new run. This transforms debugging from a process of guesswork into a systematic, iterative exploration of the agent's reasoning and execution logic. The transcript becomes the fundamental unit of debugging for agentic systems.

Part II: The Command-Line Interface (CLI) User Journey
This section provides a detailed, prescriptive design for the Command-Line Interface (CLI) experience. The design prioritizes developer ergonomics, balancing the need for rich, human-centric interaction with the essential requirement for scriptability and automation in CI/CD pipelines. The philosophy is to create a tool that is both intuitive for interactive use and predictable for machine consumption.

2.1 CLI Interaction Model: Conversational & Composable
The CLI will adhere to established principles of modern interface design: it will be human-first, composable with other tools, consistent with platform conventions, and will provide just enough information to be clear without being overwhelming. ¬† 

Default Mode (Dry-Run):
The cornerstone of the agent's safety model is its default behavior. Executing the agent command (e.g., my-agent "refactor this component") without an explicit "apply" or "approve" flag will always perform a dry run. The agent will proceed through the Planning and Previewing states, displaying its full plan, diffs, and artifacts, but will not enter the Awaiting Approval state or perform any modifications. This read-only default ensures that users can safely explore the agent's proposed changes without risk of unintended side effects.

Explicit Approval:
To proceed with execution, the user must provide explicit consent. This can be done in two ways to support both interactive and automated workflows:

Interactive Prompt: If the command is run without an approval flag in an interactive terminal, the agent will pause after the preview and present a confirmation prompt (e.g., Do you want to apply these changes? (y/N)).

Approval Flag: For use in scripts and automated pipelines, an explicit flag (e.g., my-agent "..." --approve) will pre-approve the plan, bypassing the interactive prompt. This follows the model of tools like GitHub Copilot CLI, which require explicit consent for every action, but provide flags to manage this in non-interactive sessions. ¬† 

Flags and Arguments:
A consistent and predictable set of flags will be used across all commands. Long, descriptive names will be used for clarity, with conventional short aliases for convenience and speed. ¬† 

--approve: Pre-approves the generated plan, enabling execution without an interactive prompt. Essential for scripting.

--output <format>: Specifies the output format for the final summary and transcript. Supported formats will include text (default, human-readable) and json (machine-readable). ¬† 

--no-color: Disables all ANSI color codes and terminal formatting in the output, for environments that do not support them or for clean logging. ¬† 

--quiet: Suppresses non-essential output like spinners and progress indicators, leaving only the final plan and summary. Ideal for CI logs where progress animations create noise. ¬† 

--transcript <path>: Specifies a file path to save the signed transcript upon completion.

Exit Codes:
To ensure proper composability with shell scripts and automation tools, the CLI will use standard exit codes to signal its final state. This pattern, seen in tools like Terraform's plan -detailed-exitcode , provides granular feedback to calling processes. ¬† 

0: Success. The command completed without errors. In a dry run, this means no changes were proposed. In an approved run, this means execution was successful.

1: Error. The command terminated due to an unexpected error (e.g., failed to parse prompt, unhandled exception).

2: Success with Changes. The command completed successfully, and a non-empty plan was generated (dry run) or applied (approved run). This allows scripts to easily determine if any action was taken.

2.2 State Diagram: CLI Flow
The following state diagram visualizes the complete user journey and the transitions between states within the CLI environment.

Code snippet

stateDiagram-v2
    [*] --> Planning: User executes `my-agent "prompt"`
    Planning --> Previewing: Plan generated successfully
    Previewing --> AwaitingApproval: Plan & diff displayed in interactive TTY
    Previewing --> Canceled: Dry run completes (non-interactive or no --approve flag)
    AwaitingApproval --> Executing: User types 'y' or command was run with `--approve`
    AwaitingApproval --> Canceled: User types 'n'
    Executing --> Summarizing: Execution successful
    Executing --> FailureHandling: Execution fails
    FailureHandling --> Summarizing: Terminal error presented, run terminates
    FailureHandling --> Executing: Automatic retry on transient error is attempted
    Summarizing --> [*]: Run complete, exit code set
    Canceled --> [*]: Run complete, exit code set
2.3 Presenting the Plan: Diffs and Artifacts in the Terminal
Presenting complex, structured information clearly within the constraints of a terminal is a critical UX challenge. The agent's output must be designed for maximum readability to enable informed decision-making.

Diff Presentation:
The agent must present file modifications in a format that is immediately understandable. The choice of diff format can significantly impact readability, especially for changes that are not simple line additions or deletions.

Default Format: The default presentation will be a colorized, unified diff, which is a familiar standard for developers. ¬† 

Word-Level Highlighting: For changes within a single line, such as refactoring a variable name or correcting a typo in documentation, a standard line-based diff is insufficient. It shows the entire line as removed and added. In these cases, a word-level or character-level diff provides far greater clarity. The CLI will default to using an intra-line diff algorithm, similar to git diff --word-diff=color or the more modern --color-words flag, which highlights only the specific tokens that have changed. ¬† 

User Configuration: Power users often have preferred diffing tools, such as delta, which offers advanced features like side-by-side views, syntax highlighting, and line wrapping. The agent's configuration will allow users to specify an external diff command to be used for rendering previews. ¬† 

The following table provides a guide to the diff presentation strategies the agent will support.

Format	Recommended Tool/Flags	Pros	Cons	Best For
Unified Diff	diff -u (with color)	Universally understood, compact, good for line-based changes.	Poor at showing changes within a single long line.	Simple code changes (adding/removing lines), patch files.
Word-level Diff	git diff --color-words	Excellent clarity for intra-line changes, highlights the exact modification.	Can be slightly more verbose for large, multi-line changes.	Refactoring, renaming variables, editing configuration files or prose.
Side-by-Side	delta --side-by-side	Easy to compare old and new versions directly, great for visual review.	Requires more horizontal screen space, can be cluttered for small changes.	Reviewing complex changes, understanding the context of a modification.
JSON Patch	(internal)	Machine-readable, unambiguous, standard format (RFC 6902).	Not human-readable, requires a separate tool to visualize.	API communication, scripting, generating structured transcripts.

Export to Sheets
Artifact Presentation:
New files created by the agent will be presented in a clean, hierarchical tree view. This format provides immediate context for where the new files will be placed within the project structure. This presentation mimics the well-understood output of the tree command, a standard utility on both Linux and Windows systems. ¬† 

Each artifact listed in the tree will be accompanied by a status indicator (e.g., (new), (modified)). If the terminal emulator supports it, file paths will be rendered as hyperlinks, allowing the developer to click and open the temporary file in their default editor for immediate, full-featured inspection before approval.

/
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ components/
    ‚îÇ   ‚îî‚îÄ‚îÄ Button.tsx (modified)
    ‚îÇ   ‚îî‚îÄ‚îÄ Button.module.css (new)
    ‚îî‚îÄ‚îÄ styles/
        ‚îî‚îÄ‚îÄ theme.css (modified)
2.4 The Approval Checkpoint: CLI Wireframes & Copy
The final step before execution is the approval checkpoint. The prompt must be unambiguous, summarize the impending action, and contain a clear warning about the consequences. The copy will be precise and direct, following best practices for developer-focused communication. ¬† 

+--------------------------------------------------------------------------+

| Agent Plan Summary |
+--------------------------------------------------------------------------+

| |
| üéØ Objective: Refactor the Button component to use CSS Modules. |
| |
| üìù Plan: |
| 1. Create `src/components/Button.module.css` with new styles. |
| 2. Update `src/components/Button.tsx` to import and use the module. |
| 3. Remove old button styles from `src/styles/theme.css`. |
| |
| üîç Changes Proposed: |
| - 1 file to be created |
| - 2 files to be modified |
| |
| ‚ö†Ô∏è  WARNING: This action will permanently modify files on your local |
| filesystem. Review the diffs above carefully before proceeding. |
| |
| üëâ Apply these changes? (y/N) |
+--------------------------------------------------------------------------+
This design ensures the user has all necessary context‚Äîthe goal, the steps, a summary of changes, and a stark warning‚Äîbefore making a decision. The default option is "No" (N), requiring an affirmative action (y) to proceed, which is a safer default for destructive operations.

Part III: The Storybook Panel User Journey
This section details the design of the agent's graphical user interface (GUI), which will be integrated directly into Storybook as an addon panel. This provides a rich, visual, and contextual alternative to the CLI, meeting developers where they work during the UI development lifecycle.

3.1 Integrating the Agent into the Storybook Workflow
Rationale:
Storybook is the industry-standard workshop for building, viewing, and testing UI components in isolation. By embedding the agent directly within this environment, it gains immediate context about the component the developer is working on. This enables powerful, context-aware workflows such as "refactor this component," "generate documentation for this story," or "create a new story for the disabled state," all without requiring the developer to switch applications or manually specify file paths. The agent becomes a natural extension of the component-driven development process. ¬† 

Implementation Overview:
The agent's UI will be implemented as a Storybook addon. This involves creating a custom panel that appears in the addons area of the Storybook interface, alongside existing tools like "Actions," "Controls," and "Docs". The development will leverage the official Storybook Addon Kit, which provides a standardized template and tooling for creating UI-based addons. The addon will be registered with Storybook's manager API using the types.PANEL designation, which instructs Storybook to render our custom React component in the panel area. ¬† 

3.2 State Diagram: Storybook Panel Flow
The following state diagram illustrates the user's journey through the Storybook panel UI, from initiating a task to reviewing the final results.

Code snippet

stateDiagram-v2
    state "Idle" as Idle
    state "Configuring" as Configuring
    state "Previewing" as Previewing
    state "Approving" as Approving
    state "Executing" as Executing
    state "Reviewing" as Reviewing
    state "Failure" as Failure

    [*] --> Idle
    Idle --> Configuring: User types prompt in input field
    Configuring --> Previewing: User clicks "Generate Plan"
    Previewing --> Approving: Plan & diffs are displayed, "Approve" button is enabled
    Previewing --> Configuring: User clicks "Edit Prompt" or modifies input
    Approving --> Executing: User clicks "Approve & Run"
    Executing --> Reviewing: Execution completes successfully
    Executing --> Failure: Execution fails
    Failure --> Reviewing: User acknowledges error message
    Reviewing --> Idle: User clicks "Start New Task" or clears the panel
3.3 Visualizing the Plan: The Agent Panel UI
The agent's panel will be composed of several key components, each designed for clarity and interactivity. The following wireframes, described in text and Mermaid diagrams, outline the structure and function of the UI.

AgentPanel (Main View):
This is the top-level component for the addon. It will feature a tabbed interface to organize its different functions, preventing a cluttered UI.

Code snippet

graph TD
    subgraph AgentPanel
        direction TB
        A --> B{Content Area};
        subgraph Run Tab
            direction TB
            C --> D;
            D --> E;
            E --> F[Plan/Preview Area];
        end
        B --> Run Tab
    end
Run Tab: The primary interface for interacting with the agent. Contains the prompt input field, a dropdown to select the active tool (if multiple are available), and the main area where plans and results are displayed.

History Tab: Provides a list of past agent runs, allowing the user to select and review historical transcripts.

Settings Tab: Contains configuration options, such as API keys and the telemetry opt-out setting.

DiffViewer Component:
This component is responsible for displaying file modifications. It will be a rich, interactive viewer, not just a block of pre-formatted text.

View: A side-by-side diff view will be the default, as it offers the clearest comparison in a graphical environment.

Features:

Syntax highlighting appropriate to the file type.

A toggle to switch between side-by-side and unified diff layouts.

A "word-level diff" toggle to highlight intra-line changes.

Line numbers and copy-to-clipboard functionality for each pane.

ArtifactList Component:
This component displays the list of newly generated files. Its design is heavily inspired by the interactive and user-friendly "Artifacts" feature pioneered by Anthropic's Claude, which treats generated content as live, usable objects rather than static code blocks. ¬† 

Code snippet

graph TD
    subgraph ArtifactList
        direction TB
        A["üìÅ src/components/"] --> B;
        A --> C;
        C -- Click --> D{Preview Pane};
        subgraph Preview Pane for Button.module.css
            direction TB
            E
            F
        end
        D --> E & F
    end
View: A collapsible file tree, similar to the one in a standard IDE.

Interactivity: Clicking on a new artifact (e.g., a .tsx component, an .svg icon, or a .md documentation file) will open a preview of its content directly within the panel. For UI components, the panel could even render a live preview.

Actions: Each artifact will have associated actions, such as "Copy Content," "Download File," and "Open in Editor" (using a custom URL scheme like vscode://file/...).

ApproveDialog Component:
This is the final confirmation step. It will be presented as a distinct, unmissable section of the UI that appears after a plan is generated.

Layout: A fixed or prominent panel section with a summary of the plan (e.g., "This will create 1 file and modify 2 files.").

Safety Mechanism: The primary action button, labeled "Approve & Apply Changes," will be disabled by default. It will only become enabled after the user has scrolled to the bottom of the DiffViewer component, providing a reasonable assurance that they have reviewed the proposed changes. This simple interaction design helps prevent accidental approvals of unreviewed code.

Secondary Actions: A clear "Reject" or "Cancel" button will be present to discard the plan and return to the configuration state.

3.4 Interactive Approval and Artifact Review
The approval workflow is designed to be deliberate and safe. The approval button's text will dynamically update to reflect the scope of the changes (e.g., "Apply 3 Changes"), reinforcing the consequence of the action.

Once the execution is complete, the panel transitions to the Reviewing state. The focus shifts from the plan to the results. The ArtifactList now represents the final state of the generated files on disk. The user can seamlessly continue their workflow by using the artifact actions: copying a snippet of generated code, opening a new component directly in their IDE to continue working on it, or downloading a generated SVG icon to place it in their assets folder. This tight integration between generation, review, and utilization is key to making the agent a productive part of the developer's toolkit.

Part IV: Cross-Cutting Concerns and Advanced Features
This final section addresses critical functionalities that are integral to both the CLI and UI experiences. These features ensure the agent is robust, resilient, trustworthy, and capable of improvement over time. They include a comprehensive strategy for handling failures, a powerful replay mechanism for debugging, consistent communication guidelines, and a privacy-conscious telemetry framework.

4.1 Failure Handling and Resiliency
A robust error-handling strategy is fundamental to building user trust. A tool that fails gracefully with clear, actionable advice is far more valuable than one that fails silently or with cryptic messages. The agent's error handling will be built on a clear distinction between different classes of failure. ¬† 

Transient vs. Terminal Failures:
Errors will be categorized to determine the appropriate response:

Transient Failures: These are temporary, often network-related issues where a subsequent attempt is likely to succeed. Examples include network timeouts, API rate limiting (HTTP 429), or temporary service unavailability (HTTP 503). The agent is expected to handle these automatically without requiring user intervention. ¬† 

Terminal Failures: These are permanent errors that will not be resolved by a simple retry. Examples include syntax errors in user-provided configuration, logical impossibilities in a request (e.g., asking to modify a non-existent file), or filesystem permission errors. For these, the agent must fail fast, clearly explain the problem, and provide guidance for resolution. ¬† 

Retry with Exponential Backoff:
For all identified transient failures, the agent will employ an automatic retry mechanism with exponential backoff. This is a standard resilience pattern that prevents a system from overwhelming a temporarily struggling service with rapid, repeated requests. The delay between retries increases exponentially (e.g., 1s, 2s, 4s), often with added "jitter" (randomness) to prevent synchronized retries from multiple clients. ¬† 

The user experience for this mechanism must be transparent but not intrusive:

CLI: A clear, single-line message will indicate the status.

(Attempt 1/3) Request failed: 503 Service Unavailable. Retrying in 2s...
UI: A non-modal toast notification or a status indicator in the panel will display a similar message, informing the user that the agent is working to recover without interrupting their flow.

The retry behavior will be configurable, similar to the AWS CLI's retry modes, allowing advanced users to set the maximum number of attempts (max_attempts) and the retry mode itself. ¬† 

Presenting Terminal Errors:
When a terminal failure occurs, the error message is a critical piece of user interface. It must be human-readable, actionable, and provide sufficient context. All terminal error messages will follow a structured format: ¬† 

ERROR: <A clear, one-sentence summary of what failed.>

> <The specific, verbatim error message from the underlying tool or system.>

üí° Suggestion: <A concrete, actionable step the user can take to resolve the issue.>

üìÑ For more details, run the command again with the --debug flag to see the full stack trace.
This format, inspired by best practices from tools like the Azure Developer CLI , respects the user by explaining the problem in plain language, providing the raw technical details for expert diagnosis, and offering a direct path toward a solution. ¬† 

4.2 Replay and Refinement from Transcripts
The "replay from transcript" feature, enabled by the structured and signed transcripts detailed in Part I, is a cornerstone of the agent's advanced capabilities. It is designed not just as a simple re-execution tool but as a powerful debugging and iteration environment.

This feature can be conceptually understood as a form of "session replay," a technique widely used in UX research to understand user behavior and diagnose issues by watching a recording of a user's interaction with a website or application. In this context, the developer's interaction with the agent is the "session," and the transcript is the high-fidelity recording. It captures all inputs (the prompt), the system's internal state and reasoning (the plan), and the resulting outputs and errors (the execution log). ¬† 

Therefore, the interface for viewing a transcript should not be a raw text viewer. It should be a "player" that reconstructs the state of the agent run at each critical step. When a developer reviews a past run, they should see the plan that was presented, the exact diffs that were shown, and the specific error that was encountered, all in their original context. This provides an invaluable, data-driven insight into the agent's behavior, transforming debugging from an art into a science. ¬† 

User Experience Flow:

Access History: Both the CLI and the Storybook panel will provide a "History" view.

CLI: my-agent history list will display a table of recent runs, including run_id, timestamp, prompt, and status.

Storybook Panel: The "History" tab will display a similar list in a graphical format.

Select and View: The user selects a specific run_id to view its full transcript.

CLI: my-agent history view <run_id> will print the structured transcript to the console, formatted for readability.

Storybook Panel: The panel will render a "replay" view, reconstructing the UI state of the original run.

Re-apply or Replay: From the transcript view, the user is presented with two distinct actions:

Re-apply: This option is available only for runs that were previously approved and successfully executed. It re-runs the execution phase of the original plan. This is useful for deterministically reapplying a known-good set of changes, for example, after a git reset or when setting up a new environment.

Replay (for Debugging): This action starts a new agent run, pre-populating the prompt with the initial_prompt from the selected transcript. The user can then edit this prompt before initiating the new run. This is the primary workflow for iterating on a failed or unsatisfactory run, allowing the user to tweak their instructions and observe how the agent's behavior changes.

4.3 Copy and Communication Guidelines
Consistent, clear, and helpful language is essential for building a trustworthy and usable developer tool. All user-facing text, from command-line prompts to UI button labels, must adhere to a single, consistent style. The tone should be helpful and precise, respecting the user's control and expertise, and aligning with established developer documentation style guides. ¬† 

The following checklist provides a single source of truth for microcopy across key interaction points, ensuring a coherent voice for the agent.

Scenario	Key Information to Convey	CLI Copy Example	UI Copy Example (Button/Label)	Rationale / Tone
First-time Use	Telemetry collection, link to docs, basic command.	Welcome! This agent can help you automate tasks. We collect anonymous usage data to improve the tool (run my-agent telemetry off to disable). See docs at .	Welcome! This panel helps you automate component tasks. Anonymous usage data is collected to improve the agent.	Helpful, transparent, respectful of privacy.
Plan Summary	Objective, number of steps, number of changes.	Plan Summary: 3 steps to create 1 file and modify 2 files.	Plan Summary:‚Ä¢ Create: 1 file‚Ä¢ Modify: 2 files	Precise, concise, scannable. Provides an at-a-glance overview.
Approval Warning	Irreversible action, local filesystem modification, need for review.	‚ö†Ô∏è WARNING: This action will permanently modify files on your local filesystem. Review the diffs above carefully.	‚ö†Ô∏è This will permanently modify files on your local filesystem. Please review all changes before approving.	Direct, unambiguous, serious. Uses warning emoji/icon to draw attention.
Approval Prompt	Clear question, default to "no."	üëâ Apply these changes? (y/N)	Approve & Apply 3 Changes	Action-oriented. The button copy explicitly states the consequence.
Execution Success	Confirmation of action, summary of results, link to transcript.	‚úÖ Success! Applied 3 changes. Transcript saved to .agent/transcripts/<run_id>.json.	Run SuccessfulApplied 3 changes. View Transcript.	Reassuring, informative. Provides a clear path to the audit record.
Failure (Transient)	What failed, what is being done, attempt count.	(Attempt 1/3) Request failed. Retrying in 2s...	Retrying... (1/3)	Informative, reassuring. Manages expectations during a temporary issue.
Failure (Terminal)	What failed, why it failed, how to fix it.	ERROR: Failed to write file.> Permission denied.üí° Suggestion: Check write permissions for the target directory.	Error: Permission DeniedThe agent could not write to the target directory. Please check your filesystem permissions.	Empathetic, actionable, diagnostic. Follows the structured error format.
Post-Run Summary	Final status, artifacts created, next steps.	Run complete. 1 new component and 2 stories created.	Results:- Button.tsx- Button.stories.tsx	Conclusive, useful. Guides the user to the valuable output of the run.

Export to Sheets
4.4 Minimal Telemetry for UX Improvement
To continuously improve the agent's user experience, a minimal and anonymized set of telemetry data will be collected. The entire telemetry system is designed with transparency and user control as its highest priorities. ¬† 

Opt-Out Mechanism:
Telemetry is an opt-out feature. On the first run of the CLI or first use of the Storybook panel, a message will be displayed informing the user about data collection and providing clear instructions on how to disable it.

Notice: To help improve this tool, we collect anonymous usage data. To disable, run 'my-agent telemetry off' or set the AGENT_TELEMETRY_OPTOUT=1 environment variable.

Control: The user will have multiple, convenient ways to disable telemetry, catering to different environments and preferences. ¬† 

CLI Command: my-agent telemetry [on|off|status]

Environment Variable: AGENT_TELEMETRY_OPTOUT=1

UI Toggle: A switch in the Storybook panel's "Settings" tab.

Tracking Plan:
The data collection is strictly limited to events that help answer specific questions about product usage and performance. No personally identifiable information (PII), code, file contents, or command arguments will ever be collected. The following table defines the complete tracking plan. ¬† 

Event Name	Trigger	Properties to Collect (Anonymized)	Rationale / Question Answered
run_started	User initiates a run.	agent_version, interface_type: [cli, storybook], os_type, is_ci: [true, false]	What environments are our users in? Which interface is more popular? Is it being used in automation?
plan_generated	Agent successfully generates a plan.	duration_ms, num_steps, num_diffs, num_artifacts, tool_used	How complex are the tasks users are requesting? How performant is the planning phase? Which tools are most valuable?
run_approved	User approves the plan.	interface_type	What is the overall approval rate for generated plans? (A low rate may indicate poor plan quality).
run_rejected	User rejects the plan.	interface_type	What is the rejection rate? (A high rate signals a significant problem with agent output or user trust).
run_executed	Execution phase completes successfully.	duration_ms	How long do approved runs take to execute? Are there performance bottlenecks?
run_failed	Execution fails during the run.	error_type: [transient, terminal], error_code (hashed), retry_attempts	What are the most common failure modes? Are our retry policies effective for transient errors?

Export to Sheets
This intentional and minimal approach to telemetry ensures that data collection is a tool for improving the developer experience, not for surveillance. By tying every event to a specific, actionable question, we focus our analytics on metrics that directly inform product improvements while respecting user privacy.


Sources used in the report

medium.com
Building for Agentic AI - Agent SDKs & Design Patterns | by Ryan ...
Opens in a new window

langchain-ai.github.io
Human-in-the-loop - Overview
Opens in a new window

dev.to
The ultimate guide to AI agent architectures in 2025 - DEV Community
Opens in a new window

beetroot.co
What is Human-in-the-Loop? A Guide to AI Agent Workflows | Beetroot
Opens in a new window

orkes.io
Human-in-the-Loop in Agentic Workflows: From Definition to ... - Orkes
Opens in a new window

help.asana.com
Human in the Loop Workflows with Asana AI Studio
Opens in a new window

valanor.co
6 Design Patterns for AI Agents in 2025 - Valanor
Opens in a new window

uxmag.medium.com
Secrets of Agentic UX: Emerging Design Patterns for Human Interaction with AI Agents
Opens in a new window

codefresh.io
How to Preview and Diff Your Argo CD Deployments | Codefresh
Opens in a new window

docs.github.com
About GitHub Copilot CLI
Opens in a new window

vellum.ai
Agentic Workflows in 2025: The ultimate guide - Vellum AI
Opens in a new window

clig.dev
Command Line Interface Guidelines
Opens in a new window

atlassian.com
10 design principles for delightful CLIs - Work Life by Atlassian
Opens in a new window

github.com
Copilot CLI - GitHub
Opens in a new window

thoughtworks.com
Elevate developer experiences with CLI design guidelines | Thoughtworks United States
Opens in a new window

codyaray.com
CLI Design Best Practices - Cody A. Ray
Opens in a new window

dev.to
14 great tips to make amazing CLI applications - DEV Community
Opens in a new window

developer.hashicorp.com
terraform plan command reference - HashiCorp Developer
Opens in a new window

evilmartians.com
CLI UX best practices: 3 patterns for improving progress displays - Evil Martians
Opens in a new window

medium.com
Error Handling in CLI Tools: A Practical Pattern That's Worked for Me | by Chloe Zhou
Opens in a new window

git-tower.com
Inspecting Changes with Diffs | Learn Git Ebook (CLI Edition)
Opens in a new window

stackoverflow.com
command line - Using 'diff' (or anything else) to get character-level ...
Opens in a new window

unix.stackexchange.com
Command-line diff tool - Unix & Linux Stack Exchange
Opens in a new window

learn.microsoft.com
tree | Microsoft Learn
Opens in a new window

geeksforgeeks.org
tree Command in Linux with Examples - GeeksforGeeks
Opens in a new window

stackoverflow.com
Tree view of a directory/folder in Windows? [closed] - Stack Overflow
Opens in a new window

uxdesign.cc
How to improve copywriting in your interface | by Nicole Tay - UX Collective
Opens in a new window

storybook.js.org
Build a simple component - Storybook Tutorials
Opens in a new window

storybook.js.org
Write an addon | Storybook docs
Opens in a new window

storybook.js.org
Storybook Addons | Storybook docs
Opens in a new window

storybooks.netlify.app
Writing Addons - Storybook
Opens in a new window

support.claude.com
What are artifacts and how do I use them? - Claude Help Center
Opens in a new window

albato.com
How to use Claude Artifacts: 7 Ways with examples | Guide 2025 - Albato
Opens in a new window

zapier.com
How to use Claude Artifacts to create web apps - Zapier
Opens in a new window

learn.microsoft.com
Foundations - Error Handling In Workflows - Microsoft Learn
Opens in a new window

docs.aws.amazon.com
Retry with backoff pattern - AWS Prescriptive Guidance
Opens in a new window

docs.aws.amazon.com
AWS CLI retries in the AWS CLI - AWS Command Line Interface - AWS Documentation
Opens in a new window

orkes.io
Handling Failures | Orkes Conductor Documentation
Opens in a new window

betterstack.com
Mastering Exponential Backoff in Distributed Systems | Better Stack Community
Opens in a new window

hackerone.com
Retrying and Exponential Backoff: Smart Strategies for Robust Software | HackerOne
Opens in a new window

learn.microsoft.com
Troubleshoot Azure Developer CLI | Microsoft Learn
Opens in a new window

userpilot.com
How I Use Session Replay as a UX Researcher - Userpilot
Opens in a new window

contentsquare.com
UX Analysis: A 6-Step Strategy & Framework for Improving UX - Contentsquare
Opens in a new window

userinterviews.com
User Interviews for UX Research: What, Why & How
Opens in a new window

askable.com
5 advanced ChatGPT prompts to streamline your UX research - Askable
Opens in a new window

undo.io
CI/CD - What To Do About Failing Tests - Time Travel Debugging for C/C++ and Java ¬¶ Undo
Opens in a new window

developers.google.com
About this guide | Google developer documentation style guide
Opens in a new window

technicalwriterhq.com
Technical Writer Style Guide Examples
Opens in a new window

developer.adobe.com
Telemetry - Adobe Developer
Opens in a new window

marcon.me
6 telemetry best practices for CLI tools - Massimiliano Marcon
Opens in a new window

learn.microsoft.com
NET SDK and .NET CLI telemetry - Microsoft Learn
Opens in a new window

encore.dev
Telemetry - Encore Docs