Performance Instrumentation Strategy for Sprint 18
I. Executive Summary: Recommended Performance Instrumentation Stack
This report presents a comprehensive performance instrumentation strategy designed to meet the exit criteria for Sprint 18. The primary objective is to establish a robust, automated system for monitoring and preventing regressions across key performance budgets: compositor interactions, list rendering, token transformation, and usage aggregation.

The core recommendation is the adoption of a hybrid instrumentation stack centered on Playwright as the primary automation and measurement driver. This foundational tool will be augmented by Google Lighthouse for capturing standardized Web Vitals and direct instrumentation via the browser's User Timing API and the React Profiler API for high-precision, application-specific metrics.

This hybrid approach provides the optimal balance of high-fidelity, real-browser testing, granular metric capture, and stable Continuous Integration (CI) automation. It moves beyond the limitations of framework-specific addons to create a durable, scalable, and precise performance harness. The strategy is designed to directly address the mission's objectives by enabling the measurement of specific, low-level operations that are critical to user-perceived performance.

Implementation of this strategy will yield three key outcomes:

A reliable, automated performance regression detection system integrated into the CI/CD pipeline, configured to alert on statistically significant deviations greater than 15%.

A standardized and streamlined developer workflow for local, pre-commit performance validation, empowering engineers to "shift left" on performance.

A structured, queryable dataset of performance metrics logged to diagnostics.json, establishing a historical record for trend analysis and deeper performance investigations.

This document provides a detailed analysis of tooling options, a formal methodology for metric capture, a schema for diagnostics integration, a defined CI cadence, and a concrete implementation plan with actionable tasks for the engineering team.

II. Tooling Analysis and Rationale
The selection of a tooling stack is the foundational decision for a durable performance harness. The following analysis evaluates the leading options against critical criteria derived from the mission's objectives, culminating in a justified recommendation for a hybrid stack.

2.1. Evaluation Criteria
To ensure the selected tools meet the specific and demanding requirements of this project, each option is assessed against the following criteria:

Metric Precision: The ability to measure the specific, low-level metrics required by the mission, such as compositor update time, arbitrary JavaScript function duration (token transform), and React component render cycles. This goes beyond standard page-load timings.

CI/CD Stability: The reliability, consistency, and performance of the tool when executed in headless, automated environments. The tool must produce low-variance results to minimize false-positive regression alerts.

Developer Workflow Integration: The ease with which developers can run the performance harness locally to validate their changes before committing code. The workflow should be simple, fast, and provide clear, actionable feedback.

Cross-Browser Capability: The ability to execute performance tests across all supported browser engines: Chromium, Firefox, and WebKit, as is standard for comprehensive web application testing.   

Ecosystem & Maintainability: The long-term viability, community support, and integration capabilities of the tool. A strong ecosystem ensures the tool will evolve and remain supported.

2.2. Option 1: Storybook-Native Instrumentation (Analysis & Verdict)
This approach leverages tools and addons that operate directly within the Storybook ecosystem. The primary candidates are storybook-addon-performance and the official @storybook/test-runner.

Strengths:

Seamless Developer Experience: The most significant advantage is the tight integration with the local developer workflow. Addons like storybook-addon-performance display performance metrics directly within the Storybook UI, providing immediate, interactive feedback as components are developed.   

Powerful Interaction Measurement: The storybook-addon-performance addon provides a robust API for defining and measuring custom user interaction tasks. It also includes a valuable feature to save and load performance results, which facilitates manual comparison of before-and-after changes.   

React-Specific Insights: These tools often integrate with the React Profiler, offering detailed insights into component render times and commit phases.   

Weaknesses:

High CI Variability: The primary drawback is the potential for high variability in performance results, especially in a shared CI environment. The addon's documentation explicitly warns that results can fluctuate significantly based on CPU utilization and memory at the time of the run, making it difficult to establish a stable baseline for automated regression detection. Achieving consistency would require a highly controlled, dedicated hardware environment.   

Limited Scope: While excellent for React component profiling, these tools are less suited for measuring the performance of arbitrary, non-UI JavaScript logic like the specified "token transform" and "usage aggregation" functions. A more general-purpose instrumentation method is required for these.

Focus on Debugging over Data Extraction: The @storybook/test-runner is primarily designed for functional, interaction, and accessibility testing, verifying that stories render without errors and that user interactions behave as expected. While it can run tests in CI, its core purpose is not high-precision performance measurement and programmatic data extraction for diagnostics.   

Verdict: Not recommended as the primary stack. The potential for CI flakiness and the narrow focus on React component rendering make this approach unsuitable for building the authoritative, automated performance harness required by the mission. However, these tools are invaluable for local development and debugging. Therefore, storybook-addon-performance will be recommended as a complementary tool for developers to use during the component authoring phase.

2.3. Option 2: Headless Browser Automation with Playwright (Analysis & Verdict)
This approach utilizes a dedicated browser automation framework, such as Playwright, to programmatically control a real browser, navigate to the relevant Storybook URL, execute interactions, and collect performance data.

Strengths:

Maximum Control and Fidelity: Playwright automates a real, unmodified browser engine, ensuring that all measurements—from network requests to rendering and script execution—reflect the actual user experience with the highest possible fidelity. It provides first-class support for Chromium, Firefox, and WebKit, satisfying the cross-browser requirement.   

Powerful Measurement Primitives: Playwright is more than just a clicking and typing tool. It offers deep introspection into browser operations. Its built-in tracing capabilities can capture detailed performance timelines, network activity, and rendering information, akin to a programmatic version of the Chrome DevTools Performance panel. Crucially, it allows for the execution of arbitrary JavaScript within the page context, enabling direct access to browser-native APIs like the User Timing API (performance.mark, performance.measure) and the React Profiler API, which are essential for capturing the custom metrics specified in the mission.   

Designed for CI/CD: Playwright is engineered from the ground up for stable, reliable, and efficient headless execution. Its architecture minimizes flakiness, making it an ideal choice for integration into an automated CI/CD pipeline where consistency is paramount.   

Stateful and Complex Interactions: A key requirement is to measure the performance of dynamic components, such as a list after it has been populated with data and filtered by the user. Playwright excels at scripting these complex, stateful user workflows, a capability where static analysis tools fall short.   

Weaknesses:

Higher Initial Setup Overhead: Compared to installing a Storybook addon, setting up a Playwright test suite requires more initial configuration and boilerplate code.

No "Out-of-the-Box" Score: Unlike Lighthouse, Playwright does not provide a pre-packaged performance score or a set of standard metrics. It is a powerful but unopinionated tool; the logic to define, capture, and analyze performance metrics must be custom-built.   

Verdict: Recommended as the core automation driver. Playwright's unparalleled control, high-fidelity measurement capabilities, and CI stability make it the ideal foundation for this project. It provides the necessary power and flexibility to build a durable, precise, and maintainable performance harness that can evolve with the application.

2.4. Option 3: Lighthouse-Driven Auditing (Analysis & Verdict)
This approach involves using Google Lighthouse, the industry-standard tool for web page quality audits, to analyze the performance of Storybook components.

Strengths:

Standardized, Actionable Metrics: Lighthouse provides a suite of well-understood, user-centric metrics, including the Core Web Vitals (LCP, CLS) and others like Time to Interactive (TTI) and First Contentful Paint (FCP). It generates a single, easy-to-understand performance score that is useful for high-level health checks and executive reporting.   

Rich Diagnostics and Recommendations: A key benefit of Lighthouse is that it not only reports on metrics but also provides a list of actionable recommendations for improving performance, such as identifying render-blocking resources or unused JavaScript.   

Weaknesses:

Poor Handling of Dynamic State: Lighthouse, when used in isolation, is designed to analyze a page from a "cold" load. It cannot, by itself, perform the necessary user interactions to get a component into a specific state for testing (e.g., measuring a list's render performance after a filter has been applied). It is fundamentally a static analysis tool for a given URL.   

Insufficient Metric Granularity: The metrics provided by Lighthouse are page-load oriented. They are not granular enough to measure the duration of a specific, short-lived JavaScript function like "token transform" or to isolate the precise "compositor ms" time for a component update triggered by a user interaction.

Verdict: Recommended as a powerful supplement, but not as a standalone solution. The limitations of Lighthouse in handling dynamic application states make it unsuitable as the sole tool. However, the industry-standard best practice is to combine Playwright and Lighthouse. In this pattern, Playwright acts as the orchestrator, scripting the complex user interactions to bring the component to the desired state. Once the component is ready, Playwright programmatically invokes a Lighthouse audit on that specific, stateful page. This hybrid approach leverages the strengths of both tools: Playwright's dynamic interaction capabilities and Lighthouse's standardized auditing and reporting.   

The evolution of web development tooling reveals a clear convergence between end-to-end (E2E) testing frameworks and performance measurement tools. Historically, component testing was often confined to simulated DOM environments like JSDOM, which are incapable of measuring real-world rendering and paint performance. The need for higher fidelity drove the adoption of real-browser automation tools like Playwright. The mission's requirements—measuring compositing and painting—explicitly necessitate a real browser environment. Playwright's architecture provides deep, programmatic control over this environment, making it a natural platform not only for functional validation but also for performance observation. Its built-in tracing feature is effectively a scriptable interface to the Chrome DevTools Performance panel. This convergence implies that the most efficient and powerful strategy is to unify functional and performance testing on a single platform. By building the performance harness on Playwright, the same foundation used for E2E testing, we can reduce toolchain complexity, reuse interaction logic and page objects, and ultimately create a more maintainable and holistic testing suite.   

2.5. Recommended Stack and Tooling Trade-Offs
Based on the preceding analysis, the recommended stack is a hybrid model leveraging Playwright as the primary test orchestrator, programmatically invoking Lighthouse for standardized audits, and using browser-native APIs for custom, high-precision metrics.

The following table provides a summary of the trade-offs between the evaluated tooling approaches.

Tool/Approach	Metric Precision	CI/CD Stability	Developer Workflow	Cross-Browser Support	Key Strengths	Key Weaknesses
storybook-addon-performance	High (for React)	Low	Excellent	N/A (Browser-dependent)	Excellent for local debugging; tight Storybook UI integration.	High result variability in CI; primarily for React components.
@storybook/test-runner	Low	High	Good	High (Playwright-based)	Excellent for functional/interaction tests in CI.	Not designed for precision performance measurement.
Playwright (standalone)	Very High	Very High	Good	Very High (Cr, Fx, Wk)	Total control over browser; stable in CI; can measure anything.	Requires custom logic for metric capture and analysis.
Lighthouse (standalone)	Medium	High	Good	Chromium Only	Industry-standard metrics; actionable recommendations.	Cannot test dynamic states; metrics are not granular enough.
Playwright + Lighthouse (Hybrid)	Very High	Very High	Good	Very High	Combines strengths of both; tests dynamic states with standard audits.	Highest initial setup complexity.
This recommended hybrid stack provides the most comprehensive and robust solution, directly addressing all requirements of the mission while establishing a scalable and future-proof foundation for performance monitoring.

III. Measurement and Diagnostics Integration Strategy
A successful performance harness requires not only the right tools but also a precise methodology for capturing metrics and a well-defined schema for storing the resulting data. This section details the proposed approach for instrumentation and integration with the existing diagnostics.json system.

3.1. Metric Capture Methodology
The specific nature of each required metric dictates the optimal capture technique. A multi-faceted approach using the React Profiler API, the browser's User Timing API, and programmatic Lighthouse runs will be employed.

Compositor & List Render Time: These metrics relate to the time it takes for the UI to update in response to a state change. They are best measured using a combination of framework-aware and browser-native APIs to capture a complete picture.

React Profiler API: The target list component and any other components under scrutiny will be wrapped in a React <Profiler> component. The profiler's onRender callback provides highly accurate timing data for the component tree's render and commit phases. Specifically, the actualDuration metric, which measures the time spent rendering the component and its descendants, will be captured. This gives a precise, framework-aware measurement of the work done by React itself, excluding time the main thread might be blocked by other tasks. The Playwright test script will trigger an action (e.g., loading data into the list) and programmatically retrieve this value from the onRender callback.   

User Timing API: To capture the end-to-end user-perceived duration, the User Timing API will be used as a complementary measure. A performance.mark() will be created immediately before the action that triggers the re-render is dispatched in the Playwright script. A second performance.mark() will be placed within a useEffect or componentDidUpdate hook that runs after the component has finished rendering to the DOM. A performance.measure() call will then calculate the total duration between these two marks. This captures the entire lifecycle from user input to final paint, providing a holistic view of the component's responsiveness.   

Token Transform Duration: This metric pertains to a pure JavaScript operation, likely a function that processes data without direct UI interaction. This is a classic use case for the User Timing API. The function responsible for the transformation will be instrumented by wrapping its execution with performance.mark('transformStart') and performance.mark('transformEnd'). A subsequent call to performance.measure('tokenTransformDuration', 'transformStart', 'transformEnd') will create a high-resolution timestamp in the browser's performance timeline, which can be easily extracted by the Playwright test runner.   

Usage Aggregation Performance: Similar to the token transform, this is expected to be a client-side data processing task. The entry and exit points of the primary aggregation function will be instrumented using the same performance.mark/measure pattern to capture its execution duration with high precision.

General Performance Budgets (Web Vitals): To provide a baseline of overall component health and catch regressions in standard user-centric metrics, each key Storybook scenario will be subjected to a programmatic Lighthouse audit. The Playwright script will first navigate to the component's story and perform any necessary setup actions. It will then invoke Lighthouse to analyze the page in its current state. Key metrics such as Largest Contentful Paint (LCP), Time to Interactive (TTI), and Total Blocking Time (TBT) will be extracted from the Lighthouse report and logged to diagnostics.   

3.2. diagnostics.json Performance Schema Definition
To ensure that performance data is stored in a consistent, structured, and queryable format, a formal schema will be defined for the diagnostics.json file. Adhering to a JSON Schema provides a clear data contract, simplifies downstream processing, and enables automated validation.   

The proposed structure introduces a top-level performanceHarness object. This object will contain metadata about the test run and an array of snapshots, where each snapshot represents a single, atomic performance measurement.

Top-Level Schema Structure:

JSON
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Performance Harness Results",
  "type": "object",
  "properties": {
    "performanceHarness": {
      "type": "object",
      "properties": {
        "version": { "type": "string", "description": "Version of the harness schema." },
        "runTimestamp": { "type": "string", "format": "date-time", "description": "ISO 8601 timestamp of the test run." },
        "commitSha": { "type": "string", "description": "The git commit SHA that was tested." },
        "environment": { "type": "string", "enum": ["CI", "local"], "description": "The environment where the test was run." },
        "snapshots": {
          "type": "array",
          "items": {
            "$ref": "#/definitions/snapshot"
          }
        }
      },
      "required":
    }
  },
  "definitions": {
    "snapshot": {
      // Snapshot schema defined in the table below
    }
  }
}
The following table provides the formal schema definition for an individual snapshot object. This schema is the critical data contract that will govern all performance metric reporting.

Field Name	Data Type	Description	Example
snapshotId	string (uuid)	A unique identifier for this specific measurement instance.	"a1b2c3d4-e5f6-7890-1234-567890abcdef"
scenarioId	string	A dot-notation identifier for the test scenario.	"List.With1000Items.Filtered"
metricName	string	A dot-notation identifier for the specific metric being measured.	"React.actualDuration"
value	number	The measured numerical value of the metric.	123.45
unit	string	The unit of measurement for the value.	"ms"
browser	string	The browser engine on which the test was executed.	"chromium"
parameters	object	Key-value pairs describing the specific configuration of this run.	{"tenancyMode": "enterprise", "featureFlags": {"new-list-virtualization": true}}
isRegression	boolean	A flag indicating if this measurement was identified as a regression by the CI process.	true
baselineValue	number	The baseline value against which this measurement was compared.	105.21
baselineSourceCommitSha	string	The git commit SHA from which the baseline value was derived.	"fedcba098765"
This structured approach ensures that every piece of performance data is captured with sufficient context to be meaningful. It allows for sophisticated querying and analysis, such as comparing the performance of the same scenario across different browsers, tenancy modes, or feature flag configurations.

IV. Automation Cadence and Regression Alerting
An effective performance monitoring system must balance the need for rapid feedback to developers with the need for stable, reliable regression detection. Running a full, statistically significant performance suite on every commit can be slow, costly, and introduce friction into the development process. Therefore, a tiered CI cadence is proposed to optimize for both speed and accuracy.   

4.1. A Tiered CI Cadence Strategy
Tier 1: PR/Commit Check (Gating): This is a lightweight, fast-executing check that runs on every pull request submission. Its purpose is not to detect minor regressions but to act as a smoke test to catch catastrophic performance issues (e.g., an infinite loop, a memory leak causing a crash) before code is merged into the main branch. This tier will execute a small, representative subset of performance scenarios—perhaps one simple component render and one instrumented JavaScript function. A failure at this stage provides immediate, blocking feedback to the developer, preventing major issues from entering the primary codebase.   

Tier 2: Post-Merge/Nightly Main Branch Run (Monitoring): This is the authoritative performance measurement run. The full performance harness suite will be executed against the main branch, either on a nightly schedule or triggered after a merge. This run operates in a non-blocking fashion, meaning it does not prevent subsequent merges. Its outputs are considered the source of truth for performance data. The diagnostics.json file generated from this run is archived and used to update the performance baselines and detect regressions over time. This cadence provides the most stable and reliable data, as it runs against a known-good state of the codebase in a more controlled, off-peak environment.   

4.2. Establishing and Maintaining Dynamic Baselines
A static performance target (e.g., "list render must be under 100ms") is brittle and fails to adapt to the natural evolution of the application. A more robust approach is to establish a dynamic baseline that reflects the current, accepted performance characteristics of the application.

The proposed process for baselining is as follows:

The diagnostics.json artifacts from the last N successful Tier 2 runs on the main branch will be retrieved from the artifact store (e.g., for N=10).

For each unique metric (a combination of scenarioId, metricName, browser, and parameters), a baseline will be calculated.

The baseline value will be the median of the N historical data points. The median is chosen over the mean (average) as it is more resilient to outliers, providing a more stable central tendency.   

Alongside the median, the standard deviation of the N data points will also be calculated. This measures the historical variability or "noise" of the metric.   

This baseline data (median and standard deviation) will be stored and used for comparison in the next performance run.

This moving-window approach ensures that the baseline automatically adapts to gradual, intentional performance improvements or changes, while still being stable enough to detect sudden, unintentional regressions.   

4.3. Statistical Regression Thresholding
The mission's success criterion of alerting on ">15% regressions" provides a useful starting point, but a naive implementation can lead to a noisy and unreliable alerting system. A 15% regression on a 10ms metric (1.5ms) is likely statistical noise, whereas on a 1000ms metric (150ms) it is highly significant. Robust performance monitoring is fundamentally a data analysis problem, requiring a statistical approach to differentiate a true signal from environmental noise.

A performance test run does not produce a single, immutable number; it produces a data point from a distribution. A regression is therefore a statistically significant deviation from the baseline distribution.

To address this, an alert for a performance regression will be triggered only when a new measurement meets a combination of three conditions:

Statistical Significance: The new value must be statistically unlikely to be part of the baseline distribution. Using the "3-sigma rule," a value is considered an outlier if it falls outside three standard deviations from the mean (or median, in our case). The check will be: newValue > (baselineMedian + 3 * baselineStdDev). This ensures that we only alert on changes that are highly unlikely to be the result of random CI environment fluctuations.   

Absolute Significance: The absolute change must exceed a minimum practical threshold. This prevents alerts on trivial changes that, while statistically significant for low-variance metrics, have no user-perceptible impact. For example, an alert will only trigger if (newValue - baselineMedian) > 50ms. This threshold can be tuned per metric class.

Relative Significance: The percentage change must exceed the business requirement of 15%. This is calculated as ((newValue - baselineMedian) / baselineMedian) * 100 > 15.

An alert is triggered only if all three conditions are met. This multi-faceted approach creates a high-signal, low-noise system. When a regression is detected, the CI job will fail, and a diagnostic report will be automatically generated and logged. This report will include the metric name, the new value, the baseline value and standard deviation, and a link to the detailed Playwright trace file to aid in debugging.

V. Reconciling Performance with Application State
Modern applications are not monolithic; their behavior and performance can vary significantly based on user context, entitlements, and feature flags. The performance harness must be able to account for this variability to provide a complete and accurate picture.

5.1. Parameterized Scenarios for Tenancy and Usage Modes
The requirement to test different "tenancy modes" and "usage ingest modes" is a direct application of the data-driven testing (DDT) methodology. Instead of creating separate, duplicative test scripts for each mode, a single, flexible test will be created that can be parameterized with different configurations.   

Implementation:

A series of configuration files (e.g., enterprise-mode.json, free-mode.json) will be created. These files will define the specific conditions for each mode, which might include different API endpoints to mock, specific user credentials, or UI settings to apply.

The core Playwright test scripts will be designed to accept one of these configurations as an input parameter. The test logic will remain generic (e.g., "load 1000 items into the list"), but the setup phase of the test will use the provided configuration to put the application into the correct state before measurements begin.

The test runner will be configured to iterate through all defined configuration files, executing the same performance test logic for each distinct mode.   

Crucially, the specific configuration used for each test run will be recorded in the parameters field of the diagnostics.json snapshot. This allows for the establishment of separate and independent performance baselines for each mode. For example, the system will track the List.Render performance for tenancy=enterprise completely separately from tenancy=free.

This parameterized approach avoids code duplication, simplifies maintenance, and creates a scalable architecture that can easily accommodate the addition of new tenancy or usage modes in the future without requiring new test development.   

5.2. Instrumentation for Feature-Flagged Variations
Feature flags introduce a combinatorial explosion of possible application states, making it infeasible to test every possible permutation. However, for new, performance-critical features being developed behind a flag, it is essential to measure their impact before a full rollout.   

Strategy: The performance suite will be explicitly designed to test key user scenarios with the performance-critical feature flag in both its enabled and disabled states. This allows for a direct, A/B-style comparison of the performance impact.

Implementation:

Playwright tests provide the necessary control to manipulate the application's state to force a feature flag into a desired configuration for the duration of a test run. This can be achieved by setting a specific cookie, a value in local storage, or passing a designated query parameter that the application's feature flagging client will recognize.   

The test scenario will be executed twice: once with the flag forced "on" and once with it forced "off".

The results from both runs will be logged as separate snapshots in diagnostics.json. The parameters field will be used to record the state of the feature flag for each run (e.g., {"featureFlags": {"new-list-virtualization": true}}).

This methodology generates two distinct and directly comparable performance profiles for the same scenario. This data is invaluable for making informed decisions about the feature's production readiness and ensures that a new feature does not introduce an unexpected performance regression before it is rolled out to the entire user base.   

VI. Developer Workflow: Local Harness Execution
A core principle of modern performance engineering is to "shift left," enabling developers to identify and address performance issues early in the development cycle. To facilitate this, a simple and efficient local workflow for running the performance harness is essential.

6.1. Objective
The goal is to empower every developer to quickly and easily check the performance impact of their code changes on their local machine before creating a pull request. This proactive approach will reduce the frequency of CI pipeline failures, shorten feedback loops, and foster a culture of performance ownership.

6.2. Proposed Workflow
The developer experience will be streamlined into a single, intuitive command-line interface.

A developer makes code changes to a component within their local feature branch.

Before committing, they run a single command from their terminal: npm run perf-harness -- --scenario List.With1000Items --browser chromium

This command will trigger an automated sequence of events:

It will first build and start a production-optimized version of the local Storybook instance. Using a production build is critical for obtaining accurate and consistent performance metrics, as development builds contain extra overhead that can skew results.   

Next, it will invoke the Playwright test runner, instructing it to execute only the specified performance scenario (List.With1000Items) against the locally running Storybook.

Upon completion, the script will fetch the last known baseline data for that scenario from the main branch (e.g., from a shared artifact repository or a dedicated service).

It will then output a clear, concise summary table to the console, comparing the new local measurement against the established baseline, highlighting any potential regression.

Finally, it will generate a local diagnostics.json file and a Playwright trace file. These artifacts can be used by the developer for more in-depth analysis and debugging using the Playwright Trace Viewer if the summary indicates a problem.

6.3. Tooling
This workflow will be implemented as a simple Node.js CLI script that wraps the core Playwright test suite. This wrapper will handle the logic of starting the Storybook server, parsing command-line arguments to select scenarios, fetching baseline data, and formatting the output for the console. This approach makes the powerful underlying harness easily accessible without requiring developers to interact directly with the complexities of the Playwright configuration.

VII. Gaps, Uncertainties, and Proposed Follow-ups
A transparent assessment of potential risks and uncertainties is critical for successful project execution. This section identifies known gaps in the current plan and proposes concrete follow-up actions to mitigate them.

7.1. CI Environment Variability
Uncertainty: The performance of tests running on shared CI runners can be affected by factors outside of the test's control, such as other jobs competing for CPU, memory, and I/O resources. This "noisy neighbor" problem can introduce significant variability into performance measurements, potentially undermining the stability of our statistical baselines and leading to false-positive or false-negative regression alerts. The exact degree of this variability on the current infrastructure is unknown.

Proposed Follow-up: A two-phased approach is recommended to address this uncertainty.

Phase 1 (Sprint 18 Implementation): The performance harness will be implemented on the existing shared CI runners. For the first 1-2 sprints of operation, the primary goal will be data collection. The system will monitor and log the standard deviation of key metrics to quantify the inherent variability of the environment. Alerting may be temporarily disabled or set to a higher threshold during this calibration period.

Phase 2 (Sprint 19+ Analysis and Mitigation): If the data collected in Phase 1 indicates that the environmental variability is too high to maintain a stable baseline (e.g., the standard deviation is consistently greater than 5-10% of the mean), a proposal will be made to provision a dedicated, isolated pool of CI runners exclusively for the performance test suite. A stable and consistent hardware environment is a known best practice for reducing measurement noise and ensuring the reliability of performance tests.   

7.2. Measuring "Compositor ms"
Uncertainty: The term "compositor ms" in the mission objective is ambiguous. It could refer to several different things:

The duration of React's "commit" phase, where it applies changes to the DOM.

The end-to-end time from a state change until the browser has painted the resulting pixels to the screen.

The time spent specifically on the browser's internal compositor thread, which is responsible for layering and drawing textures to the screen.

Measuring the third interpretation—the actual work on the compositor thread—is not directly possible from within the JavaScript execution context of a web page. It requires analyzing low-level browser performance traces, which is a more complex form of instrumentation.   

Proposed Follow-up:

Clarification and Assumption: This implementation will proceed under the assumption that "compositor ms" refers to the user-perceptible time from an interaction to a completed screen update (interpretation #2), which is the most relevant metric for user experience. The combination of the React Profiler API and the User Timing API, as described in Section 3.1, is designed to measure this effectively.

Precise Metric Naming: To avoid future ambiguity, the captured metric will be named precisely, for example, ReactCommitAndPaintDuration or UserInteractionToNextPaint. This clarifies exactly what is being measured.

Future Exploration: If deeper analysis of the browser's internal rendering pipeline, including the compositor and raster threads, is required in the future, it will be scoped as a separate technical research mission. This would likely involve programmatically capturing and analyzing Chrome trace files, a significantly more advanced technique that is beyond the scope of this initial harness implementation.

VIII. Build Implications and Implementation Plan
This section translates the research and strategy into a concrete, actionable implementation plan. The following tasks are designed to be executed during Sprint 18, providing clear guidance for the assigned build and QA/perf missions.

Task 1: Harness Scaffolding and Playwright Setup

Description: Initialize a new Playwright project within the main application repository. Configure the playwright.config.ts file to target the Storybook development server URL. Create the basic directory structure for tests, page objects, and utility functions. Implement a single, simple "hello world" test that launches a browser, navigates to the Storybook index, and verifies the page title to ensure the foundational setup is correct.

Mission: Build

Task 2: Implement Metric Capture for Core Scenarios

Description: Instrument the target application components and functions. Wrap the Compositor and List demo components with the React <Profiler> component. Implement the onRender callback and expose the captured timing data (e.g., via a global object or a custom event) so it can be retrieved by Playwright. For the Token Transform and Usage Aggregation functions, inject performance.mark() and performance.measure() calls around the core logic to enable measurement via the User Timing API.

Mission: QA/Perf

Task 3: Develop Playwright Test Scenarios

Description: Author the Playwright test scripts for each of the four target areas (compositor, list, token transform, usage aggregation). These scripts will contain the logic to navigate to the correct Storybook story, perform necessary user interactions (e.g., clicking buttons, typing in filters, loading data), and then programmatically collect the performance measurements exposed by the instrumentation in Task 2 from the browser's performance timeline.

Mission: QA/Perf

Task 4: Define and Implement diagnostics.json Schema

Description: Create the formal JSON Schema file (performance-harness.schema.json) that defines the data contract for the performanceHarness object, as specified in the table in Section 3.2. Update the Playwright test scripts from Task 3 to format their output results into a JSON object that conforms strictly to this schema.

Mission: Build

Task 5: CI Integration - Nightly Job and Baselining

Description: Create a new CI workflow (e.g., in GitHub Actions, GitLab CI) that executes the full Playwright performance suite. Configure this job to run on a nightly schedule against the main branch. Implement steps within the job to: 1) Run the tests. 2) Store the resulting diagnostics.json file as a persistent build artifact. 3) Add a subsequent step that fetches the artifacts from the last 10 successful runs, calculates the baseline median and standard deviation for each metric, and stores this new baseline data as another artifact.

Mission: Build

Task 6: Implement Regression Alerting Logic

Description: Enhance the nightly CI job from Task 5. Add a final step that compares the diagnostics.json from the current run against the newly calculated baseline data. Implement the statistical thresholding logic defined in Section 4.3 (3-sigma, absolute threshold, and 15% relative threshold). If any metric is flagged as a regression, the script should fail the CI job, causing it to turn red. Configure CI notifications to post an alert to the appropriate engineering channel (e.g., Slack) upon failure, including a summary of the regressed metric.

Mission: Build

Task 7: Create Local Developer Workflow CLI

Description: Develop the npm run perf-harness script as specified in Section VI. This Node.js script will act as a user-friendly wrapper around Playwright, handling the logic for starting a production Storybook server, running a targeted test scenario, and printing a comparative summary to the console. This script should be added to the package.json.

Mission: Build

Task 8: Documentation and Onboarding

Description: Author two key pieces of documentation. First, a developer-facing guide in the project's repository (e.g., PERFORMANCE_TESTING.md) that explains how to run and interpret the local performance harness. Second, a runbook for the on-call or performance team that details how to investigate a CI performance regression alert, including how to use the Playwright trace files and interpret the diagnostics.json output.

Mission: QA/Perf


dev.to
End-to-End SEO Testing with Playwright and Lighthouse - DEV Community
Opens in a new window

blog.planetargon.com
End-to-End SEO Testing with Playwright and Lighthouse - Planet Argon Blog
Opens in a new window

storybook.js.org
Performance | Storybook integrations
Opens in a new window

storybook.js.org
Story React Profiler Addon - Storybook
Opens in a new window

reddit.com
Introducing storybook-addon-performance : r/reactjs - Reddit
Opens in a new window

storybook.js.org
Test runner | Storybook docs
Opens in a new window

storybook.js.org
How to test UIs with Storybook | Storybook docs
Opens in a new window

hackernoon.com
Go Beyond Functional Testing: How to Use Playwright for Front-End Performance Insights
Opens in a new window

qualiti.ai
Playwright Performance Testing - Tutorial & 7 Best Practices - Qualiti
Opens in a new window

checklyhq.com
Measuring Page Performance Using Playwright - Best Practices - Checkly Docs
Opens in a new window

aragorn-talks.beehiiv.com
Mastering Playwright Performance Testing: Strategies for Optimizing Web Application Performance - Aragorn Talks
Opens in a new window

geekyants.com
Playwright and Lighthouse: Optimize Web Performance Like a Pro ...
Opens in a new window

testrig.medium.com
Performance Testing Using Playwright and Lighthouse: Automate What Matters Most
Opens in a new window

testingplus.me
How to Integrate Lighthouse with Playwright for Web Performance Testing (2025 Guide)
Opens in a new window

react.dev
<Profiler> – React
Opens in a new window

walkingtree.tech
Measuring Component Performance using React Profiler API - WalkingTree Technologies
Opens in a new window

deadsimplechat.com
React Profiler: A Step by step guide to measuring app performance - DeadSimpleChat
Opens in a new window

developer.mozilla.org
Performance: measure() method - Web APIs - MDN Web Docs
Opens in a new window

country-code.ghost.io
Getting Started with Angular Performance Monitoring - Country Code - Ghost
Opens in a new window

imkev.dev
Learning the React reconciliation algorithm with performance measures - Kevin Farrugia
Opens in a new window

nodejs.org
Performance measurement APIs | Node.js v25.0.0 Documentation
Opens in a new window

json-schema.org
JSON Schema
Opens in a new window

devzery.com
JSON Schema Tests: Best Practices, Implementation, and Tools - Devzery
Opens in a new window

stackoverflow.com
Continuous Integration vs. Nightly Builds - Stack Overflow
Opens in a new window

devops.stackexchange.com
Should performance tests be run using a CI and how often? - DevOps Stack Exchange
Opens in a new window

jetbrains.com
Best Practices for Successful CI/CD | TeamCity CI/CD Guide - JetBrains
Opens in a new window

browserstack.com
Test Strategies for Daily and Nightly Builds | BrowserStack
Opens in a new window

perforce.com
How Does Continuous Integration Work? - Perforce Software
Opens in a new window

myshyft.com
Performance Baseline Strategies For Optimized Shift Management Implementation - Shyft
Opens in a new window

loadfocus.com
Quickly Establish a Performance Baseline: A Simple Guide for Immediate Results
Opens in a new window

ministryoftesting.com
How To Build A Performance Testing Stack From Scratch: Statistics For Testers
Opens in a new window

docs.microfocus.com
Baselines - Application Performance Management - OpenText Documentation Portal
Opens in a new window

browserstack.com
Data-Driven Testing: What it is, How it Works, and Tools to Use
Opens in a new window

tricentis.com
www.tricentis.com
Opens in a new window

tricentis.com
What is data-driven testing and why it matters - Tricentis
Opens in a new window

getxray.app
Test Parameterization Techniques - Xray Blog
Opens in a new window

qaratest.com
Parameterization of Test Cases: Enhancing Flexibility and Efficiency in Software Testing
Opens in a new window

aiotests.com
What is Parameterization in Testing and How Can It Improve Your Testing Efficiency?
Opens in a new window

medium.com
Unveiling the Power of Data Parameterization: A Cornerstone in Software Testing - Medium
Opens in a new window

leapwork.com
An Introduction to Data-Driven Testing - Leapwork
Opens in a new window

medium.com
Testing Feature Flags | by Martin Chaov | DraftKings Engineering - Medium
Opens in a new window

cloudbees.com
Feature Flag Testing: How to Run A/B Tests - CloudBees
Opens in a new window

statsig.com
4 best practices for testing with feature flags - Statsig
Opens in a new window

amplitude.com
What are Feature Flags? Best Practice Guide - Amplitude
Opens in a new window

chromium.org
Compositor Thread Architecture - The Chromium Projects
Opens in a new window

learn.microsoft.com
Performance features reference - Microsoft Edge Developer documentation