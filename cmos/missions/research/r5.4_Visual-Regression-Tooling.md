R5.4 Technical Research Report: Visual Regression Tooling Decision
1. Executive Summary & Recommendation
1.1. Final Recommendation
This report recommends the adoption of the Chromatic SaaS platform for visual regression testing of key Storybook components, including Foundations, Contexts, and Status Chips. Chromatic presents the most direct and sustainable path to achieving the project's success criteria by offering a superior developer experience, robust flake mitigation, and near-zero maintenance overhead. This approach prioritizes development velocity and collaborative efficiency over the granular control and zero-licensing-fee model of a self-hosted solution.

1.2. Key Justification
The recommendation for Chromatic is based on a comprehensive analysis of developer experience, total cost of ownership, and long-term maintainability. The key justifications are:

Superior Developer Experience & Collaboration: Chromatic is not merely a testing tool but a complete, managed review workflow. Its web-based UI provides a central hub for developers, designers, and product stakeholders to review and approve visual changes directly within the pull request, drastically shortening feedback loops. Â  

Minimal Maintenance Overhead: As a fully managed SaaS platform, Chromatic abstracts away the significant engineering complexities of infrastructure management, baseline storage, and CI pipeline debugging. This frees the development team from the responsibility of building and maintaining a custom testing infrastructure, allowing them to focus on product development.

Intelligent Performance and Stability: Features like TurboSnap, which intelligently tests only changed components, and a proprietary engine for pausing animations and stabilizing renders, directly address the project's performance and flake-rate targets with minimal configuration. Â  

1.3. Synopsis of Findings
The investigation compared Chromatic against a robust self-hosted alternative architected with Playwright and storycap-testrun. The analysis concludes that while the self-hosted path offers unparalleled control, customizability, and avoids subscription fees, these benefits are outweighed by the substantial upfront and ongoing investment required in engineering time. The self-hosted solution necessitates building and maintaining a complex system for CI integration, artifact storage (via Git LFS), and a custom PR-based review and approval workflow. Chromatic provides these critical workflow components out-of-the-box, representing a "buy" decision that delivers immediate value and aligns with a strategy of leveraging managed services to accelerate development. Â  

1.4. Projected Impact for Sprint 05
Upon adoption in Sprint 05, the team can expect an immediate positive impact on the PR review process. Onboarding is projected to be minimal, with initial setup and baseline generation for critical components achievable within a single day. The integration of visual diffs directly into PR checks will increase UI quality and consistency. The performance target of a sub-90-second snapshot suite is achievable from the outset due to Chromatic's TurboSnap feature, and the false-positive rate is expected to be well below the 2% target due to its automated flake mitigation capabilities.

2. The SaaS Path: A Deep Dive into Chromatic
Chromatic is positioned as a comprehensive visual testing and review service built by the maintainers of Storybook. This ensures a first-class integration and a deep understanding of the component-driven development workflow. Its value extends beyond simple pixel-by-pixel comparison, offering a managed platform that handles the entire lifecycle of visual testing, from snapshotting to collaborative review and baseline management. Â  

2.1. Developer Experience (DX) & Collaboration Workflow
The primary strength of Chromatic lies in its highly refined developer experience and integrated collaboration tools.

"Zero-Config" Onboarding: The initial setup is exceptionally streamlined. It requires installing a single npm package (chromatic) and running a command with a project-specific token obtained from the web UI. This process drastically reduces the initial time investment compared to architecting a multi-component self-hosted solution, enabling a team to go from zero to a functioning visual test suite in under an hour. Â  

Integrated UI Review Hub: Chromatic transforms visual testing from a developer-centric CI check into a collaborative review process. For each PR, it generates a unique build with a web-based UI where stakeholders (developers, designers, QA, product managers) can view visual diffs, leave comments, and formally approve or reject changes. This shared workspace keeps all UI-related decisions documented and tied directly to the relevant code changes, eliminating ambiguous feedback and shortening the overall review cycle. Â  

TurboSnap for Performance: To meet the sub-90-second performance target, especially as the component library grows, Chromatic employs a feature called TurboSnap. By enabling the --only-changed flag, Chromatic analyzes the project's dependency graph to intelligently determine which stories have been affected by a code change. It then re-snapshots only those stories, skipping unchanged components and significantly reducing test execution time and snapshot consumption. Â  

First-Party Storybook Integration: As a product from the Storybook team, Chromatic's integration is seamless and robust. It inherently understands Storybook's architecture, reducing the risk of integration friction, build errors, or compatibility issues that can arise when combining tools from different ecosystems. Â  

2.2. Performance & Flake Mitigation
A major challenge in visual regression testing is "flakiness"â€”false positives caused by minor, irrelevant rendering differences. Chromatic addresses this with a multi-layered, automated approach.

Automated Stability Engine: Chromatic utilizes a proprietary detection algorithm designed to eliminate flakiness. It automatically handles inconsistencies arising from resource loading latency, minor DOM structure changes, and other common sources of noise, without requiring manual configuration from the developer. While this "black box" approach sacrifices granular control, it provides a highly effective, zero-effort solution for the majority of use cases. Â  

Automatic Animation Handling: CSS/SVG animations and videos are a primary source of flaky tests. Chromatic proactively pauses these animations and resets them to their initial state before taking a snapshot, ensuring a deterministic visual output. This built-in behavior handles a major class of flakiness by default. Â  

Conditional Logic with isChromatic(): For complex components with JavaScript-driven animations or non-deterministic data (e.g., Date.now()), Chromatic provides an escape hatch. The isChromatic() utility function can be imported into stories to conditionally alter a component's rendering logic specifically when it's running in Chromatic's test environment. This allows for surgical adjustments to ensure deterministic rendering without affecting the component's behavior in the actual application. Â  

2.3. CI Integration & Baseline Management
Chromatic's architecture is designed to integrate seamlessly with modern Git and CI workflows.

Seamless CI Setup: Integration into GitHub Actions is straightforward using the official chromaui/action. This action is purpose-built and handles the necessary steps of checking out the full Git history (required for baseline detection) and running the Chromatic CLI with the correct context. Â  

Cloud-Based Baseline Management: This is a core architectural advantage over self-hosted solutions. Chromatic manages all baseline snapshots in its cloud infrastructure, tightly coupling them to the project's Git history. When a build runs on a feature branch, Chromatic automatically traverses the Git graph to find the most relevant ancestor commit on the main branch that has a successful build, using that as the baseline for comparison. This intelligent baseline detection completely abstracts away the complexities of storing, versioning, retrieving, and managing large binary snapshot files, which is a significant operational burden in self-hosted systems. Â  

2.4. Total Cost of Ownership (TCO) Analysis
The TCO for Chromatic is a balance of direct subscription costs against the significant savings in indirect engineering costs.

Direct Costs (Subscription): Chromatic's pricing is tiered based on monthly snapshot volume. The key tiers are : Â  

Free: 5,000 snapshots/month (Chrome only)

Starter: $179/month for 35,000 snapshots/month (includes Firefox, Safari, Edge)

Pro: $399/month for 85,000 snapshots/month

Snapshot Volume Projection: An initial projection for the critical stories is necessary. Assuming 10 critical components, each tested across 3 viewports and 3 browsers (a reasonable starting point for cross-browser coverage):
10Â storiesÃ—3Â viewportsÃ—3Â browsers=90Â snapshotsÂ perÂ build
Assuming an average of 20 CI builds per day across the team during active development over 22 working days:
90Â snapshots/buildÃ—20Â builds/dayÃ—22Â days/month=39,600Â snapshots/month
This volume places the project in the Pro tier ($399/month) or the Starter tier ($179/month) with overage charges ($0.008/snapshot). It is important to note that enterprise-level pricing can scale significantly, with median annual costs reported around $48,000. Â  

Indirect Costs (Savings): The primary financial benefit of Chromatic is the avoidance of engineering costs associated with a self-hosted solution. This includes the person-hours that would be spent on:

Initial setup and configuration of the entire testing pipeline.

Development and maintenance of CI workflows.

Management of an artifact storage solution like Git LFS.

Debugging environment-specific rendering inconsistencies.

Building a custom review and approval workflow.
These saved hours represent a significant and recurring financial advantage.

Risks & Dependencies: The primary risks of the SaaS path are dependency on an external service, which could experience downtime, and the recurring subscription cost. For projects with stringent data privacy requirements, storing UI snapshots on a third-party cloud could also be a concern. Â  

3. The Self-Hosted Path: A Deep Dive into Playwright with Storycap
A self-hosted visual regression testing pipeline offers maximum control, no subscription fees, and deep customizability. This path leverages the power of Microsoft's Playwright for browser automation and screenshotting, enhanced by the storycap-testrun library to ensure snapshot stability when testing Storybook components. However, this control comes at the cost of significant implementation and maintenance responsibility.

3.1. Developer Experience & Implementation Overhead
The developer experience for a self-hosted solution is fundamentally different from the managed workflow of Chromatic. It is a powerful, CLI-centric approach that places more responsibility on the individual developer.

Initial Setup Complexity: The setup is a multi-stage process involving the installation and configuration of several distinct tools. This includes setting up Playwright (@playwright/test), configuring Storybook's test-runner (@storybook/test-runner), and integrating @storycap-testrun/node to provide the necessary stability hooks. This process requires creating and orchestrating multiple configuration files (playwright.config.ts, .storybook/test-runner.mjs), demanding a deeper understanding of the underlying tooling. Â  

Local Workflow: The local development loop involves three distinct steps:

Start the Storybook development server.

Execute the Playwright test command (e.g., npx playwright test --config=storybook.config.ts).

If tests fail, open a locally generated HTML report to view the visual diffs.
This workflow is effective for individual developers but lacks the centralized, web-based review interface that facilitates collaboration with non-technical stakeholders. Â  

Baseline Updates: When UI changes are intentional, developers must manually update the baseline "golden" images by running the test command with an --update-snapshots flag. While this provides direct control, it is a potentially destructive operation that requires strict team protocols to prevent accidental overwrites of correct baselines. Â  

3.2. Performance Tuning & Advanced Flake Mitigation (The "White Box" Approach)
The self-hosted path provides a "white box" of configurable tools to combat flakiness, offering more transparency and control than Chromatic's automated engine.

storycap-testrun for Stability: This library is the cornerstone of a reliable self-hosted setup for Storybook. It brings the battle-tested stability mechanisms of the original storycap to the modern Storybook test-runner, providing two key features : Â  

Metrics Monitoring: It leverages the Chrome DevTools Protocol (CDP) to monitor browser rendering metrics, waiting until layout shifts, network activity, and paint events have ceased before capturing a screenshot.

Hash Verification: As an additional guarantee of stability, it can capture multiple screenshots in quick succession and compare their image hashes, only proceeding when consecutive captures are identical.

Playwright's Native Controls: Playwright itself offers a rich set of options within the toHaveScreenshot assertion to fine-tune the capture process and mitigate flake : Â  

Masking: The mask option accepts an array of Playwright locators, which will be covered by a solid color block in the snapshot. This is ideal for ignoring dynamic content like timestamps, user avatars, or generated data.

Disabling Animations: The animations: 'disabled' option automatically fast-forwards CSS transitions and Web Animations to their end state, providing a simple and effective way to stabilize components. For more complex cases, custom CSS can be injected to enforce prefers-reduced-motion. Â  

Network Throttling: For components with complex loading states, Playwright's context.route API or direct CDP commands can be used to simulate slow or unreliable network conditions. This allows for consistent capturing and testing of loading spinners and skeleton states, an advanced technique not readily available in Chromatic. Â  

Threshold Configuration: The playwright.config.ts file allows for global or per-project configuration of diffing sensitivity. Options like maxDiffPixels (an absolute number of different pixels) and threshold (per-pixel color difference tolerance) can be tuned to find the right balance between catching meaningful regressions and ignoring insignificant anti-aliasing artifacts. Â  

3.3. CI Integration & Baseline Management Architecture
This is the most complex aspect of the self-hosted path, as the team becomes responsible for building the entire infrastructure that Chromatic provides as a service.

CI Pipeline Setup: A robust CI pipeline is essential. The recommended approach uses GitHub Actions and relies on the official Microsoft Playwright Docker image (mcr.microsoft.com/playwright:v1.56.0-noble). Using a consistent Docker image for every run is critical to eliminate subtle rendering differences between the CI environment and local machines (e.g., Linux vs. macOS), which is a common source of persistent flakiness. Â  

Baseline Storage Strategy (Git LFS): Committing thousands of binary PNG snapshot files directly into a Git repository is not scalable and will quickly bloat the repository size, degrading performance. The industry-standard solution for this problem is Git LFS (Large File Storage). This strategy involves configuring Git to store the image files on a separate LFS server while keeping lightweight pointers in the main repository. This requires configuring .gitattributes and ensuring the CI environment has Git LFS installed and configured. Â  

PR Review & Approval Workflow: A custom, automated workflow must be built to replicate the review process. A highly effective pattern involves a two-part GitHub Actions setup : Â  

test-and-report-diffs.yml: This workflow runs on every PR push. It executes the Playwright tests. If visual diffs are detected (causing the test to fail), it uploads the generated HTML report as a build artifact. It then uses an action like thollander/actions-comment-pull-request to post a comment to the PR, notifying the author of the failure and providing a direct link to the downloadable artifact for review.

approve-and-update-snapshots.yml: This workflow is triggered by a specific PR comment, such as /approve-snapshots. When a developer posts this comment after reviewing the diffs, the action checks out the PR's branch, re-runs the tests with the --update-snapshots flag, and then uses an action like stefanzweifel/git-auto-commit-action to automatically commit and push the updated baseline images back to the PR branch.

3.4. Total Cost of Ownership (TCO) Analysis
The TCO for the self-hosted path has minimal direct costs but significant and recurring indirect costs in the form of engineering time.

Direct Costs:

CI/CD Minutes: Visual tests are more computationally intensive than unit tests, consuming more GitHub Actions minutes. This consumption will scale with the number of components and browsers tested.

Git LFS Data Packs: GitHub provides a free tier of 1 GB of storage and 1 GB of bandwidth per month for LFS. Based on the projection of ~2 GB/month of new snapshots, this would require purchasing additional data packs, which cost $5 per 50 GB pack.

Indirect Costs (Engineering Time): This is the largest and most critical component of the TCO. It includes the engineering hours required for:

Initial Setup: A significant one-time investment from a senior or DevOps engineer to architect the CI workflows, configure Docker environments, set up Git LFS, and perform initial flake-tuning.

Ongoing Maintenance: A recurring time cost for debugging pipeline failures, updating the Docker environment, managing LFS storage, and evolving the custom PR bot workflows as needs change.

Developer Onboarding: Time spent by every new team member to learn the CLI commands, the process for reviewing artifacts, and the PR comment-based approval workflow.

This model effectively inverts the responsibility. Instead of paying a vendor to manage a testing service, the organization pays its own engineers to build and operate a complex piece of internal infrastructure.

4. Head-to-Head Comparison Matrix
The following matrix summarizes the core trade-offs between the two solutions, mapping them directly to the key decision criteria outlined in the mission objective.

Criteria	Chromatic (SaaS)	Playwright + Storycap (Self-Hosted)
Developer Experience (DX)	
Highly integrated, web-based UI for review and collaboration. Near-zero config. Low cognitive load for developers. 

CLI-driven workflow. Requires manual review of CI artifacts. Higher cognitive load but offers maximum control. 

Total Cost of Ownership (TCO)	
Monthly subscription fee based on snapshot volume (~$179-$399/mo for this project). Minimal engineering maintenance cost. 

No license fees. Costs incurred via CI minutes and Git LFS data packs. Significant indirect cost in engineering hours for setup and maintenance.
Performance (Snapshot Suite)	
Optimized via TurboSnap (--only-changed). Cloud infrastructure handles parallelization. Performance is consistent but a "black box". 

Performance depends on CI runner specs and test parallelization configuration. Can be highly optimized but requires manual tuning. 

Flake Mitigation	
Automated, proprietary engine for pausing animations and handling latency. "Black box" approach. 

Granular, "white box" control via storycap-testrun (metrics, hash checks) and Playwright (masking, delays, network mocking). Powerful but requires expert configuration. 

CI Integration	
Mature, first-party GitHub Action. Handles Git history and baseline detection automatically. 

Requires building a custom, multi-part workflow for testing, reporting, and baseline approval. Requires Docker for environment consistency. 

Baseline Management	
Fully managed, cloud-based, and tied to Git history. Abstracted from the developer. 

Requires an explicit storage strategy (Git LFS recommended) and a custom-built PR-based approval workflow. Full ownership and control. 

Onboarding Time	Low. New team members can be productive within an hour, focusing on the web UI.	Medium to High. Requires understanding of Playwright, the CI workflow, the CLI commands, and the baseline approval process.
Â  
5. Final Recommendation and Adoption Plan
5.1. The Recommended Path for Sprint 05
The final recommendation is to adopt Chromatic for visual regression testing, starting in Sprint 05. This decision is based on the platform's ability to deliver the highest immediate value with the lowest long-term maintenance burden. It directly addresses the core requirements of reliable PR diffs, strong CI integration, and effective flake mitigation, while providing a superior collaborative workflow that will benefit the entire product team, not just developers.

5.2. Rationale and Acknowledged Trade-offs
The rationale for choosing Chromatic is a strategic decision to prioritize development speed and operational simplicity.

We recommend Chromatic, accepting its recurring financial cost and dependency on a third-party service in exchange for a superior developer experience, a significant reduction in ongoing maintenance liability, and faster team onboarding. This aligns with a strategic priority of maximizing feature development velocity and empowering the team with best-in-class tools that "just work."

The trade-offs are clear: we are choosing to pay a subscription fee to offload the complexity of building and maintaining a testing infrastructure. We are also accepting a "black box" approach to some aspects of performance and flake mitigation in exchange for automation and ease of use. This is deemed an acceptable trade-off given the team's primary focus on product delivery.

5.3. The Fallback Path
While Chromatic is the recommended path, a fallback plan is prudent. The team should reconsider this decision and evaluate a pivot to the self-hosted Playwright solution if any of the following conditions are met:

Cost Overrun: The annual cost of Chromatic exceeds the projected budget by more than 25% due to unforeseen snapshot volume.

Service Instability: The Chromatic service experiences significant downtime or performance degradation, with availability dropping below 99.5% for a sustained period (e.g., one fiscal quarter).

Workflow Limitations: The team discovers a critical testing requirement (e.g., a highly specific network condition simulation) that cannot be met by the Chromatic platform but is achievable with Playwright's advanced APIs.

Data Security Policy Change: The organization's data security policies change to prohibit the storage of UI snapshots on third-party cloud services.

6. Appendices: Ready-to-Use Configurations & Guides
6.1. Appendix A: Chromatic Implementation Kit
chromatic.config.json
This file should be placed in the root of the project.

JSON

{
  "$schema": "https://www.chromatic.com/config-file.schema.json",
  // Use TurboSnap to only test components affected by changes, maximizing speed and minimizing snapshot consumption. [3]
  "onlyChanged": true,
  // Prevents CI jobs from failing due to visual changes on feature branches, allowing for review before merging. The job will still fail for component errors. [28]
  "exitZeroOnChanges": "!(main)",
  // Automatically accept any visual changes on the 'main' branch after a PR is merged. This keeps the baseline up-to-date. [28]
  "autoAcceptChanges": "main",
  // Exclude non-code assets from TurboSnap's dependency analysis to improve accuracy.
  "externals": [
    "public/**",
    "**/*.mdx"
  ]
}
GitHub Actions Workflow (.github/workflows/chromatic.yml)
This workflow file automates the process of running Chromatic on every push.

YAML

# Workflow name
name: 'Chromatic Visual Tests'

# Run workflow on every push to any branch
on: push

jobs:
  chromatic-deployment:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        # The fetch-depth: 0 is crucial for Chromatic to correctly determine baselines by accessing the full Git history. [6]
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm' # Or 'yarn', 'pnpm'

      - name: Install dependencies
        run: npm ci # Or 'yarn install', 'pnpm install'

      - name: Publish to Chromatic
        # Use the official Chromatic GitHub Action. [11]
        uses: chromaui/action@latest
        with:
          # The project token must be stored as a secret in the repository settings.
          projectToken: ${{ secrets.CHROMATIC_PROJECT_TOKEN }}
6.2. Appendix B: Playwright + Storycap Implementation Kit
playwright.config.ts (for Storybook)
A dedicated Playwright configuration for visual testing stories.

TypeScript

import { defineConfig, devices } from '@playwright/test';

// Base URL for the Storybook instance
const storybookUrl = 'http://localhost:6006';

export default defineConfig({
  testDir: './src', // Point to your stories directory
  testMatch: '**/*.stories.tsx', // Match story files
  timeout: 30 * 1000,
  fullyParallel: true,
  forbidOnly:!!process.env.CI,
  retries: process.env.CI? 2 : 0,
  workers: process.env.CI? 1 : undefined,
  reporter: 'html',

  use: {
    baseURL: storybookUrl,
    trace: 'on-first-retry',
  },

  // Configure projects for major browsers. [15]
  projects: },
    },
    {
      name: 'firefox',
      use: {...devices },
    },
    {
      name: 'webkit',
      use: {...devices },
    },
  ],

  // Launch the Storybook dev server before running tests. [17]
  webServer: {
    command: 'npm run storybook',
    url: storybookUrl,
    reuseExistingServer:!process.env.CI,
    timeout: 120 * 1000,
  },

  // Global snapshot configurations. [24, 25]
  expect: {
    toHaveScreenshot: {
      // Disable animations globally for all snapshots to reduce flakiness. [21]
      animations: 'disabled',
      // A small threshold to account for minor anti-aliasing differences.
      threshold: 0.2,
    },
  },
});
Storybook Test Runner Config (.storybook/test-runner.mjs)
This file integrates @storycap-testrun/node for stable screenshots.

JavaScript

import { injectAxe, checkA11y } from 'axe-playwright';
import { screenshot } from '@storycap-testrun/node';
import path from 'path';

export default {
  async preVisit(page) {
    await injectAxe(page);
  },
  async postVisit(page, context) {
    // Run accessibility tests
    await checkA11y(page, '#storybook-root', {
      detailedReport: true,
      detailedReportOptions: { html: true },
    });

    // Take a stable screenshot using storycap-testrun's advanced stability checks. [16, 20]
    const image = await screenshot(page, context, {
      // Define a consistent naming scheme for snapshots.
      fileName: path.join(
        context.title.replace(/\//g, '-'),
        `${context.name}.png`
      ),
      // Advanced stability options can be configured here if needed.
      flakiness: {
        retake: { enabled: true, retries: 3 },
        metrics: { enabled: true },
      },
    });

    // Use Playwright's built-in assertion to compare the snapshot.
    expect(image).toMatchSnapshot();
  },
};
GitHub Actions Workflows
test-and-report.yml:

YAML

name: 'Playwright Visual Tests'
on:
  pull_request:
    branches: [ main ]
jobs:
  test:
    timeout-minutes: 60
    runs-on: ubuntu-latest
    container:
      # Use the official Playwright Docker image for a consistent rendering environment. [26]
      image: mcr.microsoft.com/playwright:v1.44.0-jammy
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20
      - name: Install dependencies
        run: npm ci
      - name: Install Git LFS
        run: |
          apt-get update
          apt-get install -y git-lfs
          git lfs install
      - name: Pull LFS files
        run: git lfs pull
      - name: Run Playwright tests
        id: playwright_run
        # Continue on error to allow artifact upload even if tests fail.
        continue-on-error: true
        run: npx test-storybook
      - name: Upload report artifact
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report
          path: playwright-report/
          retention-days: 30
      - name: Post PR comment on failure
        if: failure()
        uses: thollander/actions-comment-pull-request@v2
        with:
          message: |
            ðŸš¨ **Visual Regressions Detected** ðŸš¨

            Playwright tests failed. Please review the visual diffs in the(https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}).

            To approve intentional changes, comment `/approve-snapshots` on this PR.
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Check test run outcome
        if: steps.playwright_run.outcome == 'failure'
        run: exit 1
approve-snapshots.yml:

YAML

name: 'Approve & Update Snapshots'
on:
  issue_comment:
    types: [created]
jobs:
  approve:
    # Only run if the comment is on a PR and contains the magic phrase. [4]
    if: github.event.issue.pull_request && contains(github.event.comment.body, '/approve-snapshots')
    runs-on: ubuntu-latest
    steps:
      - name: Get PR branch
        id: comment-branch
        uses: xt0rted/pull-request-comment-branch@v2
      - name: Checkout PR branch
        uses: actions/checkout@v4
        with:
          ref: ${{ steps.comment-branch.outputs.head_ref }}
      - uses: actions/setup-node@v4
        with:
          node-version: 20
      - name: Install dependencies
        run: npm ci
      - name: Install Git LFS
        run: |
          apt-get update
          apt-get install -y git-lfs
          git lfs install
      - name: Update snapshots
        run: npx test-storybook --update-snapshots
      - name: Commit and push updated snapshots
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: 'chore: update visual regression snapshots'
          branch: ${{ steps.comment-branch.outputs.head_ref }}
.gitattributes for Git LFS
Place this file in the root of the repository to track PNG snapshots with LFS.

# Track all PNG files in the snapshot directories with Git LFS. [5]
src/**/*.png filter=lfs diff=lfs merge=lfs -text
6.3. Appendix C: Getting Started Guide
Creating the First Baseline
Follow these steps to generate the initial set of "golden" snapshots for the chosen solution.

For Chromatic:

Ensure the chromatic.config.json and .github/workflows/chromatic.yml files are in place.

Add your CHROMATIC_PROJECT_TOKEN as a secret to the GitHub repository.

Push your code to the main branch. The GitHub Action will run automatically.

Navigate to the build URL provided in the action logs.

Review the initial snapshots in the Chromatic UI and click "Accept all" to establish the first baseline.

For Playwright + Storycap:

Ensure all configuration files (playwright.config.ts, .storybook/test-runner.mjs, .gitattributes) are in place.

Install Git LFS on your local machine (git lfs install).

Run the following command in your terminal:
npx test-storybook --update-snapshots

This will generate the initial PNG files in your source tree.

Commit and push these new snapshot files to your branch. They will be stored via Git LFS.

Tagging Critical Stories
To start with a focused set of tests on core components (Foundations, Contexts, Status Chips) without refactoring stories, use Storybook's parameters API.

For Chromatic:
To disable a story that is not yet considered critical, add a parameter to its meta or story definition:

JavaScript

export default {
  title: 'Components/NonCriticalComponent',
  component: NonCriticalComponent,
  parameters: {
    // This will tell Chromatic to skip snapshotting for all stories in this file.
    chromatic: { disableSnapshot: true },
  },
};
For Playwright/Storycap:
A custom tag can be added to critical stories, which can then be filtered in the test runner.

Tag the story: Add a custom parameter to the stories you want to test.

JavaScript

export default {
  title: 'Foundations/Colors',
  component: ColorPalette,
  parameters: {
    // Add a custom tag for visual regression testing.
    vrt: { tags: ['critical'] },
  },
};
Filter in the test runner: Modify the .storybook/test-runner.mjs to only snapshot stories with this tag.

JavaScript

// In.storybook/test-runner.mjs
//... inside postVisit function
const { vrt } = context.parameters |

| {};
if (vrt && vrt.tags && vrt.tags.includes('critical')) {
// This story is tagged as critical, so proceed with the screenshot.
const image = await screenshot(page, context, { /*... options... */ });
expect(image).toMatchSnapshot();
} else {
// Skip screenshotting for non-critical stories.
console.log(Skipping snapshot for non-critical story: ${context.title}/${context.name});
}
```


Sources used in the report

chromatic.com
Visual testing & review for web user interfaces â€¢ Chromatic
Opens in a new window

medium.com
Percy vs Chromatic: Which visual regression testing tool to use? | by Crissy Joshua
Opens in a new window

dev.to
Visual Testing with Chromatic - DEV Community
Opens in a new window

medium.com
Streamlining Playwright Visual Regression Testing with GitHub ...
Opens in a new window

github.com
[Feature] Support remote storage for visual regression testing Â· Issue #29227 Â· microsoft/playwright - GitHub
Opens in a new window

storybook.js.org
Visual tests | Storybook docs
Opens in a new window

dev.to
Visual Testing Your Components With Chromatic - DEV Community
Opens in a new window

m.youtube.com
Visual Regression testing with Storybook and Chromatic - YouTube
Opens in a new window

itnext.io
You don't need Chromatic. Use PW instead - ITNEXT
Opens in a new window

chromatic.com
Animations â€¢ Chromatic docs
Opens in a new window

storybook.js.org
Deploy Storybook
Opens in a new window

medium.com
Introducing Chromatic GitHub Action | by Dominic Nguyen - Medium
Opens in a new window

chromatic.com
Pricing â€¢ Chromatic
Opens in a new window

vendr.com
Chromatic Software Pricing & Plans 2025: See Your Cost - Vendr
Opens in a new window

dev.to
Automating Visual Regression Testing with Playwright - DEV Community
Opens in a new window

storybook.js.org
@storycap-testrun/node | Storybook integrations
Opens in a new window

medium.com
Automated Visual Regression Testing with Playwright and ... - Medium
Opens in a new window

youtube.com
Visual Regression Testing with Playwright: Catch Every UI Change! - YouTube
Opens in a new window

medium.com
Advanced Visual Testing with Playwright | by Dipen Chavan - Medium
Opens in a new window

github.com
reg-viz/storycap-testrun: :camera: A utility that provides stable screenshot capture functionality using `@storybook/test-runner`. - GitHub
Opens in a new window

playwright.dev
PageAssertions - Playwright
Opens in a new window

thegreenreport.blog
Automating Animation Testing with Playwright: A Practical Guide - The Green Report
Opens in a new window

webscraping.ai
Is there a way to throttle the network in Playwright? | WebScraping.AI
Opens in a new window

playwright.dev
Visual comparisons - Playwright
Opens in a new window

playwright.dev
Configuration - Playwright
Opens in a new window

playwright.dev
Continuous Integration - Playwright
Opens in a new window

reddit.com
I have a playwright action that runs playwright across my environemnts. However I want to do screenshot testing as well... any tips on that? : r/github - Reddit
Opens in a new window

chromatic.com
Configuration reference â€¢ Chromatic docs